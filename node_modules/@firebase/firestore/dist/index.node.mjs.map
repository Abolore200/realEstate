{"version":3,"file":"index.node.mjs","sources":["index.node.mjs"],"sourcesContent":["import { _getProvider, getApp, _removeServiceInstance, _registerComponent, registerVersion, SDK_VERSION as SDK_VERSION$1 } from '@firebase/app';\nimport { Component } from '@firebase/component';\nimport { Logger, LogLevel } from '@firebase/logger';\nimport { inspect, TextEncoder, TextDecoder } from 'util';\nimport { FirebaseError, getUA, isIndexedDBAvailable, isSafari, createMockUserToken, getModularInstance, deepEqual, getDefaultEmulatorHostnameAndPort } from '@firebase/util';\nimport { randomBytes as randomBytes$1 } from 'crypto';\nimport { Integer, Md5 } from '@firebase/webchannel-wrapper';\nimport * as grpc from '@grpc/grpc-js';\nimport * as protoLoader from '@grpc/proto-loader';\n\nconst name = \"@firebase/firestore\";\nconst version$1 = \"3.13.0\";\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Simple wrapper around a nullable UID. Mostly exists to make code more\r\n * readable.\r\n */\r\nclass User {\r\n    constructor(uid) {\r\n        this.uid = uid;\r\n    }\r\n    isAuthenticated() {\r\n        return this.uid != null;\r\n    }\r\n    /**\r\n     * Returns a key representing this user, suitable for inclusion in a\r\n     * dictionary.\r\n     */\r\n    toKey() {\r\n        if (this.isAuthenticated()) {\r\n            return 'uid:' + this.uid;\r\n        }\r\n        else {\r\n            return 'anonymous-user';\r\n        }\r\n    }\r\n    isEqual(otherUser) {\r\n        return otherUser.uid === this.uid;\r\n    }\r\n}\r\n/** A user with a null UID. */\r\nUser.UNAUTHENTICATED = new User(null);\r\n// TODO(mikelehen): Look into getting a proper uid-equivalent for\r\n// non-FirebaseAuth providers.\r\nUser.GOOGLE_CREDENTIALS = new User('google-credentials-uid');\r\nUser.FIRST_PARTY = new User('first-party-uid');\r\nUser.MOCK_USER = new User('mock-user');\n\nconst version = \"9.23.0\";\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nlet SDK_VERSION = version;\r\nfunction setSDKVersion(version) {\r\n    SDK_VERSION = version;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Formats an object as a JSON string, suitable for logging. */\r\nfunction formatJSON(value) {\r\n    // util.inspect() results in much more readable output than JSON.stringify()\r\n    return inspect(value, { depth: 100 });\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst logClient = new Logger('@firebase/firestore');\r\n// Helper methods are needed because variables can't be exported as read/write\r\nfunction getLogLevel() {\r\n    return logClient.logLevel;\r\n}\r\n/**\r\n * Sets the verbosity of Cloud Firestore logs (debug, error, or silent).\r\n *\r\n * @param logLevel - The verbosity you set for activity and error logging. Can\r\n *   be any of the following values:\r\n *\r\n *   <ul>\r\n *     <li>`debug` for the most verbose logging level, primarily for\r\n *     debugging.</li>\r\n *     <li>`error` to log errors only.</li>\r\n *     <li><code>`silent` to turn off logging.</li>\r\n *   </ul>\r\n */\r\nfunction setLogLevel(logLevel) {\r\n    logClient.setLogLevel(logLevel);\r\n}\r\nfunction logDebug(msg, ...obj) {\r\n    if (logClient.logLevel <= LogLevel.DEBUG) {\r\n        const args = obj.map(argToString);\r\n        logClient.debug(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\r\n    }\r\n}\r\nfunction logError(msg, ...obj) {\r\n    if (logClient.logLevel <= LogLevel.ERROR) {\r\n        const args = obj.map(argToString);\r\n        logClient.error(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\r\n    }\r\n}\r\n/**\r\n * @internal\r\n */\r\nfunction logWarn(msg, ...obj) {\r\n    if (logClient.logLevel <= LogLevel.WARN) {\r\n        const args = obj.map(argToString);\r\n        logClient.warn(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\r\n    }\r\n}\r\n/**\r\n * Converts an additional log parameter to a string representation.\r\n */\r\nfunction argToString(obj) {\r\n    if (typeof obj === 'string') {\r\n        return obj;\r\n    }\r\n    else {\r\n        try {\r\n            return formatJSON(obj);\r\n        }\r\n        catch (e) {\r\n            // Converting to JSON failed, just log the object directly\r\n            return obj;\r\n        }\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Unconditionally fails, throwing an Error with the given message.\r\n * Messages are stripped in production builds.\r\n *\r\n * Returns `never` and can be used in expressions:\r\n * @example\r\n * let futureVar = fail('not implemented yet');\r\n */\r\nfunction fail(failure = 'Unexpected state') {\r\n    // Log the failure in addition to throw an exception, just in case the\r\n    // exception is swallowed.\r\n    const message = `FIRESTORE (${SDK_VERSION}) INTERNAL ASSERTION FAILED: ` + failure;\r\n    logError(message);\r\n    // NOTE: We don't use FirestoreError here because these are internal failures\r\n    // that cannot be handled by the user. (Also it would create a circular\r\n    // dependency between the error and assert modules which doesn't work.)\r\n    throw new Error(message);\r\n}\r\n/**\r\n * Fails if the given assertion condition is false, throwing an Error with the\r\n * given message if it did.\r\n *\r\n * Messages are stripped in production builds.\r\n */\r\nfunction hardAssert(assertion, message) {\r\n    if (!assertion) {\r\n        fail();\r\n    }\r\n}\r\n/**\r\n * Fails if the given assertion condition is false, throwing an Error with the\r\n * given message if it did.\r\n *\r\n * The code of callsites invoking this function are stripped out in production\r\n * builds. Any side-effects of code within the debugAssert() invocation will not\r\n * happen in this case.\r\n *\r\n * @internal\r\n */\r\nfunction debugAssert(assertion, message) {\r\n    if (!assertion) {\r\n        fail();\r\n    }\r\n}\r\n/**\r\n * Casts `obj` to `T`. In non-production builds, verifies that `obj` is an\r\n * instance of `T` before casting.\r\n */\r\nfunction debugCast(obj, \r\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\r\nconstructor) {\r\n    return obj;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst Code = {\r\n    // Causes are copied from:\r\n    // https://github.com/grpc/grpc/blob/bceec94ea4fc5f0085d81235d8e1c06798dc341a/include/grpc%2B%2B/impl/codegen/status_code_enum.h\r\n    /** Not an error; returned on success. */\r\n    OK: 'ok',\r\n    /** The operation was cancelled (typically by the caller). */\r\n    CANCELLED: 'cancelled',\r\n    /** Unknown error or an error from a different error domain. */\r\n    UNKNOWN: 'unknown',\r\n    /**\r\n     * Client specified an invalid argument. Note that this differs from\r\n     * FAILED_PRECONDITION. INVALID_ARGUMENT indicates arguments that are\r\n     * problematic regardless of the state of the system (e.g., a malformed file\r\n     * name).\r\n     */\r\n    INVALID_ARGUMENT: 'invalid-argument',\r\n    /**\r\n     * Deadline expired before operation could complete. For operations that\r\n     * change the state of the system, this error may be returned even if the\r\n     * operation has completed successfully. For example, a successful response\r\n     * from a server could have been delayed long enough for the deadline to\r\n     * expire.\r\n     */\r\n    DEADLINE_EXCEEDED: 'deadline-exceeded',\r\n    /** Some requested entity (e.g., file or directory) was not found. */\r\n    NOT_FOUND: 'not-found',\r\n    /**\r\n     * Some entity that we attempted to create (e.g., file or directory) already\r\n     * exists.\r\n     */\r\n    ALREADY_EXISTS: 'already-exists',\r\n    /**\r\n     * The caller does not have permission to execute the specified operation.\r\n     * PERMISSION_DENIED must not be used for rejections caused by exhausting\r\n     * some resource (use RESOURCE_EXHAUSTED instead for those errors).\r\n     * PERMISSION_DENIED must not be used if the caller can not be identified\r\n     * (use UNAUTHENTICATED instead for those errors).\r\n     */\r\n    PERMISSION_DENIED: 'permission-denied',\r\n    /**\r\n     * The request does not have valid authentication credentials for the\r\n     * operation.\r\n     */\r\n    UNAUTHENTICATED: 'unauthenticated',\r\n    /**\r\n     * Some resource has been exhausted, perhaps a per-user quota, or perhaps the\r\n     * entire file system is out of space.\r\n     */\r\n    RESOURCE_EXHAUSTED: 'resource-exhausted',\r\n    /**\r\n     * Operation was rejected because the system is not in a state required for\r\n     * the operation's execution. For example, directory to be deleted may be\r\n     * non-empty, an rmdir operation is applied to a non-directory, etc.\r\n     *\r\n     * A litmus test that may help a service implementor in deciding\r\n     * between FAILED_PRECONDITION, ABORTED, and UNAVAILABLE:\r\n     *  (a) Use UNAVAILABLE if the client can retry just the failing call.\r\n     *  (b) Use ABORTED if the client should retry at a higher-level\r\n     *      (e.g., restarting a read-modify-write sequence).\r\n     *  (c) Use FAILED_PRECONDITION if the client should not retry until\r\n     *      the system state has been explicitly fixed. E.g., if an \"rmdir\"\r\n     *      fails because the directory is non-empty, FAILED_PRECONDITION\r\n     *      should be returned since the client should not retry unless\r\n     *      they have first fixed up the directory by deleting files from it.\r\n     *  (d) Use FAILED_PRECONDITION if the client performs conditional\r\n     *      REST Get/Update/Delete on a resource and the resource on the\r\n     *      server does not match the condition. E.g., conflicting\r\n     *      read-modify-write on the same resource.\r\n     */\r\n    FAILED_PRECONDITION: 'failed-precondition',\r\n    /**\r\n     * The operation was aborted, typically due to a concurrency issue like\r\n     * sequencer check failures, transaction aborts, etc.\r\n     *\r\n     * See litmus test above for deciding between FAILED_PRECONDITION, ABORTED,\r\n     * and UNAVAILABLE.\r\n     */\r\n    ABORTED: 'aborted',\r\n    /**\r\n     * Operation was attempted past the valid range. E.g., seeking or reading\r\n     * past end of file.\r\n     *\r\n     * Unlike INVALID_ARGUMENT, this error indicates a problem that may be fixed\r\n     * if the system state changes. For example, a 32-bit file system will\r\n     * generate INVALID_ARGUMENT if asked to read at an offset that is not in the\r\n     * range [0,2^32-1], but it will generate OUT_OF_RANGE if asked to read from\r\n     * an offset past the current file size.\r\n     *\r\n     * There is a fair bit of overlap between FAILED_PRECONDITION and\r\n     * OUT_OF_RANGE. We recommend using OUT_OF_RANGE (the more specific error)\r\n     * when it applies so that callers who are iterating through a space can\r\n     * easily look for an OUT_OF_RANGE error to detect when they are done.\r\n     */\r\n    OUT_OF_RANGE: 'out-of-range',\r\n    /** Operation is not implemented or not supported/enabled in this service. */\r\n    UNIMPLEMENTED: 'unimplemented',\r\n    /**\r\n     * Internal errors. Means some invariants expected by underlying System has\r\n     * been broken. If you see one of these errors, Something is very broken.\r\n     */\r\n    INTERNAL: 'internal',\r\n    /**\r\n     * The service is currently unavailable. This is a most likely a transient\r\n     * condition and may be corrected by retrying with a backoff.\r\n     *\r\n     * See litmus test above for deciding between FAILED_PRECONDITION, ABORTED,\r\n     * and UNAVAILABLE.\r\n     */\r\n    UNAVAILABLE: 'unavailable',\r\n    /** Unrecoverable data loss or corruption. */\r\n    DATA_LOSS: 'data-loss'\r\n};\r\n/** An error returned by a Firestore operation. */\r\nclass FirestoreError extends FirebaseError {\r\n    /** @hideconstructor */\r\n    constructor(\r\n    /**\r\n     * The backend error code associated with this error.\r\n     */\r\n    code, \r\n    /**\r\n     * A custom error description.\r\n     */\r\n    message) {\r\n        super(code, message);\r\n        this.code = code;\r\n        this.message = message;\r\n        // HACK: We write a toString property directly because Error is not a real\r\n        // class and so inheritance does not work correctly. We could alternatively\r\n        // do the same \"back-door inheritance\" trick that FirebaseError does.\r\n        this.toString = () => `${this.name}: [code=${this.code}]: ${this.message}`;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass Deferred {\r\n    constructor() {\r\n        this.promise = new Promise((resolve, reject) => {\r\n            this.resolve = resolve;\r\n            this.reject = reject;\r\n        });\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass OAuthToken {\r\n    constructor(value, user) {\r\n        this.user = user;\r\n        this.type = 'OAuth';\r\n        this.headers = new Map();\r\n        this.headers.set('Authorization', `Bearer ${value}`);\r\n    }\r\n}\r\n/**\r\n * A CredentialsProvider that always yields an empty token.\r\n * @internal\r\n */\r\nclass EmptyAuthCredentialsProvider {\r\n    getToken() {\r\n        return Promise.resolve(null);\r\n    }\r\n    invalidateToken() { }\r\n    start(asyncQueue, changeListener) {\r\n        // Fire with initial user.\r\n        asyncQueue.enqueueRetryable(() => changeListener(User.UNAUTHENTICATED));\r\n    }\r\n    shutdown() { }\r\n}\r\n/**\r\n * A CredentialsProvider that always returns a constant token. Used for\r\n * emulator token mocking.\r\n */\r\nclass EmulatorAuthCredentialsProvider {\r\n    constructor(token) {\r\n        this.token = token;\r\n        /**\r\n         * Stores the listener registered with setChangeListener()\r\n         * This isn't actually necessary since the UID never changes, but we use this\r\n         * to verify the listen contract is adhered to in tests.\r\n         */\r\n        this.changeListener = null;\r\n    }\r\n    getToken() {\r\n        return Promise.resolve(this.token);\r\n    }\r\n    invalidateToken() { }\r\n    start(asyncQueue, changeListener) {\r\n        this.changeListener = changeListener;\r\n        // Fire with initial user.\r\n        asyncQueue.enqueueRetryable(() => changeListener(this.token.user));\r\n    }\r\n    shutdown() {\r\n        this.changeListener = null;\r\n    }\r\n}\r\nclass FirebaseAuthCredentialsProvider {\r\n    constructor(authProvider) {\r\n        this.authProvider = authProvider;\r\n        /** Tracks the current User. */\r\n        this.currentUser = User.UNAUTHENTICATED;\r\n        /**\r\n         * Counter used to detect if the token changed while a getToken request was\r\n         * outstanding.\r\n         */\r\n        this.tokenCounter = 0;\r\n        this.forceRefresh = false;\r\n        this.auth = null;\r\n    }\r\n    start(asyncQueue, changeListener) {\r\n        let lastTokenId = this.tokenCounter;\r\n        // A change listener that prevents double-firing for the same token change.\r\n        const guardedChangeListener = user => {\r\n            if (this.tokenCounter !== lastTokenId) {\r\n                lastTokenId = this.tokenCounter;\r\n                return changeListener(user);\r\n            }\r\n            else {\r\n                return Promise.resolve();\r\n            }\r\n        };\r\n        // A promise that can be waited on to block on the next token change.\r\n        // This promise is re-created after each change.\r\n        let nextToken = new Deferred();\r\n        this.tokenListener = () => {\r\n            this.tokenCounter++;\r\n            this.currentUser = this.getUser();\r\n            nextToken.resolve();\r\n            nextToken = new Deferred();\r\n            asyncQueue.enqueueRetryable(() => guardedChangeListener(this.currentUser));\r\n        };\r\n        const awaitNextToken = () => {\r\n            const currentTokenAttempt = nextToken;\r\n            asyncQueue.enqueueRetryable(async () => {\r\n                await currentTokenAttempt.promise;\r\n                await guardedChangeListener(this.currentUser);\r\n            });\r\n        };\r\n        const registerAuth = (auth) => {\r\n            logDebug('FirebaseAuthCredentialsProvider', 'Auth detected');\r\n            this.auth = auth;\r\n            this.auth.addAuthTokenListener(this.tokenListener);\r\n            awaitNextToken();\r\n        };\r\n        this.authProvider.onInit(auth => registerAuth(auth));\r\n        // Our users can initialize Auth right after Firestore, so we give it\r\n        // a chance to register itself with the component framework before we\r\n        // determine whether to start up in unauthenticated mode.\r\n        setTimeout(() => {\r\n            if (!this.auth) {\r\n                const auth = this.authProvider.getImmediate({ optional: true });\r\n                if (auth) {\r\n                    registerAuth(auth);\r\n                }\r\n                else {\r\n                    // If auth is still not available, proceed with `null` user\r\n                    logDebug('FirebaseAuthCredentialsProvider', 'Auth not yet detected');\r\n                    nextToken.resolve();\r\n                    nextToken = new Deferred();\r\n                }\r\n            }\r\n        }, 0);\r\n        awaitNextToken();\r\n    }\r\n    getToken() {\r\n        // Take note of the current value of the tokenCounter so that this method\r\n        // can fail (with an ABORTED error) if there is a token change while the\r\n        // request is outstanding.\r\n        const initialTokenCounter = this.tokenCounter;\r\n        const forceRefresh = this.forceRefresh;\r\n        this.forceRefresh = false;\r\n        if (!this.auth) {\r\n            return Promise.resolve(null);\r\n        }\r\n        return this.auth.getToken(forceRefresh).then(tokenData => {\r\n            // Cancel the request since the token changed while the request was\r\n            // outstanding so the response is potentially for a previous user (which\r\n            // user, we can't be sure).\r\n            if (this.tokenCounter !== initialTokenCounter) {\r\n                logDebug('FirebaseAuthCredentialsProvider', 'getToken aborted due to token change.');\r\n                return this.getToken();\r\n            }\r\n            else {\r\n                if (tokenData) {\r\n                    hardAssert(typeof tokenData.accessToken === 'string');\r\n                    return new OAuthToken(tokenData.accessToken, this.currentUser);\r\n                }\r\n                else {\r\n                    return null;\r\n                }\r\n            }\r\n        });\r\n    }\r\n    invalidateToken() {\r\n        this.forceRefresh = true;\r\n    }\r\n    shutdown() {\r\n        if (this.auth) {\r\n            this.auth.removeAuthTokenListener(this.tokenListener);\r\n        }\r\n    }\r\n    // Auth.getUid() can return null even with a user logged in. It is because\r\n    // getUid() is synchronous, but the auth code populating Uid is asynchronous.\r\n    // This method should only be called in the AuthTokenListener callback\r\n    // to guarantee to get the actual user.\r\n    getUser() {\r\n        const currentUid = this.auth && this.auth.getUid();\r\n        hardAssert(currentUid === null || typeof currentUid === 'string');\r\n        return new User(currentUid);\r\n    }\r\n}\r\n/*\r\n * FirstPartyToken provides a fresh token each time its value\r\n * is requested, because if the token is too old, requests will be rejected.\r\n * Technically this may no longer be necessary since the SDK should gracefully\r\n * recover from unauthenticated errors (see b/33147818 for context), but it's\r\n * safer to keep the implementation as-is.\r\n */\r\nclass FirstPartyToken {\r\n    constructor(sessionIndex, iamToken, authTokenFactory) {\r\n        this.sessionIndex = sessionIndex;\r\n        this.iamToken = iamToken;\r\n        this.authTokenFactory = authTokenFactory;\r\n        this.type = 'FirstParty';\r\n        this.user = User.FIRST_PARTY;\r\n        this._headers = new Map();\r\n    }\r\n    /**\r\n     * Gets an authorization token, using a provided factory function, or return\r\n     * null.\r\n     */\r\n    getAuthToken() {\r\n        if (this.authTokenFactory) {\r\n            return this.authTokenFactory();\r\n        }\r\n        else {\r\n            return null;\r\n        }\r\n    }\r\n    get headers() {\r\n        this._headers.set('X-Goog-AuthUser', this.sessionIndex);\r\n        // Use array notation to prevent minification\r\n        const authHeaderTokenValue = this.getAuthToken();\r\n        if (authHeaderTokenValue) {\r\n            this._headers.set('Authorization', authHeaderTokenValue);\r\n        }\r\n        if (this.iamToken) {\r\n            this._headers.set('X-Goog-Iam-Authorization-Token', this.iamToken);\r\n        }\r\n        return this._headers;\r\n    }\r\n}\r\n/*\r\n * Provides user credentials required for the Firestore JavaScript SDK\r\n * to authenticate the user, using technique that is only available\r\n * to applications hosted by Google.\r\n */\r\nclass FirstPartyAuthCredentialsProvider {\r\n    constructor(sessionIndex, iamToken, authTokenFactory) {\r\n        this.sessionIndex = sessionIndex;\r\n        this.iamToken = iamToken;\r\n        this.authTokenFactory = authTokenFactory;\r\n    }\r\n    getToken() {\r\n        return Promise.resolve(new FirstPartyToken(this.sessionIndex, this.iamToken, this.authTokenFactory));\r\n    }\r\n    start(asyncQueue, changeListener) {\r\n        // Fire with initial uid.\r\n        asyncQueue.enqueueRetryable(() => changeListener(User.FIRST_PARTY));\r\n    }\r\n    shutdown() { }\r\n    invalidateToken() { }\r\n}\r\nclass AppCheckToken {\r\n    constructor(value) {\r\n        this.value = value;\r\n        this.type = 'AppCheck';\r\n        this.headers = new Map();\r\n        if (value && value.length > 0) {\r\n            this.headers.set('x-firebase-appcheck', this.value);\r\n        }\r\n    }\r\n}\r\nclass FirebaseAppCheckTokenProvider {\r\n    constructor(appCheckProvider) {\r\n        this.appCheckProvider = appCheckProvider;\r\n        this.forceRefresh = false;\r\n        this.appCheck = null;\r\n        this.latestAppCheckToken = null;\r\n    }\r\n    start(asyncQueue, changeListener) {\r\n        const onTokenChanged = tokenResult => {\r\n            if (tokenResult.error != null) {\r\n                logDebug('FirebaseAppCheckTokenProvider', `Error getting App Check token; using placeholder token instead. Error: ${tokenResult.error.message}`);\r\n            }\r\n            const tokenUpdated = tokenResult.token !== this.latestAppCheckToken;\r\n            this.latestAppCheckToken = tokenResult.token;\r\n            logDebug('FirebaseAppCheckTokenProvider', `Received ${tokenUpdated ? 'new' : 'existing'} token.`);\r\n            return tokenUpdated\r\n                ? changeListener(tokenResult.token)\r\n                : Promise.resolve();\r\n        };\r\n        this.tokenListener = (tokenResult) => {\r\n            asyncQueue.enqueueRetryable(() => onTokenChanged(tokenResult));\r\n        };\r\n        const registerAppCheck = (appCheck) => {\r\n            logDebug('FirebaseAppCheckTokenProvider', 'AppCheck detected');\r\n            this.appCheck = appCheck;\r\n            this.appCheck.addTokenListener(this.tokenListener);\r\n        };\r\n        this.appCheckProvider.onInit(appCheck => registerAppCheck(appCheck));\r\n        // Our users can initialize AppCheck after Firestore, so we give it\r\n        // a chance to register itself with the component framework.\r\n        setTimeout(() => {\r\n            if (!this.appCheck) {\r\n                const appCheck = this.appCheckProvider.getImmediate({ optional: true });\r\n                if (appCheck) {\r\n                    registerAppCheck(appCheck);\r\n                }\r\n                else {\r\n                    // If AppCheck is still not available, proceed without it.\r\n                    logDebug('FirebaseAppCheckTokenProvider', 'AppCheck not yet detected');\r\n                }\r\n            }\r\n        }, 0);\r\n    }\r\n    getToken() {\r\n        const forceRefresh = this.forceRefresh;\r\n        this.forceRefresh = false;\r\n        if (!this.appCheck) {\r\n            return Promise.resolve(null);\r\n        }\r\n        return this.appCheck.getToken(forceRefresh).then(tokenResult => {\r\n            if (tokenResult) {\r\n                hardAssert(typeof tokenResult.token === 'string');\r\n                this.latestAppCheckToken = tokenResult.token;\r\n                return new AppCheckToken(tokenResult.token);\r\n            }\r\n            else {\r\n                return null;\r\n            }\r\n        });\r\n    }\r\n    invalidateToken() {\r\n        this.forceRefresh = true;\r\n    }\r\n    shutdown() {\r\n        if (this.appCheck) {\r\n            this.appCheck.removeTokenListener(this.tokenListener);\r\n        }\r\n    }\r\n}\r\n/**\r\n * An AppCheck token provider that always yields an empty token.\r\n * @internal\r\n */\r\nclass EmptyAppCheckTokenProvider {\r\n    getToken() {\r\n        return Promise.resolve(new AppCheckToken(''));\r\n    }\r\n    invalidateToken() { }\r\n    start(asyncQueue, changeListener) { }\r\n    shutdown() { }\r\n}\r\n/**\r\n * Builds a CredentialsProvider depending on the type of\r\n * the credentials passed in.\r\n */\r\nfunction makeAuthCredentialsProvider(credentials) {\r\n    if (!credentials) {\r\n        return new EmptyAuthCredentialsProvider();\r\n    }\r\n    switch (credentials['type']) {\r\n        case 'firstParty':\r\n            return new FirstPartyAuthCredentialsProvider(credentials['sessionIndex'] || '0', credentials['iamToken'] || null, credentials['authTokenFactory'] || null);\r\n        case 'provider':\r\n            return credentials['client'];\r\n        default:\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'makeAuthCredentialsProvider failed due to invalid credential type');\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Generates `nBytes` of random bytes.\r\n *\r\n * If `nBytes < 0` , an error will be thrown.\r\n */\r\nfunction randomBytes(nBytes) {\r\n    return randomBytes$1(nBytes);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass AutoId {\r\n    static newId() {\r\n        // Alphanumeric characters\r\n        const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';\r\n        // The largest byte value that is a multiple of `char.length`.\r\n        const maxMultiple = Math.floor(256 / chars.length) * chars.length;\r\n        let autoId = '';\r\n        const targetLength = 20;\r\n        while (autoId.length < targetLength) {\r\n            const bytes = randomBytes(40);\r\n            for (let i = 0; i < bytes.length; ++i) {\r\n                // Only accept values that are [0, maxMultiple), this ensures they can\r\n                // be evenly mapped to indices of `chars` via a modulo operation.\r\n                if (autoId.length < targetLength && bytes[i] < maxMultiple) {\r\n                    autoId += chars.charAt(bytes[i] % chars.length);\r\n                }\r\n            }\r\n        }\r\n        return autoId;\r\n    }\r\n}\r\nfunction primitiveComparator(left, right) {\r\n    if (left < right) {\r\n        return -1;\r\n    }\r\n    if (left > right) {\r\n        return 1;\r\n    }\r\n    return 0;\r\n}\r\n/** Helper to compare arrays using isEqual(). */\r\nfunction arrayEquals(left, right, comparator) {\r\n    if (left.length !== right.length) {\r\n        return false;\r\n    }\r\n    return left.every((value, index) => comparator(value, right[index]));\r\n}\r\n/**\r\n * Returns the immediate lexicographically-following string. This is useful to\r\n * construct an inclusive range for indexeddb iterators.\r\n */\r\nfunction immediateSuccessor(s) {\r\n    // Return the input string, with an additional NUL byte appended.\r\n    return s + '\\0';\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// The earliest date supported by Firestore timestamps (0001-01-01T00:00:00Z).\r\nconst MIN_SECONDS = -62135596800;\r\n// Number of nanoseconds in a millisecond.\r\nconst MS_TO_NANOS = 1e6;\r\n/**\r\n * A `Timestamp` represents a point in time independent of any time zone or\r\n * calendar, represented as seconds and fractions of seconds at nanosecond\r\n * resolution in UTC Epoch time.\r\n *\r\n * It is encoded using the Proleptic Gregorian Calendar which extends the\r\n * Gregorian calendar backwards to year one. It is encoded assuming all minutes\r\n * are 60 seconds long, i.e. leap seconds are \"smeared\" so that no leap second\r\n * table is needed for interpretation. Range is from 0001-01-01T00:00:00Z to\r\n * 9999-12-31T23:59:59.999999999Z.\r\n *\r\n * For examples and further specifications, refer to the\r\n * {@link https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto | Timestamp definition}.\r\n */\r\nclass Timestamp {\r\n    /**\r\n     * Creates a new timestamp.\r\n     *\r\n     * @param seconds - The number of seconds of UTC time since Unix epoch\r\n     *     1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to\r\n     *     9999-12-31T23:59:59Z inclusive.\r\n     * @param nanoseconds - The non-negative fractions of a second at nanosecond\r\n     *     resolution. Negative second values with fractions must still have\r\n     *     non-negative nanoseconds values that count forward in time. Must be\r\n     *     from 0 to 999,999,999 inclusive.\r\n     */\r\n    constructor(\r\n    /**\r\n     * The number of seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z.\r\n     */\r\n    seconds, \r\n    /**\r\n     * The fractions of a second at nanosecond resolution.*\r\n     */\r\n    nanoseconds) {\r\n        this.seconds = seconds;\r\n        this.nanoseconds = nanoseconds;\r\n        if (nanoseconds < 0) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp nanoseconds out of range: ' + nanoseconds);\r\n        }\r\n        if (nanoseconds >= 1e9) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp nanoseconds out of range: ' + nanoseconds);\r\n        }\r\n        if (seconds < MIN_SECONDS) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp seconds out of range: ' + seconds);\r\n        }\r\n        // This will break in the year 10,000.\r\n        if (seconds >= 253402300800) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp seconds out of range: ' + seconds);\r\n        }\r\n    }\r\n    /**\r\n     * Creates a new timestamp with the current date, with millisecond precision.\r\n     *\r\n     * @returns a new timestamp representing the current date.\r\n     */\r\n    static now() {\r\n        return Timestamp.fromMillis(Date.now());\r\n    }\r\n    /**\r\n     * Creates a new timestamp from the given date.\r\n     *\r\n     * @param date - The date to initialize the `Timestamp` from.\r\n     * @returns A new `Timestamp` representing the same point in time as the given\r\n     *     date.\r\n     */\r\n    static fromDate(date) {\r\n        return Timestamp.fromMillis(date.getTime());\r\n    }\r\n    /**\r\n     * Creates a new timestamp from the given number of milliseconds.\r\n     *\r\n     * @param milliseconds - Number of milliseconds since Unix epoch\r\n     *     1970-01-01T00:00:00Z.\r\n     * @returns A new `Timestamp` representing the same point in time as the given\r\n     *     number of milliseconds.\r\n     */\r\n    static fromMillis(milliseconds) {\r\n        const seconds = Math.floor(milliseconds / 1000);\r\n        const nanos = Math.floor((milliseconds - seconds * 1000) * MS_TO_NANOS);\r\n        return new Timestamp(seconds, nanos);\r\n    }\r\n    /**\r\n     * Converts a `Timestamp` to a JavaScript `Date` object. This conversion\r\n     * causes a loss of precision since `Date` objects only support millisecond\r\n     * precision.\r\n     *\r\n     * @returns JavaScript `Date` object representing the same point in time as\r\n     *     this `Timestamp`, with millisecond precision.\r\n     */\r\n    toDate() {\r\n        return new Date(this.toMillis());\r\n    }\r\n    /**\r\n     * Converts a `Timestamp` to a numeric timestamp (in milliseconds since\r\n     * epoch). This operation causes a loss of precision.\r\n     *\r\n     * @returns The point in time corresponding to this timestamp, represented as\r\n     *     the number of milliseconds since Unix epoch 1970-01-01T00:00:00Z.\r\n     */\r\n    toMillis() {\r\n        return this.seconds * 1000 + this.nanoseconds / MS_TO_NANOS;\r\n    }\r\n    _compareTo(other) {\r\n        if (this.seconds === other.seconds) {\r\n            return primitiveComparator(this.nanoseconds, other.nanoseconds);\r\n        }\r\n        return primitiveComparator(this.seconds, other.seconds);\r\n    }\r\n    /**\r\n     * Returns true if this `Timestamp` is equal to the provided one.\r\n     *\r\n     * @param other - The `Timestamp` to compare against.\r\n     * @returns true if this `Timestamp` is equal to the provided one.\r\n     */\r\n    isEqual(other) {\r\n        return (other.seconds === this.seconds && other.nanoseconds === this.nanoseconds);\r\n    }\r\n    /** Returns a textual representation of this `Timestamp`. */\r\n    toString() {\r\n        return ('Timestamp(seconds=' +\r\n            this.seconds +\r\n            ', nanoseconds=' +\r\n            this.nanoseconds +\r\n            ')');\r\n    }\r\n    /** Returns a JSON-serializable representation of this `Timestamp`. */\r\n    toJSON() {\r\n        return { seconds: this.seconds, nanoseconds: this.nanoseconds };\r\n    }\r\n    /**\r\n     * Converts this object to a primitive string, which allows `Timestamp` objects\r\n     * to be compared using the `>`, `<=`, `>=` and `>` operators.\r\n     */\r\n    valueOf() {\r\n        // This method returns a string of the form <seconds>.<nanoseconds> where\r\n        // <seconds> is translated to have a non-negative value and both <seconds>\r\n        // and <nanoseconds> are left-padded with zeroes to be a consistent length.\r\n        // Strings with this format then have a lexiographical ordering that matches\r\n        // the expected ordering. The <seconds> translation is done to avoid having\r\n        // a leading negative sign (i.e. a leading '-' character) in its string\r\n        // representation, which would affect its lexiographical ordering.\r\n        const adjustedSeconds = this.seconds - MIN_SECONDS;\r\n        // Note: Up to 12 decimal digits are required to represent all valid\r\n        // 'seconds' values.\r\n        const formattedSeconds = String(adjustedSeconds).padStart(12, '0');\r\n        const formattedNanoseconds = String(this.nanoseconds).padStart(9, '0');\r\n        return formattedSeconds + '.' + formattedNanoseconds;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A version of a document in Firestore. This corresponds to the version\r\n * timestamp, such as update_time or read_time.\r\n */\r\nclass SnapshotVersion {\r\n    constructor(timestamp) {\r\n        this.timestamp = timestamp;\r\n    }\r\n    static fromTimestamp(value) {\r\n        return new SnapshotVersion(value);\r\n    }\r\n    static min() {\r\n        return new SnapshotVersion(new Timestamp(0, 0));\r\n    }\r\n    static max() {\r\n        return new SnapshotVersion(new Timestamp(253402300799, 1e9 - 1));\r\n    }\r\n    compareTo(other) {\r\n        return this.timestamp._compareTo(other.timestamp);\r\n    }\r\n    isEqual(other) {\r\n        return this.timestamp.isEqual(other.timestamp);\r\n    }\r\n    /** Returns a number representation of the version for use in spec tests. */\r\n    toMicroseconds() {\r\n        // Convert to microseconds.\r\n        return this.timestamp.seconds * 1e6 + this.timestamp.nanoseconds / 1000;\r\n    }\r\n    toString() {\r\n        return 'SnapshotVersion(' + this.timestamp.toString() + ')';\r\n    }\r\n    toTimestamp() {\r\n        return this.timestamp;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst DOCUMENT_KEY_NAME = '__name__';\r\n/**\r\n * Path represents an ordered sequence of string segments.\r\n */\r\nclass BasePath {\r\n    constructor(segments, offset, length) {\r\n        if (offset === undefined) {\r\n            offset = 0;\r\n        }\r\n        else if (offset > segments.length) {\r\n            fail();\r\n        }\r\n        if (length === undefined) {\r\n            length = segments.length - offset;\r\n        }\r\n        else if (length > segments.length - offset) {\r\n            fail();\r\n        }\r\n        this.segments = segments;\r\n        this.offset = offset;\r\n        this.len = length;\r\n    }\r\n    get length() {\r\n        return this.len;\r\n    }\r\n    isEqual(other) {\r\n        return BasePath.comparator(this, other) === 0;\r\n    }\r\n    child(nameOrPath) {\r\n        const segments = this.segments.slice(this.offset, this.limit());\r\n        if (nameOrPath instanceof BasePath) {\r\n            nameOrPath.forEach(segment => {\r\n                segments.push(segment);\r\n            });\r\n        }\r\n        else {\r\n            segments.push(nameOrPath);\r\n        }\r\n        return this.construct(segments);\r\n    }\r\n    /** The index of one past the last segment of the path. */\r\n    limit() {\r\n        return this.offset + this.length;\r\n    }\r\n    popFirst(size) {\r\n        size = size === undefined ? 1 : size;\r\n        return this.construct(this.segments, this.offset + size, this.length - size);\r\n    }\r\n    popLast() {\r\n        return this.construct(this.segments, this.offset, this.length - 1);\r\n    }\r\n    firstSegment() {\r\n        return this.segments[this.offset];\r\n    }\r\n    lastSegment() {\r\n        return this.get(this.length - 1);\r\n    }\r\n    get(index) {\r\n        return this.segments[this.offset + index];\r\n    }\r\n    isEmpty() {\r\n        return this.length === 0;\r\n    }\r\n    isPrefixOf(other) {\r\n        if (other.length < this.length) {\r\n            return false;\r\n        }\r\n        for (let i = 0; i < this.length; i++) {\r\n            if (this.get(i) !== other.get(i)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    isImmediateParentOf(potentialChild) {\r\n        if (this.length + 1 !== potentialChild.length) {\r\n            return false;\r\n        }\r\n        for (let i = 0; i < this.length; i++) {\r\n            if (this.get(i) !== potentialChild.get(i)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    forEach(fn) {\r\n        for (let i = this.offset, end = this.limit(); i < end; i++) {\r\n            fn(this.segments[i]);\r\n        }\r\n    }\r\n    toArray() {\r\n        return this.segments.slice(this.offset, this.limit());\r\n    }\r\n    static comparator(p1, p2) {\r\n        const len = Math.min(p1.length, p2.length);\r\n        for (let i = 0; i < len; i++) {\r\n            const left = p1.get(i);\r\n            const right = p2.get(i);\r\n            if (left < right) {\r\n                return -1;\r\n            }\r\n            if (left > right) {\r\n                return 1;\r\n            }\r\n        }\r\n        if (p1.length < p2.length) {\r\n            return -1;\r\n        }\r\n        if (p1.length > p2.length) {\r\n            return 1;\r\n        }\r\n        return 0;\r\n    }\r\n}\r\n/**\r\n * A slash-separated path for navigating resources (documents and collections)\r\n * within Firestore.\r\n *\r\n * @internal\r\n */\r\nclass ResourcePath extends BasePath {\r\n    construct(segments, offset, length) {\r\n        return new ResourcePath(segments, offset, length);\r\n    }\r\n    canonicalString() {\r\n        // NOTE: The client is ignorant of any path segments containing escape\r\n        // sequences (e.g. __id123__) and just passes them through raw (they exist\r\n        // for legacy reasons and should not be used frequently).\r\n        return this.toArray().join('/');\r\n    }\r\n    toString() {\r\n        return this.canonicalString();\r\n    }\r\n    /**\r\n     * Creates a resource path from the given slash-delimited string. If multiple\r\n     * arguments are provided, all components are combined. Leading and trailing\r\n     * slashes from all components are ignored.\r\n     */\r\n    static fromString(...pathComponents) {\r\n        // NOTE: The client is ignorant of any path segments containing escape\r\n        // sequences (e.g. __id123__) and just passes them through raw (they exist\r\n        // for legacy reasons and should not be used frequently).\r\n        const segments = [];\r\n        for (const path of pathComponents) {\r\n            if (path.indexOf('//') >= 0) {\r\n                throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid segment (${path}). Paths must not contain // in them.`);\r\n            }\r\n            // Strip leading and traling slashed.\r\n            segments.push(...path.split('/').filter(segment => segment.length > 0));\r\n        }\r\n        return new ResourcePath(segments);\r\n    }\r\n    static emptyPath() {\r\n        return new ResourcePath([]);\r\n    }\r\n}\r\nconst identifierRegExp = /^[_a-zA-Z][_a-zA-Z0-9]*$/;\r\n/**\r\n * A dot-separated path for navigating sub-objects within a document.\r\n * @internal\r\n */\r\nclass FieldPath$1 extends BasePath {\r\n    construct(segments, offset, length) {\r\n        return new FieldPath$1(segments, offset, length);\r\n    }\r\n    /**\r\n     * Returns true if the string could be used as a segment in a field path\r\n     * without escaping.\r\n     */\r\n    static isValidIdentifier(segment) {\r\n        return identifierRegExp.test(segment);\r\n    }\r\n    canonicalString() {\r\n        return this.toArray()\r\n            .map(str => {\r\n            str = str.replace(/\\\\/g, '\\\\\\\\').replace(/`/g, '\\\\`');\r\n            if (!FieldPath$1.isValidIdentifier(str)) {\r\n                str = '`' + str + '`';\r\n            }\r\n            return str;\r\n        })\r\n            .join('.');\r\n    }\r\n    toString() {\r\n        return this.canonicalString();\r\n    }\r\n    /**\r\n     * Returns true if this field references the key of a document.\r\n     */\r\n    isKeyField() {\r\n        return this.length === 1 && this.get(0) === DOCUMENT_KEY_NAME;\r\n    }\r\n    /**\r\n     * The field designating the key of a document.\r\n     */\r\n    static keyField() {\r\n        return new FieldPath$1([DOCUMENT_KEY_NAME]);\r\n    }\r\n    /**\r\n     * Parses a field string from the given server-formatted string.\r\n     *\r\n     * - Splitting the empty string is not allowed (for now at least).\r\n     * - Empty segments within the string (e.g. if there are two consecutive\r\n     *   separators) are not allowed.\r\n     *\r\n     * TODO(b/37244157): we should make this more strict. Right now, it allows\r\n     * non-identifier path components, even if they aren't escaped.\r\n     */\r\n    static fromServerFormat(path) {\r\n        const segments = [];\r\n        let current = '';\r\n        let i = 0;\r\n        const addCurrentSegment = () => {\r\n            if (current.length === 0) {\r\n                throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid field path (${path}). Paths must not be empty, begin ` +\r\n                    `with '.', end with '.', or contain '..'`);\r\n            }\r\n            segments.push(current);\r\n            current = '';\r\n        };\r\n        let inBackticks = false;\r\n        while (i < path.length) {\r\n            const c = path[i];\r\n            if (c === '\\\\') {\r\n                if (i + 1 === path.length) {\r\n                    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Path has trailing escape character: ' + path);\r\n                }\r\n                const next = path[i + 1];\r\n                if (!(next === '\\\\' || next === '.' || next === '`')) {\r\n                    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Path has invalid escape sequence: ' + path);\r\n                }\r\n                current += next;\r\n                i += 2;\r\n            }\r\n            else if (c === '`') {\r\n                inBackticks = !inBackticks;\r\n                i++;\r\n            }\r\n            else if (c === '.' && !inBackticks) {\r\n                addCurrentSegment();\r\n                i++;\r\n            }\r\n            else {\r\n                current += c;\r\n                i++;\r\n            }\r\n        }\r\n        addCurrentSegment();\r\n        if (inBackticks) {\r\n            throw new FirestoreError(Code.INVALID_ARGUMENT, 'Unterminated ` in path: ' + path);\r\n        }\r\n        return new FieldPath$1(segments);\r\n    }\r\n    static emptyPath() {\r\n        return new FieldPath$1([]);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * @internal\r\n */\r\nclass DocumentKey {\r\n    constructor(path) {\r\n        this.path = path;\r\n    }\r\n    static fromPath(path) {\r\n        return new DocumentKey(ResourcePath.fromString(path));\r\n    }\r\n    static fromName(name) {\r\n        return new DocumentKey(ResourcePath.fromString(name).popFirst(5));\r\n    }\r\n    static empty() {\r\n        return new DocumentKey(ResourcePath.emptyPath());\r\n    }\r\n    get collectionGroup() {\r\n        return this.path.popLast().lastSegment();\r\n    }\r\n    /** Returns true if the document is in the specified collectionId. */\r\n    hasCollectionId(collectionId) {\r\n        return (this.path.length >= 2 &&\r\n            this.path.get(this.path.length - 2) === collectionId);\r\n    }\r\n    /** Returns the collection group (i.e. the name of the parent collection) for this key. */\r\n    getCollectionGroup() {\r\n        return this.path.get(this.path.length - 2);\r\n    }\r\n    /** Returns the fully qualified path to the parent collection. */\r\n    getCollectionPath() {\r\n        return this.path.popLast();\r\n    }\r\n    isEqual(other) {\r\n        return (other !== null && ResourcePath.comparator(this.path, other.path) === 0);\r\n    }\r\n    toString() {\r\n        return this.path.toString();\r\n    }\r\n    static comparator(k1, k2) {\r\n        return ResourcePath.comparator(k1.path, k2.path);\r\n    }\r\n    static isDocumentKey(path) {\r\n        return path.length % 2 === 0;\r\n    }\r\n    /**\r\n     * Creates and returns a new document key with the given segments.\r\n     *\r\n     * @param segments - The segments of the path to the document\r\n     * @returns A new instance of DocumentKey\r\n     */\r\n    static fromSegments(segments) {\r\n        return new DocumentKey(new ResourcePath(segments.slice()));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * The initial mutation batch id for each index. Gets updated during index\r\n * backfill.\r\n */\r\nconst INITIAL_LARGEST_BATCH_ID = -1;\r\n/**\r\n * The initial sequence number for each index. Gets updated during index\r\n * backfill.\r\n */\r\nconst INITIAL_SEQUENCE_NUMBER = 0;\r\n/**\r\n * An index definition for field indexes in Firestore.\r\n *\r\n * Every index is associated with a collection. The definition contains a list\r\n * of fields and their index kind (which can be `ASCENDING`, `DESCENDING` or\r\n * `CONTAINS` for ArrayContains/ArrayContainsAny queries).\r\n *\r\n * Unlike the backend, the SDK does not differentiate between collection or\r\n * collection group-scoped indices. Every index can be used for both single\r\n * collection and collection group queries.\r\n */\r\nclass FieldIndex {\r\n    constructor(\r\n    /**\r\n     * The index ID. Returns -1 if the index ID is not available (e.g. the index\r\n     * has not yet been persisted).\r\n     */\r\n    indexId, \r\n    /** The collection ID this index applies to. */\r\n    collectionGroup, \r\n    /** The field segments for this index. */\r\n    fields, \r\n    /** Shows how up-to-date the index is for the current user. */\r\n    indexState) {\r\n        this.indexId = indexId;\r\n        this.collectionGroup = collectionGroup;\r\n        this.fields = fields;\r\n        this.indexState = indexState;\r\n    }\r\n}\r\n/** An ID for an index that has not yet been added to persistence.  */\r\nFieldIndex.UNKNOWN_ID = -1;\r\n/** Returns the ArrayContains/ArrayContainsAny segment for this index. */\r\nfunction fieldIndexGetArraySegment(fieldIndex) {\r\n    return fieldIndex.fields.find(s => s.kind === 2 /* IndexKind.CONTAINS */);\r\n}\r\n/** Returns all directional (ascending/descending) segments for this index. */\r\nfunction fieldIndexGetDirectionalSegments(fieldIndex) {\r\n    return fieldIndex.fields.filter(s => s.kind !== 2 /* IndexKind.CONTAINS */);\r\n}\r\n/**\r\n * Returns the order of the document key component for the given index.\r\n *\r\n * PORTING NOTE: This is only used in the Web IndexedDb implementation.\r\n */\r\nfunction fieldIndexGetKeyOrder(fieldIndex) {\r\n    const directionalSegments = fieldIndexGetDirectionalSegments(fieldIndex);\r\n    return directionalSegments.length === 0\r\n        ? 0 /* IndexKind.ASCENDING */\r\n        : directionalSegments[directionalSegments.length - 1].kind;\r\n}\r\n/**\r\n * Compares indexes by collection group and segments. Ignores update time and\r\n * index ID.\r\n */\r\nfunction fieldIndexSemanticComparator(left, right) {\r\n    let cmp = primitiveComparator(left.collectionGroup, right.collectionGroup);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    for (let i = 0; i < Math.min(left.fields.length, right.fields.length); ++i) {\r\n        cmp = indexSegmentComparator(left.fields[i], right.fields[i]);\r\n        if (cmp !== 0) {\r\n            return cmp;\r\n        }\r\n    }\r\n    return primitiveComparator(left.fields.length, right.fields.length);\r\n}\r\n/** Returns a debug representation of the field index */\r\nfunction fieldIndexToString(fieldIndex) {\r\n    return `id=${fieldIndex.indexId}|cg=${fieldIndex.collectionGroup}|f=${fieldIndex.fields.map(f => `${f.fieldPath}:${f.kind}`).join(',')}`;\r\n}\r\n/** An index component consisting of field path and index type.  */\r\nclass IndexSegment {\r\n    constructor(\r\n    /** The field path of the component. */\r\n    fieldPath, \r\n    /** The fields sorting order. */\r\n    kind) {\r\n        this.fieldPath = fieldPath;\r\n        this.kind = kind;\r\n    }\r\n}\r\nfunction indexSegmentComparator(left, right) {\r\n    const cmp = FieldPath$1.comparator(left.fieldPath, right.fieldPath);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    return primitiveComparator(left.kind, right.kind);\r\n}\r\n/**\r\n * Stores the \"high water mark\" that indicates how updated the Index is for the\r\n * current user.\r\n */\r\nclass IndexState {\r\n    constructor(\r\n    /**\r\n     * Indicates when the index was last updated (relative to other indexes).\r\n     */\r\n    sequenceNumber, \r\n    /** The the latest indexed read time, document and batch id. */\r\n    offset) {\r\n        this.sequenceNumber = sequenceNumber;\r\n        this.offset = offset;\r\n    }\r\n    /** The state of an index that has not yet been backfilled. */\r\n    static empty() {\r\n        return new IndexState(INITIAL_SEQUENCE_NUMBER, IndexOffset.min());\r\n    }\r\n}\r\n/**\r\n * Creates an offset that matches all documents with a read time higher than\r\n * `readTime`.\r\n */\r\nfunction newIndexOffsetSuccessorFromReadTime(readTime, largestBatchId) {\r\n    // We want to create an offset that matches all documents with a read time\r\n    // greater than the provided read time. To do so, we technically need to\r\n    // create an offset for `(readTime, MAX_DOCUMENT_KEY)`. While we could use\r\n    // Unicode codepoints to generate MAX_DOCUMENT_KEY, it is much easier to use\r\n    // `(readTime + 1, DocumentKey.empty())` since `> DocumentKey.empty()` matches\r\n    // all valid document IDs.\r\n    const successorSeconds = readTime.toTimestamp().seconds;\r\n    const successorNanos = readTime.toTimestamp().nanoseconds + 1;\r\n    const successor = SnapshotVersion.fromTimestamp(successorNanos === 1e9\r\n        ? new Timestamp(successorSeconds + 1, 0)\r\n        : new Timestamp(successorSeconds, successorNanos));\r\n    return new IndexOffset(successor, DocumentKey.empty(), largestBatchId);\r\n}\r\n/** Creates a new offset based on the provided document. */\r\nfunction newIndexOffsetFromDocument(document) {\r\n    return new IndexOffset(document.readTime, document.key, INITIAL_LARGEST_BATCH_ID);\r\n}\r\n/**\r\n * Stores the latest read time, document and batch ID that were processed for an\r\n * index.\r\n */\r\nclass IndexOffset {\r\n    constructor(\r\n    /**\r\n     * The latest read time version that has been indexed by Firestore for this\r\n     * field index.\r\n     */\r\n    readTime, \r\n    /**\r\n     * The key of the last document that was indexed for this query. Use\r\n     * `DocumentKey.empty()` if no document has been indexed.\r\n     */\r\n    documentKey, \r\n    /*\r\n     * The largest mutation batch id that's been processed by Firestore.\r\n     */\r\n    largestBatchId) {\r\n        this.readTime = readTime;\r\n        this.documentKey = documentKey;\r\n        this.largestBatchId = largestBatchId;\r\n    }\r\n    /** Returns an offset that sorts before all regular offsets. */\r\n    static min() {\r\n        return new IndexOffset(SnapshotVersion.min(), DocumentKey.empty(), INITIAL_LARGEST_BATCH_ID);\r\n    }\r\n    /** Returns an offset that sorts after all regular offsets. */\r\n    static max() {\r\n        return new IndexOffset(SnapshotVersion.max(), DocumentKey.empty(), INITIAL_LARGEST_BATCH_ID);\r\n    }\r\n}\r\nfunction indexOffsetComparator(left, right) {\r\n    let cmp = left.readTime.compareTo(right.readTime);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    cmp = DocumentKey.comparator(left.documentKey, right.documentKey);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    return primitiveComparator(left.largestBatchId, right.largestBatchId);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst PRIMARY_LEASE_LOST_ERROR_MSG = 'The current tab is not in the required state to perform this operation. ' +\r\n    'It might be necessary to refresh the browser tab.';\r\n/**\r\n * A base class representing a persistence transaction, encapsulating both the\r\n * transaction's sequence numbers as well as a list of onCommitted listeners.\r\n *\r\n * When you call Persistence.runTransaction(), it will create a transaction and\r\n * pass it to your callback. You then pass it to any method that operates\r\n * on persistence.\r\n */\r\nclass PersistenceTransaction {\r\n    constructor() {\r\n        this.onCommittedListeners = [];\r\n    }\r\n    addOnCommittedListener(listener) {\r\n        this.onCommittedListeners.push(listener);\r\n    }\r\n    raiseOnCommittedEvent() {\r\n        this.onCommittedListeners.forEach(listener => listener());\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Verifies the error thrown by a LocalStore operation. If a LocalStore\r\n * operation fails because the primary lease has been taken by another client,\r\n * we ignore the error (the persistence layer will immediately call\r\n * `applyPrimaryLease` to propagate the primary state change). All other errors\r\n * are re-thrown.\r\n *\r\n * @param err - An error returned by a LocalStore operation.\r\n * @returns A Promise that resolves after we recovered, or the original error.\r\n */\r\nasync function ignoreIfPrimaryLeaseLoss(err) {\r\n    if (err.code === Code.FAILED_PRECONDITION &&\r\n        err.message === PRIMARY_LEASE_LOST_ERROR_MSG) {\r\n        logDebug('LocalStore', 'Unexpectedly lost primary lease');\r\n    }\r\n    else {\r\n        throw err;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * PersistencePromise is essentially a re-implementation of Promise except\r\n * it has a .next() method instead of .then() and .next() and .catch() callbacks\r\n * are executed synchronously when a PersistencePromise resolves rather than\r\n * asynchronously (Promise implementations use setImmediate() or similar).\r\n *\r\n * This is necessary to interoperate with IndexedDB which will automatically\r\n * commit transactions if control is returned to the event loop without\r\n * synchronously initiating another operation on the transaction.\r\n *\r\n * NOTE: .then() and .catch() only allow a single consumer, unlike normal\r\n * Promises.\r\n */\r\nclass PersistencePromise {\r\n    constructor(callback) {\r\n        // NOTE: next/catchCallback will always point to our own wrapper functions,\r\n        // not the user's raw next() or catch() callbacks.\r\n        this.nextCallback = null;\r\n        this.catchCallback = null;\r\n        // When the operation resolves, we'll set result or error and mark isDone.\r\n        this.result = undefined;\r\n        this.error = undefined;\r\n        this.isDone = false;\r\n        // Set to true when .then() or .catch() are called and prevents additional\r\n        // chaining.\r\n        this.callbackAttached = false;\r\n        callback(value => {\r\n            this.isDone = true;\r\n            this.result = value;\r\n            if (this.nextCallback) {\r\n                // value should be defined unless T is Void, but we can't express\r\n                // that in the type system.\r\n                this.nextCallback(value);\r\n            }\r\n        }, error => {\r\n            this.isDone = true;\r\n            this.error = error;\r\n            if (this.catchCallback) {\r\n                this.catchCallback(error);\r\n            }\r\n        });\r\n    }\r\n    catch(fn) {\r\n        return this.next(undefined, fn);\r\n    }\r\n    next(nextFn, catchFn) {\r\n        if (this.callbackAttached) {\r\n            fail();\r\n        }\r\n        this.callbackAttached = true;\r\n        if (this.isDone) {\r\n            if (!this.error) {\r\n                return this.wrapSuccess(nextFn, this.result);\r\n            }\r\n            else {\r\n                return this.wrapFailure(catchFn, this.error);\r\n            }\r\n        }\r\n        else {\r\n            return new PersistencePromise((resolve, reject) => {\r\n                this.nextCallback = (value) => {\r\n                    this.wrapSuccess(nextFn, value).next(resolve, reject);\r\n                };\r\n                this.catchCallback = (error) => {\r\n                    this.wrapFailure(catchFn, error).next(resolve, reject);\r\n                };\r\n            });\r\n        }\r\n    }\r\n    toPromise() {\r\n        return new Promise((resolve, reject) => {\r\n            this.next(resolve, reject);\r\n        });\r\n    }\r\n    wrapUserFunction(fn) {\r\n        try {\r\n            const result = fn();\r\n            if (result instanceof PersistencePromise) {\r\n                return result;\r\n            }\r\n            else {\r\n                return PersistencePromise.resolve(result);\r\n            }\r\n        }\r\n        catch (e) {\r\n            return PersistencePromise.reject(e);\r\n        }\r\n    }\r\n    wrapSuccess(nextFn, value) {\r\n        if (nextFn) {\r\n            return this.wrapUserFunction(() => nextFn(value));\r\n        }\r\n        else {\r\n            // If there's no nextFn, then R must be the same as T\r\n            return PersistencePromise.resolve(value);\r\n        }\r\n    }\r\n    wrapFailure(catchFn, error) {\r\n        if (catchFn) {\r\n            return this.wrapUserFunction(() => catchFn(error));\r\n        }\r\n        else {\r\n            return PersistencePromise.reject(error);\r\n        }\r\n    }\r\n    static resolve(result) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            resolve(result);\r\n        });\r\n    }\r\n    static reject(error) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            reject(error);\r\n        });\r\n    }\r\n    static waitFor(\r\n    // Accept all Promise types in waitFor().\r\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n    all) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            let expectedCount = 0;\r\n            let resolvedCount = 0;\r\n            let done = false;\r\n            all.forEach(element => {\r\n                ++expectedCount;\r\n                element.next(() => {\r\n                    ++resolvedCount;\r\n                    if (done && resolvedCount === expectedCount) {\r\n                        resolve();\r\n                    }\r\n                }, err => reject(err));\r\n            });\r\n            done = true;\r\n            if (resolvedCount === expectedCount) {\r\n                resolve();\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Given an array of predicate functions that asynchronously evaluate to a\r\n     * boolean, implements a short-circuiting `or` between the results. Predicates\r\n     * will be evaluated until one of them returns `true`, then stop. The final\r\n     * result will be whether any of them returned `true`.\r\n     */\r\n    static or(predicates) {\r\n        let p = PersistencePromise.resolve(false);\r\n        for (const predicate of predicates) {\r\n            p = p.next(isTrue => {\r\n                if (isTrue) {\r\n                    return PersistencePromise.resolve(isTrue);\r\n                }\r\n                else {\r\n                    return predicate();\r\n                }\r\n            });\r\n        }\r\n        return p;\r\n    }\r\n    static forEach(collection, f) {\r\n        const promises = [];\r\n        collection.forEach((r, s) => {\r\n            promises.push(f.call(this, r, s));\r\n        });\r\n        return this.waitFor(promises);\r\n    }\r\n    /**\r\n     * Concurrently map all array elements through asynchronous function.\r\n     */\r\n    static mapArray(array, f) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            const expectedCount = array.length;\r\n            const results = new Array(expectedCount);\r\n            let resolvedCount = 0;\r\n            for (let i = 0; i < expectedCount; i++) {\r\n                const current = i;\r\n                f(array[current]).next(result => {\r\n                    results[current] = result;\r\n                    ++resolvedCount;\r\n                    if (resolvedCount === expectedCount) {\r\n                        resolve(results);\r\n                    }\r\n                }, err => reject(err));\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * An alternative to recursive PersistencePromise calls, that avoids\r\n     * potential memory problems from unbounded chains of promises.\r\n     *\r\n     * The `action` will be called repeatedly while `condition` is true.\r\n     */\r\n    static doWhile(condition, action) {\r\n        return new PersistencePromise((resolve, reject) => {\r\n            const process = () => {\r\n                if (condition() === true) {\r\n                    action().next(() => {\r\n                        process();\r\n                    }, reject);\r\n                }\r\n                else {\r\n                    resolve();\r\n                }\r\n            };\r\n            process();\r\n        });\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// References to `window` are guarded by SimpleDb.isAvailable()\r\n/* eslint-disable no-restricted-globals */\r\nconst LOG_TAG$i = 'SimpleDb';\r\n/**\r\n * The maximum number of retry attempts for an IndexedDb transaction that fails\r\n * with a DOMException.\r\n */\r\nconst TRANSACTION_RETRY_COUNT = 3;\r\n/**\r\n * Wraps an IDBTransaction and exposes a store() method to get a handle to a\r\n * specific object store.\r\n */\r\nclass SimpleDbTransaction {\r\n    constructor(action, transaction) {\r\n        this.action = action;\r\n        this.transaction = transaction;\r\n        this.aborted = false;\r\n        /**\r\n         * A `Promise` that resolves with the result of the IndexedDb transaction.\r\n         */\r\n        this.completionDeferred = new Deferred();\r\n        this.transaction.oncomplete = () => {\r\n            this.completionDeferred.resolve();\r\n        };\r\n        this.transaction.onabort = () => {\r\n            if (transaction.error) {\r\n                this.completionDeferred.reject(new IndexedDbTransactionError(action, transaction.error));\r\n            }\r\n            else {\r\n                this.completionDeferred.resolve();\r\n            }\r\n        };\r\n        this.transaction.onerror = (event) => {\r\n            const error = checkForAndReportiOSError(event.target.error);\r\n            this.completionDeferred.reject(new IndexedDbTransactionError(action, error));\r\n        };\r\n    }\r\n    static open(db, action, mode, objectStoreNames) {\r\n        try {\r\n            return new SimpleDbTransaction(action, db.transaction(objectStoreNames, mode));\r\n        }\r\n        catch (e) {\r\n            throw new IndexedDbTransactionError(action, e);\r\n        }\r\n    }\r\n    get completionPromise() {\r\n        return this.completionDeferred.promise;\r\n    }\r\n    abort(error) {\r\n        if (error) {\r\n            this.completionDeferred.reject(error);\r\n        }\r\n        if (!this.aborted) {\r\n            logDebug(LOG_TAG$i, 'Aborting transaction:', error ? error.message : 'Client-initiated abort');\r\n            this.aborted = true;\r\n            this.transaction.abort();\r\n        }\r\n    }\r\n    maybeCommit() {\r\n        // If the browser supports V3 IndexedDB, we invoke commit() explicitly to\r\n        // speed up index DB processing if the event loop remains blocks.\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        const maybeV3IndexedDb = this.transaction;\r\n        if (!this.aborted && typeof maybeV3IndexedDb.commit === 'function') {\r\n            maybeV3IndexedDb.commit();\r\n        }\r\n    }\r\n    /**\r\n     * Returns a SimpleDbStore<KeyType, ValueType> for the specified store. All\r\n     * operations performed on the SimpleDbStore happen within the context of this\r\n     * transaction and it cannot be used anymore once the transaction is\r\n     * completed.\r\n     *\r\n     * Note that we can't actually enforce that the KeyType and ValueType are\r\n     * correct, but they allow type safety through the rest of the consuming code.\r\n     */\r\n    store(storeName) {\r\n        const store = this.transaction.objectStore(storeName);\r\n        return new SimpleDbStore(store);\r\n    }\r\n}\r\n/**\r\n * Provides a wrapper around IndexedDb with a simplified interface that uses\r\n * Promise-like return values to chain operations. Real promises cannot be used\r\n * since .then() continuations are executed asynchronously (e.g. via\r\n * .setImmediate), which would cause IndexedDB to end the transaction.\r\n * See PersistencePromise for more details.\r\n */\r\nclass SimpleDb {\r\n    /*\r\n     * Creates a new SimpleDb wrapper for IndexedDb database `name`.\r\n     *\r\n     * Note that `version` must not be a downgrade. IndexedDB does not support\r\n     * downgrading the schema version. We currently do not support any way to do\r\n     * versioning outside of IndexedDB's versioning mechanism, as only\r\n     * version-upgrade transactions are allowed to do things like create\r\n     * objectstores.\r\n     */\r\n    constructor(name, version, schemaConverter) {\r\n        this.name = name;\r\n        this.version = version;\r\n        this.schemaConverter = schemaConverter;\r\n        const iOSVersion = SimpleDb.getIOSVersion(getUA());\r\n        // NOTE: According to https://bugs.webkit.org/show_bug.cgi?id=197050, the\r\n        // bug we're checking for should exist in iOS >= 12.2 and < 13, but for\r\n        // whatever reason it's much harder to hit after 12.2 so we only proactively\r\n        // log on 12.2.\r\n        if (iOSVersion === 12.2) {\r\n            logError('Firestore persistence suffers from a bug in iOS 12.2 ' +\r\n                'Safari that may cause your app to stop working. See ' +\r\n                'https://stackoverflow.com/q/56496296/110915 for details ' +\r\n                'and a potential workaround.');\r\n        }\r\n    }\r\n    /** Deletes the specified database. */\r\n    static delete(name) {\r\n        logDebug(LOG_TAG$i, 'Removing database:', name);\r\n        return wrapRequest(window.indexedDB.deleteDatabase(name)).toPromise();\r\n    }\r\n    /** Returns true if IndexedDB is available in the current environment. */\r\n    static isAvailable() {\r\n        if (!isIndexedDBAvailable()) {\r\n            return false;\r\n        }\r\n        if (SimpleDb.isMockPersistence()) {\r\n            return true;\r\n        }\r\n        // We extensively use indexed array values and compound keys,\r\n        // which IE and Edge do not support. However, they still have indexedDB\r\n        // defined on the window, so we need to check for them here and make sure\r\n        // to return that persistence is not enabled for those browsers.\r\n        // For tracking support of this feature, see here:\r\n        // https://developer.microsoft.com/en-us/microsoft-edge/platform/status/indexeddbarraysandmultientrysupport/\r\n        // Check the UA string to find out the browser.\r\n        const ua = getUA();\r\n        // IE 10\r\n        // ua = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)';\r\n        // IE 11\r\n        // ua = 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko';\r\n        // Edge\r\n        // ua = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML,\r\n        // like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0';\r\n        // iOS Safari: Disable for users running iOS version < 10.\r\n        const iOSVersion = SimpleDb.getIOSVersion(ua);\r\n        const isUnsupportedIOS = 0 < iOSVersion && iOSVersion < 10;\r\n        // Android browser: Disable for userse running version < 4.5.\r\n        const androidVersion = SimpleDb.getAndroidVersion(ua);\r\n        const isUnsupportedAndroid = 0 < androidVersion && androidVersion < 4.5;\r\n        if (ua.indexOf('MSIE ') > 0 ||\r\n            ua.indexOf('Trident/') > 0 ||\r\n            ua.indexOf('Edge/') > 0 ||\r\n            isUnsupportedIOS ||\r\n            isUnsupportedAndroid) {\r\n            return false;\r\n        }\r\n        else {\r\n            return true;\r\n        }\r\n    }\r\n    /**\r\n     * Returns true if the backing IndexedDB store is the Node IndexedDBShim\r\n     * (see https://github.com/axemclion/IndexedDBShim).\r\n     */\r\n    static isMockPersistence() {\r\n        var _a;\r\n        return (typeof process !== 'undefined' &&\r\n            ((_a = process.env) === null || _a === void 0 ? void 0 : _a.USE_MOCK_PERSISTENCE) === 'YES');\r\n    }\r\n    /** Helper to get a typed SimpleDbStore from a transaction. */\r\n    static getStore(txn, store) {\r\n        return txn.store(store);\r\n    }\r\n    // visible for testing\r\n    /** Parse User Agent to determine iOS version. Returns -1 if not found. */\r\n    static getIOSVersion(ua) {\r\n        const iOSVersionRegex = ua.match(/i(?:phone|pad|pod) os ([\\d_]+)/i);\r\n        const version = iOSVersionRegex\r\n            ? iOSVersionRegex[1].split('_').slice(0, 2).join('.')\r\n            : '-1';\r\n        return Number(version);\r\n    }\r\n    // visible for testing\r\n    /** Parse User Agent to determine Android version. Returns -1 if not found. */\r\n    static getAndroidVersion(ua) {\r\n        const androidVersionRegex = ua.match(/Android ([\\d.]+)/i);\r\n        const version = androidVersionRegex\r\n            ? androidVersionRegex[1].split('.').slice(0, 2).join('.')\r\n            : '-1';\r\n        return Number(version);\r\n    }\r\n    /**\r\n     * Opens the specified database, creating or upgrading it if necessary.\r\n     */\r\n    async ensureDb(action) {\r\n        if (!this.db) {\r\n            logDebug(LOG_TAG$i, 'Opening database:', this.name);\r\n            this.db = await new Promise((resolve, reject) => {\r\n                // TODO(mikelehen): Investigate browser compatibility.\r\n                // https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API/Using_IndexedDB\r\n                // suggests IE9 and older WebKit browsers handle upgrade\r\n                // differently. They expect setVersion, as described here:\r\n                // https://developer.mozilla.org/en-US/docs/Web/API/IDBVersionChangeRequest/setVersion\r\n                const request = indexedDB.open(this.name, this.version);\r\n                request.onsuccess = (event) => {\r\n                    const db = event.target.result;\r\n                    resolve(db);\r\n                };\r\n                request.onblocked = () => {\r\n                    reject(new IndexedDbTransactionError(action, 'Cannot upgrade IndexedDB schema while another tab is open. ' +\r\n                        'Close all tabs that access Firestore and reload this page to proceed.'));\r\n                };\r\n                request.onerror = (event) => {\r\n                    const error = event.target.error;\r\n                    if (error.name === 'VersionError') {\r\n                        reject(new FirestoreError(Code.FAILED_PRECONDITION, 'A newer version of the Firestore SDK was previously used and so the persisted ' +\r\n                            'data is not compatible with the version of the SDK you are now using. The SDK ' +\r\n                            'will operate with persistence disabled. If you need persistence, please ' +\r\n                            're-upgrade to a newer version of the SDK or else clear the persisted IndexedDB ' +\r\n                            'data for your app to start fresh.'));\r\n                    }\r\n                    else if (error.name === 'InvalidStateError') {\r\n                        reject(new FirestoreError(Code.FAILED_PRECONDITION, 'Unable to open an IndexedDB connection. This could be due to running in a ' +\r\n                            'private browsing session on a browser whose private browsing sessions do not ' +\r\n                            'support IndexedDB: ' +\r\n                            error));\r\n                    }\r\n                    else {\r\n                        reject(new IndexedDbTransactionError(action, error));\r\n                    }\r\n                };\r\n                request.onupgradeneeded = (event) => {\r\n                    logDebug(LOG_TAG$i, 'Database \"' + this.name + '\" requires upgrade from version:', event.oldVersion);\r\n                    const db = event.target.result;\r\n                    this.schemaConverter\r\n                        .createOrUpgrade(db, request.transaction, event.oldVersion, this.version)\r\n                        .next(() => {\r\n                        logDebug(LOG_TAG$i, 'Database upgrade to version ' + this.version + ' complete');\r\n                    });\r\n                };\r\n            });\r\n        }\r\n        if (this.versionchangelistener) {\r\n            this.db.onversionchange = event => this.versionchangelistener(event);\r\n        }\r\n        return this.db;\r\n    }\r\n    setVersionChangeListener(versionChangeListener) {\r\n        this.versionchangelistener = versionChangeListener;\r\n        if (this.db) {\r\n            this.db.onversionchange = (event) => {\r\n                return versionChangeListener(event);\r\n            };\r\n        }\r\n    }\r\n    async runTransaction(action, mode, objectStores, transactionFn) {\r\n        const readonly = mode === 'readonly';\r\n        let attemptNumber = 0;\r\n        while (true) {\r\n            ++attemptNumber;\r\n            try {\r\n                this.db = await this.ensureDb(action);\r\n                const transaction = SimpleDbTransaction.open(this.db, action, readonly ? 'readonly' : 'readwrite', objectStores);\r\n                const transactionFnResult = transactionFn(transaction)\r\n                    .next(result => {\r\n                    transaction.maybeCommit();\r\n                    return result;\r\n                })\r\n                    .catch(error => {\r\n                    // Abort the transaction if there was an error.\r\n                    transaction.abort(error);\r\n                    // We cannot actually recover, and calling `abort()` will cause the transaction's\r\n                    // completion promise to be rejected. This in turn means that we won't use\r\n                    // `transactionFnResult` below. We return a rejection here so that we don't add the\r\n                    // possibility of returning `void` to the type of `transactionFnResult`.\r\n                    return PersistencePromise.reject(error);\r\n                })\r\n                    .toPromise();\r\n                // As noted above, errors are propagated by aborting the transaction. So\r\n                // we swallow any error here to avoid the browser logging it as unhandled.\r\n                transactionFnResult.catch(() => { });\r\n                // Wait for the transaction to complete (i.e. IndexedDb's onsuccess event to\r\n                // fire), but still return the original transactionFnResult back to the\r\n                // caller.\r\n                await transaction.completionPromise;\r\n                return transactionFnResult;\r\n            }\r\n            catch (e) {\r\n                const error = e;\r\n                // TODO(schmidt-sebastian): We could probably be smarter about this and\r\n                // not retry exceptions that are likely unrecoverable (such as quota\r\n                // exceeded errors).\r\n                // Note: We cannot use an instanceof check for FirestoreException, since the\r\n                // exception is wrapped in a generic error by our async/await handling.\r\n                const retryable = error.name !== 'FirebaseError' &&\r\n                    attemptNumber < TRANSACTION_RETRY_COUNT;\r\n                logDebug(LOG_TAG$i, 'Transaction failed with error:', error.message, 'Retrying:', retryable);\r\n                this.close();\r\n                if (!retryable) {\r\n                    return Promise.reject(error);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    close() {\r\n        if (this.db) {\r\n            this.db.close();\r\n        }\r\n        this.db = undefined;\r\n    }\r\n}\r\n/**\r\n * A controller for iterating over a key range or index. It allows an iterate\r\n * callback to delete the currently-referenced object, or jump to a new key\r\n * within the key range or index.\r\n */\r\nclass IterationController {\r\n    constructor(dbCursor) {\r\n        this.dbCursor = dbCursor;\r\n        this.shouldStop = false;\r\n        this.nextKey = null;\r\n    }\r\n    get isDone() {\r\n        return this.shouldStop;\r\n    }\r\n    get skipToKey() {\r\n        return this.nextKey;\r\n    }\r\n    set cursor(value) {\r\n        this.dbCursor = value;\r\n    }\r\n    /**\r\n     * This function can be called to stop iteration at any point.\r\n     */\r\n    done() {\r\n        this.shouldStop = true;\r\n    }\r\n    /**\r\n     * This function can be called to skip to that next key, which could be\r\n     * an index or a primary key.\r\n     */\r\n    skip(key) {\r\n        this.nextKey = key;\r\n    }\r\n    /**\r\n     * Delete the current cursor value from the object store.\r\n     *\r\n     * NOTE: You CANNOT do this with a keysOnly query.\r\n     */\r\n    delete() {\r\n        return wrapRequest(this.dbCursor.delete());\r\n    }\r\n}\r\n/** An error that wraps exceptions that thrown during IndexedDB execution. */\r\nclass IndexedDbTransactionError extends FirestoreError {\r\n    constructor(actionName, cause) {\r\n        super(Code.UNAVAILABLE, `IndexedDB transaction '${actionName}' failed: ${cause}`);\r\n        this.name = 'IndexedDbTransactionError';\r\n    }\r\n}\r\n/** Verifies whether `e` is an IndexedDbTransactionError. */\r\nfunction isIndexedDbTransactionError(e) {\r\n    // Use name equality, as instanceof checks on errors don't work with errors\r\n    // that wrap other errors.\r\n    return e.name === 'IndexedDbTransactionError';\r\n}\r\n/**\r\n * A wrapper around an IDBObjectStore providing an API that:\r\n *\r\n * 1) Has generic KeyType / ValueType parameters to provide strongly-typed\r\n * methods for acting against the object store.\r\n * 2) Deals with IndexedDB's onsuccess / onerror event callbacks, making every\r\n * method return a PersistencePromise instead.\r\n * 3) Provides a higher-level API to avoid needing to do excessive wrapping of\r\n * intermediate IndexedDB types (IDBCursorWithValue, etc.)\r\n */\r\nclass SimpleDbStore {\r\n    constructor(store) {\r\n        this.store = store;\r\n    }\r\n    put(keyOrValue, value) {\r\n        let request;\r\n        if (value !== undefined) {\r\n            logDebug(LOG_TAG$i, 'PUT', this.store.name, keyOrValue, value);\r\n            request = this.store.put(value, keyOrValue);\r\n        }\r\n        else {\r\n            logDebug(LOG_TAG$i, 'PUT', this.store.name, '<auto-key>', keyOrValue);\r\n            request = this.store.put(keyOrValue);\r\n        }\r\n        return wrapRequest(request);\r\n    }\r\n    /**\r\n     * Adds a new value into an Object Store and returns the new key. Similar to\r\n     * IndexedDb's `add()`, this method will fail on primary key collisions.\r\n     *\r\n     * @param value - The object to write.\r\n     * @returns The key of the value to add.\r\n     */\r\n    add(value) {\r\n        logDebug(LOG_TAG$i, 'ADD', this.store.name, value, value);\r\n        const request = this.store.add(value);\r\n        return wrapRequest(request);\r\n    }\r\n    /**\r\n     * Gets the object with the specified key from the specified store, or null\r\n     * if no object exists with the specified key.\r\n     *\r\n     * @key The key of the object to get.\r\n     * @returns The object with the specified key or null if no object exists.\r\n     */\r\n    get(key) {\r\n        const request = this.store.get(key);\r\n        // We're doing an unsafe cast to ValueType.\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        return wrapRequest(request).next(result => {\r\n            // Normalize nonexistence to null.\r\n            if (result === undefined) {\r\n                result = null;\r\n            }\r\n            logDebug(LOG_TAG$i, 'GET', this.store.name, key, result);\r\n            return result;\r\n        });\r\n    }\r\n    delete(key) {\r\n        logDebug(LOG_TAG$i, 'DELETE', this.store.name, key);\r\n        const request = this.store.delete(key);\r\n        return wrapRequest(request);\r\n    }\r\n    /**\r\n     * If we ever need more of the count variants, we can add overloads. For now,\r\n     * all we need is to count everything in a store.\r\n     *\r\n     * Returns the number of rows in the store.\r\n     */\r\n    count() {\r\n        logDebug(LOG_TAG$i, 'COUNT', this.store.name);\r\n        const request = this.store.count();\r\n        return wrapRequest(request);\r\n    }\r\n    loadAll(indexOrRange, range) {\r\n        const iterateOptions = this.options(indexOrRange, range);\r\n        // Use `getAll()` if the browser supports IndexedDB v3, as it is roughly\r\n        // 20% faster. Unfortunately, getAll() does not support custom indices.\r\n        if (!iterateOptions.index && typeof this.store.getAll === 'function') {\r\n            const request = this.store.getAll(iterateOptions.range);\r\n            return new PersistencePromise((resolve, reject) => {\r\n                request.onerror = (event) => {\r\n                    reject(event.target.error);\r\n                };\r\n                request.onsuccess = (event) => {\r\n                    resolve(event.target.result);\r\n                };\r\n            });\r\n        }\r\n        else {\r\n            const cursor = this.cursor(iterateOptions);\r\n            const results = [];\r\n            return this.iterateCursor(cursor, (key, value) => {\r\n                results.push(value);\r\n            }).next(() => {\r\n                return results;\r\n            });\r\n        }\r\n    }\r\n    /**\r\n     * Loads the first `count` elements from the provided index range. Loads all\r\n     * elements if no limit is provided.\r\n     */\r\n    loadFirst(range, count) {\r\n        const request = this.store.getAll(range, count === null ? undefined : count);\r\n        return new PersistencePromise((resolve, reject) => {\r\n            request.onerror = (event) => {\r\n                reject(event.target.error);\r\n            };\r\n            request.onsuccess = (event) => {\r\n                resolve(event.target.result);\r\n            };\r\n        });\r\n    }\r\n    deleteAll(indexOrRange, range) {\r\n        logDebug(LOG_TAG$i, 'DELETE ALL', this.store.name);\r\n        const options = this.options(indexOrRange, range);\r\n        options.keysOnly = false;\r\n        const cursor = this.cursor(options);\r\n        return this.iterateCursor(cursor, (key, value, control) => {\r\n            // NOTE: Calling delete() on a cursor is documented as more efficient than\r\n            // calling delete() on an object store with a single key\r\n            // (https://developer.mozilla.org/en-US/docs/Web/API/IDBObjectStore/delete),\r\n            // however, this requires us *not* to use a keysOnly cursor\r\n            // (https://developer.mozilla.org/en-US/docs/Web/API/IDBCursor/delete). We\r\n            // may want to compare the performance of each method.\r\n            return control.delete();\r\n        });\r\n    }\r\n    iterate(optionsOrCallback, callback) {\r\n        let options;\r\n        if (!callback) {\r\n            options = {};\r\n            callback = optionsOrCallback;\r\n        }\r\n        else {\r\n            options = optionsOrCallback;\r\n        }\r\n        const cursor = this.cursor(options);\r\n        return this.iterateCursor(cursor, callback);\r\n    }\r\n    /**\r\n     * Iterates over a store, but waits for the given callback to complete for\r\n     * each entry before iterating the next entry. This allows the callback to do\r\n     * asynchronous work to determine if this iteration should continue.\r\n     *\r\n     * The provided callback should return `true` to continue iteration, and\r\n     * `false` otherwise.\r\n     */\r\n    iterateSerial(callback) {\r\n        const cursorRequest = this.cursor({});\r\n        return new PersistencePromise((resolve, reject) => {\r\n            cursorRequest.onerror = (event) => {\r\n                const error = checkForAndReportiOSError(event.target.error);\r\n                reject(error);\r\n            };\r\n            cursorRequest.onsuccess = (event) => {\r\n                const cursor = event.target.result;\r\n                if (!cursor) {\r\n                    resolve();\r\n                    return;\r\n                }\r\n                callback(cursor.primaryKey, cursor.value).next(shouldContinue => {\r\n                    if (shouldContinue) {\r\n                        cursor.continue();\r\n                    }\r\n                    else {\r\n                        resolve();\r\n                    }\r\n                });\r\n            };\r\n        });\r\n    }\r\n    iterateCursor(cursorRequest, fn) {\r\n        const results = [];\r\n        return new PersistencePromise((resolve, reject) => {\r\n            cursorRequest.onerror = (event) => {\r\n                reject(event.target.error);\r\n            };\r\n            cursorRequest.onsuccess = (event) => {\r\n                const cursor = event.target.result;\r\n                if (!cursor) {\r\n                    resolve();\r\n                    return;\r\n                }\r\n                const controller = new IterationController(cursor);\r\n                const userResult = fn(cursor.primaryKey, cursor.value, controller);\r\n                if (userResult instanceof PersistencePromise) {\r\n                    const userPromise = userResult.catch(err => {\r\n                        controller.done();\r\n                        return PersistencePromise.reject(err);\r\n                    });\r\n                    results.push(userPromise);\r\n                }\r\n                if (controller.isDone) {\r\n                    resolve();\r\n                }\r\n                else if (controller.skipToKey === null) {\r\n                    cursor.continue();\r\n                }\r\n                else {\r\n                    cursor.continue(controller.skipToKey);\r\n                }\r\n            };\r\n        }).next(() => PersistencePromise.waitFor(results));\r\n    }\r\n    options(indexOrRange, range) {\r\n        let indexName = undefined;\r\n        if (indexOrRange !== undefined) {\r\n            if (typeof indexOrRange === 'string') {\r\n                indexName = indexOrRange;\r\n            }\r\n            else {\r\n                range = indexOrRange;\r\n            }\r\n        }\r\n        return { index: indexName, range };\r\n    }\r\n    cursor(options) {\r\n        let direction = 'next';\r\n        if (options.reverse) {\r\n            direction = 'prev';\r\n        }\r\n        if (options.index) {\r\n            const index = this.store.index(options.index);\r\n            if (options.keysOnly) {\r\n                return index.openKeyCursor(options.range, direction);\r\n            }\r\n            else {\r\n                return index.openCursor(options.range, direction);\r\n            }\r\n        }\r\n        else {\r\n            return this.store.openCursor(options.range, direction);\r\n        }\r\n    }\r\n}\r\n/**\r\n * Wraps an IDBRequest in a PersistencePromise, using the onsuccess / onerror\r\n * handlers to resolve / reject the PersistencePromise as appropriate.\r\n */\r\nfunction wrapRequest(request) {\r\n    return new PersistencePromise((resolve, reject) => {\r\n        request.onsuccess = (event) => {\r\n            const result = event.target.result;\r\n            resolve(result);\r\n        };\r\n        request.onerror = (event) => {\r\n            const error = checkForAndReportiOSError(event.target.error);\r\n            reject(error);\r\n        };\r\n    });\r\n}\r\n// Guard so we only report the error once.\r\nlet reportedIOSError = false;\r\nfunction checkForAndReportiOSError(error) {\r\n    const iOSVersion = SimpleDb.getIOSVersion(getUA());\r\n    if (iOSVersion >= 12.2 && iOSVersion < 13) {\r\n        const IOS_ERROR = 'An internal error was encountered in the Indexed Database server';\r\n        if (error.message.indexOf(IOS_ERROR) >= 0) {\r\n            // Wrap error in a more descriptive one.\r\n            const newError = new FirestoreError('internal', `IOS_INDEXEDDB_BUG1: IndexedDb has thrown '${IOS_ERROR}'. This is likely ` +\r\n                `due to an unavoidable bug in iOS. See https://stackoverflow.com/q/56496296/110915 ` +\r\n                `for details and a potential workaround.`);\r\n            if (!reportedIOSError) {\r\n                reportedIOSError = true;\r\n                // Throw a global exception outside of this promise chain, for the user to\r\n                // potentially catch.\r\n                setTimeout(() => {\r\n                    throw newError;\r\n                }, 0);\r\n            }\r\n            return newError;\r\n        }\r\n    }\r\n    return error;\r\n}\n\nconst LOG_TAG$h = 'IndexBackiller';\r\n/** How long we wait to try running index backfill after SDK initialization. */\r\nconst INITIAL_BACKFILL_DELAY_MS = 15 * 1000;\r\n/** Minimum amount of time between backfill checks, after the first one. */\r\nconst REGULAR_BACKFILL_DELAY_MS = 60 * 1000;\r\n/** The maximum number of documents to process each time backfill() is called. */\r\nconst MAX_DOCUMENTS_TO_PROCESS = 50;\r\n/** This class is responsible for the scheduling of Index Backfiller. */\r\nclass IndexBackfillerScheduler {\r\n    constructor(asyncQueue, backfiller) {\r\n        this.asyncQueue = asyncQueue;\r\n        this.backfiller = backfiller;\r\n        this.task = null;\r\n    }\r\n    start() {\r\n        this.schedule(INITIAL_BACKFILL_DELAY_MS);\r\n    }\r\n    stop() {\r\n        if (this.task) {\r\n            this.task.cancel();\r\n            this.task = null;\r\n        }\r\n    }\r\n    get started() {\r\n        return this.task !== null;\r\n    }\r\n    schedule(delay) {\r\n        logDebug(LOG_TAG$h, `Scheduled in ${delay}ms`);\r\n        this.task = this.asyncQueue.enqueueAfterDelay(\"index_backfill\" /* TimerId.IndexBackfill */, delay, async () => {\r\n            this.task = null;\r\n            try {\r\n                const documentsProcessed = await this.backfiller.backfill();\r\n                logDebug(LOG_TAG$h, `Documents written: ${documentsProcessed}`);\r\n            }\r\n            catch (e) {\r\n                if (isIndexedDbTransactionError(e)) {\r\n                    logDebug(LOG_TAG$h, 'Ignoring IndexedDB error during index backfill: ', e);\r\n                }\r\n                else {\r\n                    await ignoreIfPrimaryLeaseLoss(e);\r\n                }\r\n            }\r\n            await this.schedule(REGULAR_BACKFILL_DELAY_MS);\r\n        });\r\n    }\r\n}\r\n/** Implements the steps for backfilling indexes. */\r\nclass IndexBackfiller {\r\n    constructor(\r\n    /**\r\n     * LocalStore provides access to IndexManager and LocalDocumentView.\r\n     * These properties will update when the user changes. Consequently,\r\n     * making a local copy of IndexManager and LocalDocumentView will require\r\n     * updates over time. The simpler solution is to rely on LocalStore to have\r\n     * an up-to-date references to IndexManager and LocalDocumentStore.\r\n     */\r\n    localStore, persistence) {\r\n        this.localStore = localStore;\r\n        this.persistence = persistence;\r\n    }\r\n    async backfill(maxDocumentsToProcess = MAX_DOCUMENTS_TO_PROCESS) {\r\n        return this.persistence.runTransaction('Backfill Indexes', 'readwrite-primary', txn => this.writeIndexEntries(txn, maxDocumentsToProcess));\r\n    }\r\n    /** Writes index entries until the cap is reached. Returns the number of documents processed. */\r\n    writeIndexEntries(transation, maxDocumentsToProcess) {\r\n        const processedCollectionGroups = new Set();\r\n        let documentsRemaining = maxDocumentsToProcess;\r\n        let continueLoop = true;\r\n        return PersistencePromise.doWhile(() => continueLoop === true && documentsRemaining > 0, () => {\r\n            return this.localStore.indexManager\r\n                .getNextCollectionGroupToUpdate(transation)\r\n                .next((collectionGroup) => {\r\n                if (collectionGroup === null ||\r\n                    processedCollectionGroups.has(collectionGroup)) {\r\n                    continueLoop = false;\r\n                }\r\n                else {\r\n                    logDebug(LOG_TAG$h, `Processing collection: ${collectionGroup}`);\r\n                    return this.writeEntriesForCollectionGroup(transation, collectionGroup, documentsRemaining).next(documentsProcessed => {\r\n                        documentsRemaining -= documentsProcessed;\r\n                        processedCollectionGroups.add(collectionGroup);\r\n                    });\r\n                }\r\n            });\r\n        }).next(() => maxDocumentsToProcess - documentsRemaining);\r\n    }\r\n    /**\r\n     * Writes entries for the provided collection group. Returns the number of documents processed.\r\n     */\r\n    writeEntriesForCollectionGroup(transaction, collectionGroup, documentsRemainingUnderCap) {\r\n        // Use the earliest offset of all field indexes to query the local cache.\r\n        return this.localStore.indexManager\r\n            .getMinOffsetFromCollectionGroup(transaction, collectionGroup)\r\n            .next(existingOffset => this.localStore.localDocuments\r\n            .getNextDocuments(transaction, collectionGroup, existingOffset, documentsRemainingUnderCap)\r\n            .next(nextBatch => {\r\n            const docs = nextBatch.changes;\r\n            return this.localStore.indexManager\r\n                .updateIndexEntries(transaction, docs)\r\n                .next(() => this.getNewOffset(existingOffset, nextBatch))\r\n                .next(newOffset => {\r\n                logDebug(LOG_TAG$h, `Updating offset: ${newOffset}`);\r\n                return this.localStore.indexManager.updateCollectionGroup(transaction, collectionGroup, newOffset);\r\n            })\r\n                .next(() => docs.size);\r\n        }));\r\n    }\r\n    /** Returns the next offset based on the provided documents. */\r\n    getNewOffset(existingOffset, lookupResult) {\r\n        let maxOffset = existingOffset;\r\n        lookupResult.changes.forEach((key, document) => {\r\n            const newOffset = newIndexOffsetFromDocument(document);\r\n            if (indexOffsetComparator(newOffset, maxOffset) > 0) {\r\n                maxOffset = newOffset;\r\n            }\r\n        });\r\n        return new IndexOffset(maxOffset.readTime, maxOffset.documentKey, Math.max(lookupResult.batchId, existingOffset.largestBatchId));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * `ListenSequence` is a monotonic sequence. It is initialized with a minimum value to\r\n * exceed. All subsequent calls to next will return increasing values. If provided with a\r\n * `SequenceNumberSyncer`, it will additionally bump its next value when told of a new value, as\r\n * well as write out sequence numbers that it produces via `next()`.\r\n */\r\nclass ListenSequence {\r\n    constructor(previousValue, sequenceNumberSyncer) {\r\n        this.previousValue = previousValue;\r\n        if (sequenceNumberSyncer) {\r\n            sequenceNumberSyncer.sequenceNumberHandler = sequenceNumber => this.setPreviousValue(sequenceNumber);\r\n            this.writeNewSequenceNumber = sequenceNumber => sequenceNumberSyncer.writeSequenceNumber(sequenceNumber);\r\n        }\r\n    }\r\n    setPreviousValue(externalPreviousValue) {\r\n        this.previousValue = Math.max(externalPreviousValue, this.previousValue);\r\n        return this.previousValue;\r\n    }\r\n    next() {\r\n        const nextValue = ++this.previousValue;\r\n        if (this.writeNewSequenceNumber) {\r\n            this.writeNewSequenceNumber(nextValue);\r\n        }\r\n        return nextValue;\r\n    }\r\n}\r\nListenSequence.INVALID = -1;\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst escapeChar = '\\u0001';\r\nconst encodedSeparatorChar = '\\u0001';\r\nconst encodedNul = '\\u0010';\r\nconst encodedEscape = '\\u0011';\r\n/**\r\n * Encodes a resource path into a IndexedDb-compatible string form.\r\n */\r\nfunction encodeResourcePath(path) {\r\n    let result = '';\r\n    for (let i = 0; i < path.length; i++) {\r\n        if (result.length > 0) {\r\n            result = encodeSeparator(result);\r\n        }\r\n        result = encodeSegment(path.get(i), result);\r\n    }\r\n    return encodeSeparator(result);\r\n}\r\n/** Encodes a single segment of a resource path into the given result */\r\nfunction encodeSegment(segment, resultBuf) {\r\n    let result = resultBuf;\r\n    const length = segment.length;\r\n    for (let i = 0; i < length; i++) {\r\n        const c = segment.charAt(i);\r\n        switch (c) {\r\n            case '\\0':\r\n                result += escapeChar + encodedNul;\r\n                break;\r\n            case escapeChar:\r\n                result += escapeChar + encodedEscape;\r\n                break;\r\n            default:\r\n                result += c;\r\n        }\r\n    }\r\n    return result;\r\n}\r\n/** Encodes a path separator into the given result */\r\nfunction encodeSeparator(result) {\r\n    return result + escapeChar + encodedSeparatorChar;\r\n}\r\n/**\r\n * Decodes the given IndexedDb-compatible string form of a resource path into\r\n * a ResourcePath instance. Note that this method is not suitable for use with\r\n * decoding resource names from the server; those are One Platform format\r\n * strings.\r\n */\r\nfunction decodeResourcePath(path) {\r\n    // Event the empty path must encode as a path of at least length 2. A path\r\n    // with exactly 2 must be the empty path.\r\n    const length = path.length;\r\n    hardAssert(length >= 2);\r\n    if (length === 2) {\r\n        hardAssert(path.charAt(0) === escapeChar && path.charAt(1) === encodedSeparatorChar);\r\n        return ResourcePath.emptyPath();\r\n    }\r\n    // Escape characters cannot exist past the second-to-last position in the\r\n    // source value.\r\n    const lastReasonableEscapeIndex = length - 2;\r\n    const segments = [];\r\n    let segmentBuilder = '';\r\n    for (let start = 0; start < length;) {\r\n        // The last two characters of a valid encoded path must be a separator, so\r\n        // there must be an end to this segment.\r\n        const end = path.indexOf(escapeChar, start);\r\n        if (end < 0 || end > lastReasonableEscapeIndex) {\r\n            fail();\r\n        }\r\n        const next = path.charAt(end + 1);\r\n        switch (next) {\r\n            case encodedSeparatorChar:\r\n                const currentPiece = path.substring(start, end);\r\n                let segment;\r\n                if (segmentBuilder.length === 0) {\r\n                    // Avoid copying for the common case of a segment that excludes \\0\r\n                    // and \\001\r\n                    segment = currentPiece;\r\n                }\r\n                else {\r\n                    segmentBuilder += currentPiece;\r\n                    segment = segmentBuilder;\r\n                    segmentBuilder = '';\r\n                }\r\n                segments.push(segment);\r\n                break;\r\n            case encodedNul:\r\n                segmentBuilder += path.substring(start, end);\r\n                segmentBuilder += '\\0';\r\n                break;\r\n            case encodedEscape:\r\n                // The escape character can be used in the output to encode itself.\r\n                segmentBuilder += path.substring(start, end + 1);\r\n                break;\r\n            default:\r\n                fail();\r\n        }\r\n        start = end + 2;\r\n    }\r\n    return new ResourcePath(segments);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst DbRemoteDocumentStore$1 = 'remoteDocuments';\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Name of the IndexedDb object store.\r\n *\r\n * Note that the name 'owner' is chosen to ensure backwards compatibility with\r\n * older clients that only supported single locked access to the persistence\r\n * layer.\r\n */\r\nconst DbPrimaryClientStore = 'owner';\r\n/**\r\n * The key string used for the single object that exists in the\r\n * DbPrimaryClient store.\r\n */\r\nconst DbPrimaryClientKey = 'owner';\r\n/** Name of the IndexedDb object store.  */\r\nconst DbMutationQueueStore = 'mutationQueues';\r\n/** Keys are automatically assigned via the userId property. */\r\nconst DbMutationQueueKeyPath = 'userId';\r\n/** Name of the IndexedDb object store.  */\r\nconst DbMutationBatchStore = 'mutations';\r\n/** Keys are automatically assigned via the userId, batchId properties. */\r\nconst DbMutationBatchKeyPath = 'batchId';\r\n/** The index name for lookup of mutations by user. */\r\nconst DbMutationBatchUserMutationsIndex = 'userMutationsIndex';\r\n/** The user mutations index is keyed by [userId, batchId] pairs. */\r\nconst DbMutationBatchUserMutationsKeyPath = ['userId', 'batchId'];\r\n/**\r\n * Creates a [userId] key for use in the DbDocumentMutations index to iterate\r\n * over all of a user's document mutations.\r\n */\r\nfunction newDbDocumentMutationPrefixForUser(userId) {\r\n    return [userId];\r\n}\r\n/**\r\n * Creates a [userId, encodedPath] key for use in the DbDocumentMutations\r\n * index to iterate over all at document mutations for a given path or lower.\r\n */\r\nfunction newDbDocumentMutationPrefixForPath(userId, path) {\r\n    return [userId, encodeResourcePath(path)];\r\n}\r\n/**\r\n * Creates a full index key of [userId, encodedPath, batchId] for inserting\r\n * and deleting into the DbDocumentMutations index.\r\n */\r\nfunction newDbDocumentMutationKey(userId, path, batchId) {\r\n    return [userId, encodeResourcePath(path), batchId];\r\n}\r\n/**\r\n * Because we store all the useful information for this store in the key,\r\n * there is no useful information to store as the value. The raw (unencoded)\r\n * path cannot be stored because IndexedDb doesn't store prototype\r\n * information.\r\n */\r\nconst DbDocumentMutationPlaceholder = {};\r\nconst DbDocumentMutationStore = 'documentMutations';\r\nconst DbRemoteDocumentStore = 'remoteDocumentsV14';\r\n/**\r\n * The primary key of the remote documents store, which allows for efficient\r\n * access by collection path and read time.\r\n */\r\nconst DbRemoteDocumentKeyPath = [\r\n    'prefixPath',\r\n    'collectionGroup',\r\n    'readTime',\r\n    'documentId'\r\n];\r\n/** An index that provides access to documents by key. */\r\nconst DbRemoteDocumentDocumentKeyIndex = 'documentKeyIndex';\r\nconst DbRemoteDocumentDocumentKeyIndexPath = [\r\n    'prefixPath',\r\n    'collectionGroup',\r\n    'documentId'\r\n];\r\n/**\r\n * An index that provides access to documents by collection group and read\r\n * time.\r\n *\r\n * This index is used by the index backfiller.\r\n */\r\nconst DbRemoteDocumentCollectionGroupIndex = 'collectionGroupIndex';\r\nconst DbRemoteDocumentCollectionGroupIndexPath = [\r\n    'collectionGroup',\r\n    'readTime',\r\n    'prefixPath',\r\n    'documentId'\r\n];\r\nconst DbRemoteDocumentGlobalStore = 'remoteDocumentGlobal';\r\nconst DbRemoteDocumentGlobalKey = 'remoteDocumentGlobalKey';\r\nconst DbTargetStore = 'targets';\r\n/** Keys are automatically assigned via the targetId property. */\r\nconst DbTargetKeyPath = 'targetId';\r\n/** The name of the queryTargets index. */\r\nconst DbTargetQueryTargetsIndexName = 'queryTargetsIndex';\r\n/**\r\n * The index of all canonicalIds to the targets that they match. This is not\r\n * a unique mapping because canonicalId does not promise a unique name for all\r\n * possible queries, so we append the targetId to make the mapping unique.\r\n */\r\nconst DbTargetQueryTargetsKeyPath = ['canonicalId', 'targetId'];\r\n/** Name of the IndexedDb object store.  */\r\nconst DbTargetDocumentStore = 'targetDocuments';\r\n/** Keys are automatically assigned via the targetId, path properties. */\r\nconst DbTargetDocumentKeyPath = ['targetId', 'path'];\r\n/** The index name for the reverse index. */\r\nconst DbTargetDocumentDocumentTargetsIndex = 'documentTargetsIndex';\r\n/** We also need to create the reverse index for these properties. */\r\nconst DbTargetDocumentDocumentTargetsKeyPath = ['path', 'targetId'];\r\n/**\r\n * The key string used for the single object that exists in the\r\n * DbTargetGlobal store.\r\n */\r\nconst DbTargetGlobalKey = 'targetGlobalKey';\r\nconst DbTargetGlobalStore = 'targetGlobal';\r\n/** Name of the IndexedDb object store. */\r\nconst DbCollectionParentStore = 'collectionParents';\r\n/** Keys are automatically assigned via the collectionId, parent properties. */\r\nconst DbCollectionParentKeyPath = ['collectionId', 'parent'];\r\n/** Name of the IndexedDb object store. */\r\nconst DbClientMetadataStore = 'clientMetadata';\r\n/** Keys are automatically assigned via the clientId properties. */\r\nconst DbClientMetadataKeyPath = 'clientId';\r\n/** Name of the IndexedDb object store. */\r\nconst DbBundleStore = 'bundles';\r\nconst DbBundleKeyPath = 'bundleId';\r\n/** Name of the IndexedDb object store. */\r\nconst DbNamedQueryStore = 'namedQueries';\r\nconst DbNamedQueryKeyPath = 'name';\r\n/** Name of the IndexedDb object store. */\r\nconst DbIndexConfigurationStore = 'indexConfiguration';\r\nconst DbIndexConfigurationKeyPath = 'indexId';\r\n/**\r\n * An index that provides access to the index configurations by collection\r\n * group.\r\n *\r\n * PORTING NOTE: iOS and Android maintain this index in-memory, but this is\r\n * not possible here as the Web client supports concurrent access to\r\n * persistence via multi-tab.\r\n */\r\nconst DbIndexConfigurationCollectionGroupIndex = 'collectionGroupIndex';\r\nconst DbIndexConfigurationCollectionGroupIndexPath = 'collectionGroup';\r\n/** Name of the IndexedDb object store. */\r\nconst DbIndexStateStore = 'indexState';\r\nconst DbIndexStateKeyPath = ['indexId', 'uid'];\r\n/**\r\n * An index that provides access to documents in a collection sorted by last\r\n * update time. Used by the backfiller.\r\n *\r\n * PORTING NOTE: iOS and Android maintain this index in-memory, but this is\r\n * not possible here as the Web client supports concurrent access to\r\n * persistence via multi-tab.\r\n */\r\nconst DbIndexStateSequenceNumberIndex = 'sequenceNumberIndex';\r\nconst DbIndexStateSequenceNumberIndexPath = ['uid', 'sequenceNumber'];\r\n/** Name of the IndexedDb object store. */\r\nconst DbIndexEntryStore = 'indexEntries';\r\nconst DbIndexEntryKeyPath = [\r\n    'indexId',\r\n    'uid',\r\n    'arrayValue',\r\n    'directionalValue',\r\n    'orderedDocumentKey',\r\n    'documentKey'\r\n];\r\nconst DbIndexEntryDocumentKeyIndex = 'documentKeyIndex';\r\nconst DbIndexEntryDocumentKeyIndexPath = [\r\n    'indexId',\r\n    'uid',\r\n    'orderedDocumentKey'\r\n];\r\n/** Name of the IndexedDb object store. */\r\nconst DbDocumentOverlayStore = 'documentOverlays';\r\nconst DbDocumentOverlayKeyPath = [\r\n    'userId',\r\n    'collectionPath',\r\n    'documentId'\r\n];\r\nconst DbDocumentOverlayCollectionPathOverlayIndex = 'collectionPathOverlayIndex';\r\nconst DbDocumentOverlayCollectionPathOverlayIndexPath = [\r\n    'userId',\r\n    'collectionPath',\r\n    'largestBatchId'\r\n];\r\nconst DbDocumentOverlayCollectionGroupOverlayIndex = 'collectionGroupOverlayIndex';\r\nconst DbDocumentOverlayCollectionGroupOverlayIndexPath = [\r\n    'userId',\r\n    'collectionGroup',\r\n    'largestBatchId'\r\n];\r\n// Visible for testing\r\nconst V1_STORES = [\r\n    DbMutationQueueStore,\r\n    DbMutationBatchStore,\r\n    DbDocumentMutationStore,\r\n    DbRemoteDocumentStore$1,\r\n    DbTargetStore,\r\n    DbPrimaryClientStore,\r\n    DbTargetGlobalStore,\r\n    DbTargetDocumentStore\r\n];\r\n// Visible for testing\r\nconst V3_STORES = V1_STORES;\r\n// Note: DbRemoteDocumentChanges is no longer used and dropped with v9.\r\nconst V4_STORES = [...V3_STORES, DbClientMetadataStore];\r\nconst V6_STORES = [...V4_STORES, DbRemoteDocumentGlobalStore];\r\nconst V8_STORES = [...V6_STORES, DbCollectionParentStore];\r\nconst V11_STORES = [...V8_STORES, DbBundleStore, DbNamedQueryStore];\r\nconst V12_STORES = [...V11_STORES, DbDocumentOverlayStore];\r\nconst V13_STORES = [\r\n    DbMutationQueueStore,\r\n    DbMutationBatchStore,\r\n    DbDocumentMutationStore,\r\n    DbRemoteDocumentStore,\r\n    DbTargetStore,\r\n    DbPrimaryClientStore,\r\n    DbTargetGlobalStore,\r\n    DbTargetDocumentStore,\r\n    DbClientMetadataStore,\r\n    DbRemoteDocumentGlobalStore,\r\n    DbCollectionParentStore,\r\n    DbBundleStore,\r\n    DbNamedQueryStore,\r\n    DbDocumentOverlayStore\r\n];\r\nconst V14_STORES = V13_STORES;\r\nconst V15_STORES = [\r\n    ...V14_STORES,\r\n    DbIndexConfigurationStore,\r\n    DbIndexStateStore,\r\n    DbIndexEntryStore\r\n];\r\n/** Returns the object stores for the provided schema. */\r\nfunction getObjectStores(schemaVersion) {\r\n    if (schemaVersion === 15) {\r\n        return V15_STORES;\r\n    }\r\n    else if (schemaVersion === 14) {\r\n        return V14_STORES;\r\n    }\r\n    else if (schemaVersion === 13) {\r\n        return V13_STORES;\r\n    }\r\n    else if (schemaVersion === 12) {\r\n        return V12_STORES;\r\n    }\r\n    else if (schemaVersion === 11) {\r\n        return V11_STORES;\r\n    }\r\n    else {\r\n        fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass IndexedDbTransaction extends PersistenceTransaction {\r\n    constructor(simpleDbTransaction, currentSequenceNumber) {\r\n        super();\r\n        this.simpleDbTransaction = simpleDbTransaction;\r\n        this.currentSequenceNumber = currentSequenceNumber;\r\n    }\r\n}\r\nfunction getStore(txn, store) {\r\n    const indexedDbTransaction = debugCast(txn);\r\n    return SimpleDb.getStore(indexedDbTransaction.simpleDbTransaction, store);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nfunction objectSize(obj) {\r\n    let count = 0;\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}\r\nfunction forEach(obj, fn) {\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            fn(key, obj[key]);\r\n        }\r\n    }\r\n}\r\nfunction mapToArray(obj, fn) {\r\n    const result = [];\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            result.push(fn(obj[key], key, obj));\r\n        }\r\n    }\r\n    return result;\r\n}\r\nfunction isEmpty(obj) {\r\n    for (const key in obj) {\r\n        if (Object.prototype.hasOwnProperty.call(obj, key)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// An immutable sorted map implementation, based on a Left-leaning Red-Black\r\n// tree.\r\nclass SortedMap {\r\n    constructor(comparator, root) {\r\n        this.comparator = comparator;\r\n        this.root = root ? root : LLRBNode.EMPTY;\r\n    }\r\n    // Returns a copy of the map, with the specified key/value added or replaced.\r\n    insert(key, value) {\r\n        return new SortedMap(this.comparator, this.root\r\n            .insert(key, value, this.comparator)\r\n            .copy(null, null, LLRBNode.BLACK, null, null));\r\n    }\r\n    // Returns a copy of the map, with the specified key removed.\r\n    remove(key) {\r\n        return new SortedMap(this.comparator, this.root\r\n            .remove(key, this.comparator)\r\n            .copy(null, null, LLRBNode.BLACK, null, null));\r\n    }\r\n    // Returns the value of the node with the given key, or null.\r\n    get(key) {\r\n        let node = this.root;\r\n        while (!node.isEmpty()) {\r\n            const cmp = this.comparator(key, node.key);\r\n            if (cmp === 0) {\r\n                return node.value;\r\n            }\r\n            else if (cmp < 0) {\r\n                node = node.left;\r\n            }\r\n            else if (cmp > 0) {\r\n                node = node.right;\r\n            }\r\n        }\r\n        return null;\r\n    }\r\n    // Returns the index of the element in this sorted map, or -1 if it doesn't\r\n    // exist.\r\n    indexOf(key) {\r\n        // Number of nodes that were pruned when descending right\r\n        let prunedNodes = 0;\r\n        let node = this.root;\r\n        while (!node.isEmpty()) {\r\n            const cmp = this.comparator(key, node.key);\r\n            if (cmp === 0) {\r\n                return prunedNodes + node.left.size;\r\n            }\r\n            else if (cmp < 0) {\r\n                node = node.left;\r\n            }\r\n            else {\r\n                // Count all nodes left of the node plus the node itself\r\n                prunedNodes += node.left.size + 1;\r\n                node = node.right;\r\n            }\r\n        }\r\n        // Node not found\r\n        return -1;\r\n    }\r\n    isEmpty() {\r\n        return this.root.isEmpty();\r\n    }\r\n    // Returns the total number of nodes in the map.\r\n    get size() {\r\n        return this.root.size;\r\n    }\r\n    // Returns the minimum key in the map.\r\n    minKey() {\r\n        return this.root.minKey();\r\n    }\r\n    // Returns the maximum key in the map.\r\n    maxKey() {\r\n        return this.root.maxKey();\r\n    }\r\n    // Traverses the map in key order and calls the specified action function\r\n    // for each key/value pair. If action returns true, traversal is aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    inorderTraversal(action) {\r\n        return this.root.inorderTraversal(action);\r\n    }\r\n    forEach(fn) {\r\n        this.inorderTraversal((k, v) => {\r\n            fn(k, v);\r\n            return false;\r\n        });\r\n    }\r\n    toString() {\r\n        const descriptions = [];\r\n        this.inorderTraversal((k, v) => {\r\n            descriptions.push(`${k}:${v}`);\r\n            return false;\r\n        });\r\n        return `{${descriptions.join(', ')}}`;\r\n    }\r\n    // Traverses the map in reverse key order and calls the specified action\r\n    // function for each key/value pair. If action returns true, traversal is\r\n    // aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    reverseTraversal(action) {\r\n        return this.root.reverseTraversal(action);\r\n    }\r\n    // Returns an iterator over the SortedMap.\r\n    getIterator() {\r\n        return new SortedMapIterator(this.root, null, this.comparator, false);\r\n    }\r\n    getIteratorFrom(key) {\r\n        return new SortedMapIterator(this.root, key, this.comparator, false);\r\n    }\r\n    getReverseIterator() {\r\n        return new SortedMapIterator(this.root, null, this.comparator, true);\r\n    }\r\n    getReverseIteratorFrom(key) {\r\n        return new SortedMapIterator(this.root, key, this.comparator, true);\r\n    }\r\n} // end SortedMap\r\n// An iterator over an LLRBNode.\r\nclass SortedMapIterator {\r\n    constructor(node, startKey, comparator, isReverse) {\r\n        this.isReverse = isReverse;\r\n        this.nodeStack = [];\r\n        let cmp = 1;\r\n        while (!node.isEmpty()) {\r\n            cmp = startKey ? comparator(node.key, startKey) : 1;\r\n            // flip the comparison if we're going in reverse\r\n            if (startKey && isReverse) {\r\n                cmp *= -1;\r\n            }\r\n            if (cmp < 0) {\r\n                // This node is less than our start key. ignore it\r\n                if (this.isReverse) {\r\n                    node = node.left;\r\n                }\r\n                else {\r\n                    node = node.right;\r\n                }\r\n            }\r\n            else if (cmp === 0) {\r\n                // This node is exactly equal to our start key. Push it on the stack,\r\n                // but stop iterating;\r\n                this.nodeStack.push(node);\r\n                break;\r\n            }\r\n            else {\r\n                // This node is greater than our start key, add it to the stack and move\r\n                // to the next one\r\n                this.nodeStack.push(node);\r\n                if (this.isReverse) {\r\n                    node = node.right;\r\n                }\r\n                else {\r\n                    node = node.left;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    getNext() {\r\n        let node = this.nodeStack.pop();\r\n        const result = { key: node.key, value: node.value };\r\n        if (this.isReverse) {\r\n            node = node.left;\r\n            while (!node.isEmpty()) {\r\n                this.nodeStack.push(node);\r\n                node = node.right;\r\n            }\r\n        }\r\n        else {\r\n            node = node.right;\r\n            while (!node.isEmpty()) {\r\n                this.nodeStack.push(node);\r\n                node = node.left;\r\n            }\r\n        }\r\n        return result;\r\n    }\r\n    hasNext() {\r\n        return this.nodeStack.length > 0;\r\n    }\r\n    peek() {\r\n        if (this.nodeStack.length === 0) {\r\n            return null;\r\n        }\r\n        const node = this.nodeStack[this.nodeStack.length - 1];\r\n        return { key: node.key, value: node.value };\r\n    }\r\n} // end SortedMapIterator\r\n// Represents a node in a Left-leaning Red-Black tree.\r\nclass LLRBNode {\r\n    constructor(key, value, color, left, right) {\r\n        this.key = key;\r\n        this.value = value;\r\n        this.color = color != null ? color : LLRBNode.RED;\r\n        this.left = left != null ? left : LLRBNode.EMPTY;\r\n        this.right = right != null ? right : LLRBNode.EMPTY;\r\n        this.size = this.left.size + 1 + this.right.size;\r\n    }\r\n    // Returns a copy of the current node, optionally replacing pieces of it.\r\n    copy(key, value, color, left, right) {\r\n        return new LLRBNode(key != null ? key : this.key, value != null ? value : this.value, color != null ? color : this.color, left != null ? left : this.left, right != null ? right : this.right);\r\n    }\r\n    isEmpty() {\r\n        return false;\r\n    }\r\n    // Traverses the tree in key order and calls the specified action function\r\n    // for each node. If action returns true, traversal is aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    inorderTraversal(action) {\r\n        return (this.left.inorderTraversal(action) ||\r\n            action(this.key, this.value) ||\r\n            this.right.inorderTraversal(action));\r\n    }\r\n    // Traverses the tree in reverse key order and calls the specified action\r\n    // function for each node. If action returns true, traversal is aborted.\r\n    // Returns the first truthy value returned by action, or the last falsey\r\n    // value returned by action.\r\n    reverseTraversal(action) {\r\n        return (this.right.reverseTraversal(action) ||\r\n            action(this.key, this.value) ||\r\n            this.left.reverseTraversal(action));\r\n    }\r\n    // Returns the minimum node in the tree.\r\n    min() {\r\n        if (this.left.isEmpty()) {\r\n            return this;\r\n        }\r\n        else {\r\n            return this.left.min();\r\n        }\r\n    }\r\n    // Returns the maximum key in the tree.\r\n    minKey() {\r\n        return this.min().key;\r\n    }\r\n    // Returns the maximum key in the tree.\r\n    maxKey() {\r\n        if (this.right.isEmpty()) {\r\n            return this.key;\r\n        }\r\n        else {\r\n            return this.right.maxKey();\r\n        }\r\n    }\r\n    // Returns new tree, with the key/value added.\r\n    insert(key, value, comparator) {\r\n        let n = this;\r\n        const cmp = comparator(key, n.key);\r\n        if (cmp < 0) {\r\n            n = n.copy(null, null, null, n.left.insert(key, value, comparator), null);\r\n        }\r\n        else if (cmp === 0) {\r\n            n = n.copy(null, value, null, null, null);\r\n        }\r\n        else {\r\n            n = n.copy(null, null, null, null, n.right.insert(key, value, comparator));\r\n        }\r\n        return n.fixUp();\r\n    }\r\n    removeMin() {\r\n        if (this.left.isEmpty()) {\r\n            return LLRBNode.EMPTY;\r\n        }\r\n        let n = this;\r\n        if (!n.left.isRed() && !n.left.left.isRed()) {\r\n            n = n.moveRedLeft();\r\n        }\r\n        n = n.copy(null, null, null, n.left.removeMin(), null);\r\n        return n.fixUp();\r\n    }\r\n    // Returns new tree, with the specified item removed.\r\n    remove(key, comparator) {\r\n        let smallest;\r\n        let n = this;\r\n        if (comparator(key, n.key) < 0) {\r\n            if (!n.left.isEmpty() && !n.left.isRed() && !n.left.left.isRed()) {\r\n                n = n.moveRedLeft();\r\n            }\r\n            n = n.copy(null, null, null, n.left.remove(key, comparator), null);\r\n        }\r\n        else {\r\n            if (n.left.isRed()) {\r\n                n = n.rotateRight();\r\n            }\r\n            if (!n.right.isEmpty() && !n.right.isRed() && !n.right.left.isRed()) {\r\n                n = n.moveRedRight();\r\n            }\r\n            if (comparator(key, n.key) === 0) {\r\n                if (n.right.isEmpty()) {\r\n                    return LLRBNode.EMPTY;\r\n                }\r\n                else {\r\n                    smallest = n.right.min();\r\n                    n = n.copy(smallest.key, smallest.value, null, null, n.right.removeMin());\r\n                }\r\n            }\r\n            n = n.copy(null, null, null, null, n.right.remove(key, comparator));\r\n        }\r\n        return n.fixUp();\r\n    }\r\n    isRed() {\r\n        return this.color;\r\n    }\r\n    // Returns new tree after performing any needed rotations.\r\n    fixUp() {\r\n        let n = this;\r\n        if (n.right.isRed() && !n.left.isRed()) {\r\n            n = n.rotateLeft();\r\n        }\r\n        if (n.left.isRed() && n.left.left.isRed()) {\r\n            n = n.rotateRight();\r\n        }\r\n        if (n.left.isRed() && n.right.isRed()) {\r\n            n = n.colorFlip();\r\n        }\r\n        return n;\r\n    }\r\n    moveRedLeft() {\r\n        let n = this.colorFlip();\r\n        if (n.right.left.isRed()) {\r\n            n = n.copy(null, null, null, null, n.right.rotateRight());\r\n            n = n.rotateLeft();\r\n            n = n.colorFlip();\r\n        }\r\n        return n;\r\n    }\r\n    moveRedRight() {\r\n        let n = this.colorFlip();\r\n        if (n.left.left.isRed()) {\r\n            n = n.rotateRight();\r\n            n = n.colorFlip();\r\n        }\r\n        return n;\r\n    }\r\n    rotateLeft() {\r\n        const nl = this.copy(null, null, LLRBNode.RED, null, this.right.left);\r\n        return this.right.copy(null, null, this.color, nl, null);\r\n    }\r\n    rotateRight() {\r\n        const nr = this.copy(null, null, LLRBNode.RED, this.left.right, null);\r\n        return this.left.copy(null, null, this.color, null, nr);\r\n    }\r\n    colorFlip() {\r\n        const left = this.left.copy(null, null, !this.left.color, null, null);\r\n        const right = this.right.copy(null, null, !this.right.color, null, null);\r\n        return this.copy(null, null, !this.color, left, right);\r\n    }\r\n    // For testing.\r\n    checkMaxDepth() {\r\n        const blackDepth = this.check();\r\n        if (Math.pow(2.0, blackDepth) <= this.size + 1) {\r\n            return true;\r\n        }\r\n        else {\r\n            return false;\r\n        }\r\n    }\r\n    // In a balanced RB tree, the black-depth (number of black nodes) from root to\r\n    // leaves is equal on both sides.  This function verifies that or asserts.\r\n    check() {\r\n        if (this.isRed() && this.left.isRed()) {\r\n            throw fail();\r\n        }\r\n        if (this.right.isRed()) {\r\n            throw fail();\r\n        }\r\n        const blackDepth = this.left.check();\r\n        if (blackDepth !== this.right.check()) {\r\n            throw fail();\r\n        }\r\n        else {\r\n            return blackDepth + (this.isRed() ? 0 : 1);\r\n        }\r\n    }\r\n} // end LLRBNode\r\n// Empty node is shared between all LLRB trees.\r\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\r\nLLRBNode.EMPTY = null;\r\nLLRBNode.RED = true;\r\nLLRBNode.BLACK = false;\r\n// Represents an empty node (a leaf node in the Red-Black Tree).\r\nclass LLRBEmptyNode {\r\n    constructor() {\r\n        this.size = 0;\r\n    }\r\n    get key() {\r\n        throw fail();\r\n    }\r\n    get value() {\r\n        throw fail();\r\n    }\r\n    get color() {\r\n        throw fail();\r\n    }\r\n    get left() {\r\n        throw fail();\r\n    }\r\n    get right() {\r\n        throw fail();\r\n    }\r\n    // Returns a copy of the current node.\r\n    copy(key, value, color, left, right) {\r\n        return this;\r\n    }\r\n    // Returns a copy of the tree, with the specified key/value added.\r\n    insert(key, value, comparator) {\r\n        return new LLRBNode(key, value);\r\n    }\r\n    // Returns a copy of the tree, with the specified key removed.\r\n    remove(key, comparator) {\r\n        return this;\r\n    }\r\n    isEmpty() {\r\n        return true;\r\n    }\r\n    inorderTraversal(action) {\r\n        return false;\r\n    }\r\n    reverseTraversal(action) {\r\n        return false;\r\n    }\r\n    minKey() {\r\n        return null;\r\n    }\r\n    maxKey() {\r\n        return null;\r\n    }\r\n    isRed() {\r\n        return false;\r\n    }\r\n    // For testing.\r\n    checkMaxDepth() {\r\n        return true;\r\n    }\r\n    check() {\r\n        return 0;\r\n    }\r\n} // end LLRBEmptyNode\r\nLLRBNode.EMPTY = new LLRBEmptyNode();\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * SortedSet is an immutable (copy-on-write) collection that holds elements\r\n * in order specified by the provided comparator.\r\n *\r\n * NOTE: if provided comparator returns 0 for two elements, we consider them to\r\n * be equal!\r\n */\r\nclass SortedSet {\r\n    constructor(comparator) {\r\n        this.comparator = comparator;\r\n        this.data = new SortedMap(this.comparator);\r\n    }\r\n    has(elem) {\r\n        return this.data.get(elem) !== null;\r\n    }\r\n    first() {\r\n        return this.data.minKey();\r\n    }\r\n    last() {\r\n        return this.data.maxKey();\r\n    }\r\n    get size() {\r\n        return this.data.size;\r\n    }\r\n    indexOf(elem) {\r\n        return this.data.indexOf(elem);\r\n    }\r\n    /** Iterates elements in order defined by \"comparator\" */\r\n    forEach(cb) {\r\n        this.data.inorderTraversal((k, v) => {\r\n            cb(k);\r\n            return false;\r\n        });\r\n    }\r\n    /** Iterates over `elem`s such that: range[0] &lt;= elem &lt; range[1]. */\r\n    forEachInRange(range, cb) {\r\n        const iter = this.data.getIteratorFrom(range[0]);\r\n        while (iter.hasNext()) {\r\n            const elem = iter.getNext();\r\n            if (this.comparator(elem.key, range[1]) >= 0) {\r\n                return;\r\n            }\r\n            cb(elem.key);\r\n        }\r\n    }\r\n    /**\r\n     * Iterates over `elem`s such that: start &lt;= elem until false is returned.\r\n     */\r\n    forEachWhile(cb, start) {\r\n        let iter;\r\n        if (start !== undefined) {\r\n            iter = this.data.getIteratorFrom(start);\r\n        }\r\n        else {\r\n            iter = this.data.getIterator();\r\n        }\r\n        while (iter.hasNext()) {\r\n            const elem = iter.getNext();\r\n            const result = cb(elem.key);\r\n            if (!result) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    /** Finds the least element greater than or equal to `elem`. */\r\n    firstAfterOrEqual(elem) {\r\n        const iter = this.data.getIteratorFrom(elem);\r\n        return iter.hasNext() ? iter.getNext().key : null;\r\n    }\r\n    getIterator() {\r\n        return new SortedSetIterator(this.data.getIterator());\r\n    }\r\n    getIteratorFrom(key) {\r\n        return new SortedSetIterator(this.data.getIteratorFrom(key));\r\n    }\r\n    /** Inserts or updates an element */\r\n    add(elem) {\r\n        return this.copy(this.data.remove(elem).insert(elem, true));\r\n    }\r\n    /** Deletes an element */\r\n    delete(elem) {\r\n        if (!this.has(elem)) {\r\n            return this;\r\n        }\r\n        return this.copy(this.data.remove(elem));\r\n    }\r\n    isEmpty() {\r\n        return this.data.isEmpty();\r\n    }\r\n    unionWith(other) {\r\n        let result = this;\r\n        // Make sure `result` always refers to the larger one of the two sets.\r\n        if (result.size < other.size) {\r\n            result = other;\r\n            other = this;\r\n        }\r\n        other.forEach(elem => {\r\n            result = result.add(elem);\r\n        });\r\n        return result;\r\n    }\r\n    isEqual(other) {\r\n        if (!(other instanceof SortedSet)) {\r\n            return false;\r\n        }\r\n        if (this.size !== other.size) {\r\n            return false;\r\n        }\r\n        const thisIt = this.data.getIterator();\r\n        const otherIt = other.data.getIterator();\r\n        while (thisIt.hasNext()) {\r\n            const thisElem = thisIt.getNext().key;\r\n            const otherElem = otherIt.getNext().key;\r\n            if (this.comparator(thisElem, otherElem) !== 0) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    toArray() {\r\n        const res = [];\r\n        this.forEach(targetId => {\r\n            res.push(targetId);\r\n        });\r\n        return res;\r\n    }\r\n    toString() {\r\n        const result = [];\r\n        this.forEach(elem => result.push(elem));\r\n        return 'SortedSet(' + result.toString() + ')';\r\n    }\r\n    copy(data) {\r\n        const result = new SortedSet(this.comparator);\r\n        result.data = data;\r\n        return result;\r\n    }\r\n}\r\nclass SortedSetIterator {\r\n    constructor(iter) {\r\n        this.iter = iter;\r\n    }\r\n    getNext() {\r\n        return this.iter.getNext().key;\r\n    }\r\n    hasNext() {\r\n        return this.iter.hasNext();\r\n    }\r\n}\r\n/**\r\n * Compares two sorted sets for equality using their natural ordering. The\r\n * method computes the intersection and invokes `onAdd` for every element that\r\n * is in `after` but not `before`. `onRemove` is invoked for every element in\r\n * `before` but missing from `after`.\r\n *\r\n * The method creates a copy of both `before` and `after` and runs in O(n log\r\n * n), where n is the size of the two lists.\r\n *\r\n * @param before - The elements that exist in the original set.\r\n * @param after - The elements to diff against the original set.\r\n * @param comparator - The comparator for the elements in before and after.\r\n * @param onAdd - A function to invoke for every element that is part of `\r\n * after` but not `before`.\r\n * @param onRemove - A function to invoke for every element that is part of\r\n * `before` but not `after`.\r\n */\r\nfunction diffSortedSets(before, after, comparator, onAdd, onRemove) {\r\n    const beforeIt = before.getIterator();\r\n    const afterIt = after.getIterator();\r\n    let beforeValue = advanceIterator(beforeIt);\r\n    let afterValue = advanceIterator(afterIt);\r\n    // Walk through the two sets at the same time, using the ordering defined by\r\n    // `comparator`.\r\n    while (beforeValue || afterValue) {\r\n        let added = false;\r\n        let removed = false;\r\n        if (beforeValue && afterValue) {\r\n            const cmp = comparator(beforeValue, afterValue);\r\n            if (cmp < 0) {\r\n                // The element was removed if the next element in our ordered\r\n                // walkthrough is only in `before`.\r\n                removed = true;\r\n            }\r\n            else if (cmp > 0) {\r\n                // The element was added if the next element in our ordered walkthrough\r\n                // is only in `after`.\r\n                added = true;\r\n            }\r\n        }\r\n        else if (beforeValue != null) {\r\n            removed = true;\r\n        }\r\n        else {\r\n            added = true;\r\n        }\r\n        if (added) {\r\n            onAdd(afterValue);\r\n            afterValue = advanceIterator(afterIt);\r\n        }\r\n        else if (removed) {\r\n            onRemove(beforeValue);\r\n            beforeValue = advanceIterator(beforeIt);\r\n        }\r\n        else {\r\n            beforeValue = advanceIterator(beforeIt);\r\n            afterValue = advanceIterator(afterIt);\r\n        }\r\n    }\r\n}\r\n/**\r\n * Returns the next element from the iterator or `undefined` if none available.\r\n */\r\nfunction advanceIterator(it) {\r\n    return it.hasNext() ? it.getNext() : undefined;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Provides a set of fields that can be used to partially patch a document.\r\n * FieldMask is used in conjunction with ObjectValue.\r\n * Examples:\r\n *   foo - Overwrites foo entirely with the provided value. If foo is not\r\n *         present in the companion ObjectValue, the field is deleted.\r\n *   foo.bar - Overwrites only the field bar of the object foo.\r\n *             If foo is not an object, foo is replaced with an object\r\n *             containing foo\r\n */\r\nclass FieldMask {\r\n    constructor(fields) {\r\n        this.fields = fields;\r\n        // TODO(dimond): validation of FieldMask\r\n        // Sort the field mask to support `FieldMask.isEqual()` and assert below.\r\n        fields.sort(FieldPath$1.comparator);\r\n    }\r\n    static empty() {\r\n        return new FieldMask([]);\r\n    }\r\n    /**\r\n     * Returns a new FieldMask object that is the result of adding all the given\r\n     * fields paths to this field mask.\r\n     */\r\n    unionWith(extraFields) {\r\n        let mergedMaskSet = new SortedSet(FieldPath$1.comparator);\r\n        for (const fieldPath of this.fields) {\r\n            mergedMaskSet = mergedMaskSet.add(fieldPath);\r\n        }\r\n        for (const fieldPath of extraFields) {\r\n            mergedMaskSet = mergedMaskSet.add(fieldPath);\r\n        }\r\n        return new FieldMask(mergedMaskSet.toArray());\r\n    }\r\n    /**\r\n     * Verifies that `fieldPath` is included by at least one field in this field\r\n     * mask.\r\n     *\r\n     * This is an O(n) operation, where `n` is the size of the field mask.\r\n     */\r\n    covers(fieldPath) {\r\n        for (const fieldMaskPath of this.fields) {\r\n            if (fieldMaskPath.isPrefixOf(fieldPath)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n    isEqual(other) {\r\n        return arrayEquals(this.fields, other.fields, (l, r) => l.isEqual(r));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Converts a Base64 encoded string to a binary string. */\r\nfunction decodeBase64(encoded) {\r\n    // Note: We used to validate the base64 string here via a regular expression.\r\n    // This was removed to improve the performance of indexing.\r\n    return Buffer.from(encoded, 'base64').toString('binary');\r\n}\r\n/** Converts a binary string to a Base64 encoded string. */\r\nfunction encodeBase64(raw) {\r\n    return Buffer.from(raw, 'binary').toString('base64');\r\n}\r\n/** True if and only if the Base64 conversion functions are available. */\r\nfunction isBase64Available() {\r\n    return true;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Immutable class that represents a \"proto\" byte string.\r\n *\r\n * Proto byte strings can either be Base64-encoded strings or Uint8Arrays when\r\n * sent on the wire. This class abstracts away this differentiation by holding\r\n * the proto byte string in a common class that must be converted into a string\r\n * before being sent as a proto.\r\n * @internal\r\n */\r\nclass ByteString {\r\n    constructor(binaryString) {\r\n        this.binaryString = binaryString;\r\n    }\r\n    static fromBase64String(base64) {\r\n        const binaryString = decodeBase64(base64);\r\n        return new ByteString(binaryString);\r\n    }\r\n    static fromUint8Array(array) {\r\n        // TODO(indexing); Remove the copy of the byte string here as this method\r\n        // is frequently called during indexing.\r\n        const binaryString = binaryStringFromUint8Array(array);\r\n        return new ByteString(binaryString);\r\n    }\r\n    [Symbol.iterator]() {\r\n        let i = 0;\r\n        return {\r\n            next: () => {\r\n                if (i < this.binaryString.length) {\r\n                    return { value: this.binaryString.charCodeAt(i++), done: false };\r\n                }\r\n                else {\r\n                    return { value: undefined, done: true };\r\n                }\r\n            }\r\n        };\r\n    }\r\n    toBase64() {\r\n        return encodeBase64(this.binaryString);\r\n    }\r\n    toUint8Array() {\r\n        return uint8ArrayFromBinaryString(this.binaryString);\r\n    }\r\n    approximateByteSize() {\r\n        return this.binaryString.length * 2;\r\n    }\r\n    compareTo(other) {\r\n        return primitiveComparator(this.binaryString, other.binaryString);\r\n    }\r\n    isEqual(other) {\r\n        return this.binaryString === other.binaryString;\r\n    }\r\n}\r\nByteString.EMPTY_BYTE_STRING = new ByteString('');\r\n/**\r\n * Helper function to convert an Uint8array to a binary string.\r\n */\r\nfunction binaryStringFromUint8Array(array) {\r\n    let binaryString = '';\r\n    for (let i = 0; i < array.length; ++i) {\r\n        binaryString += String.fromCharCode(array[i]);\r\n    }\r\n    return binaryString;\r\n}\r\n/**\r\n * Helper function to convert a binary string to an Uint8Array.\r\n */\r\nfunction uint8ArrayFromBinaryString(binaryString) {\r\n    const buffer = new Uint8Array(binaryString.length);\r\n    for (let i = 0; i < binaryString.length; i++) {\r\n        buffer[i] = binaryString.charCodeAt(i);\r\n    }\r\n    return buffer;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// A RegExp matching ISO 8601 UTC timestamps with optional fraction.\r\nconst ISO_TIMESTAMP_REG_EXP = new RegExp(/^\\d{4}-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d(?:\\.(\\d+))?Z$/);\r\n/**\r\n * Converts the possible Proto values for a timestamp value into a \"seconds and\r\n * nanos\" representation.\r\n */\r\nfunction normalizeTimestamp(date) {\r\n    hardAssert(!!date);\r\n    // The json interface (for the browser) will return an iso timestamp string,\r\n    // while the proto js library (for node) will return a\r\n    // google.protobuf.Timestamp instance.\r\n    if (typeof date === 'string') {\r\n        // The date string can have higher precision (nanos) than the Date class\r\n        // (millis), so we do some custom parsing here.\r\n        // Parse the nanos right out of the string.\r\n        let nanos = 0;\r\n        const fraction = ISO_TIMESTAMP_REG_EXP.exec(date);\r\n        hardAssert(!!fraction);\r\n        if (fraction[1]) {\r\n            // Pad the fraction out to 9 digits (nanos).\r\n            let nanoStr = fraction[1];\r\n            nanoStr = (nanoStr + '000000000').substr(0, 9);\r\n            nanos = Number(nanoStr);\r\n        }\r\n        // Parse the date to get the seconds.\r\n        const parsedDate = new Date(date);\r\n        const seconds = Math.floor(parsedDate.getTime() / 1000);\r\n        return { seconds, nanos };\r\n    }\r\n    else {\r\n        // TODO(b/37282237): Use strings for Proto3 timestamps\r\n        // assert(!this.options.useProto3Json,\r\n        //   'The timestamp instance format requires Proto JS.');\r\n        const seconds = normalizeNumber(date.seconds);\r\n        const nanos = normalizeNumber(date.nanos);\r\n        return { seconds, nanos };\r\n    }\r\n}\r\n/**\r\n * Converts the possible Proto types for numbers into a JavaScript number.\r\n * Returns 0 if the value is not numeric.\r\n */\r\nfunction normalizeNumber(value) {\r\n    // TODO(bjornick): Handle int64 greater than 53 bits.\r\n    if (typeof value === 'number') {\r\n        return value;\r\n    }\r\n    else if (typeof value === 'string') {\r\n        return Number(value);\r\n    }\r\n    else {\r\n        return 0;\r\n    }\r\n}\r\n/** Converts the possible Proto types for Blobs into a ByteString. */\r\nfunction normalizeByteString(blob) {\r\n    if (typeof blob === 'string') {\r\n        return ByteString.fromBase64String(blob);\r\n    }\r\n    else {\r\n        return ByteString.fromUint8Array(blob);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a locally-applied ServerTimestamp.\r\n *\r\n * Server Timestamps are backed by MapValues that contain an internal field\r\n * `__type__` with a value of `server_timestamp`. The previous value and local\r\n * write time are stored in its `__previous_value__` and `__local_write_time__`\r\n * fields respectively.\r\n *\r\n * Notes:\r\n * - ServerTimestampValue instances are created as the result of applying a\r\n *   transform. They can only exist in the local view of a document. Therefore\r\n *   they do not need to be parsed or serialized.\r\n * - When evaluated locally (e.g. for snapshot.data()), they by default\r\n *   evaluate to `null`. This behavior can be configured by passing custom\r\n *   FieldValueOptions to value().\r\n * - With respect to other ServerTimestampValues, they sort by their\r\n *   localWriteTime.\r\n */\r\nconst SERVER_TIMESTAMP_SENTINEL = 'server_timestamp';\r\nconst TYPE_KEY = '__type__';\r\nconst PREVIOUS_VALUE_KEY = '__previous_value__';\r\nconst LOCAL_WRITE_TIME_KEY = '__local_write_time__';\r\nfunction isServerTimestamp(value) {\r\n    var _a, _b;\r\n    const type = (_b = (((_a = value === null || value === void 0 ? void 0 : value.mapValue) === null || _a === void 0 ? void 0 : _a.fields) || {})[TYPE_KEY]) === null || _b === void 0 ? void 0 : _b.stringValue;\r\n    return type === SERVER_TIMESTAMP_SENTINEL;\r\n}\r\n/**\r\n * Creates a new ServerTimestamp proto value (using the internal format).\r\n */\r\nfunction serverTimestamp$1(localWriteTime, previousValue) {\r\n    const mapValue = {\r\n        fields: {\r\n            [TYPE_KEY]: {\r\n                stringValue: SERVER_TIMESTAMP_SENTINEL\r\n            },\r\n            [LOCAL_WRITE_TIME_KEY]: {\r\n                timestampValue: {\r\n                    seconds: localWriteTime.seconds,\r\n                    nanos: localWriteTime.nanoseconds\r\n                }\r\n            }\r\n        }\r\n    };\r\n    // We should avoid storing deeply nested server timestamp map values\r\n    // because we never use the intermediate \"previous values\".\r\n    // For example:\r\n    // previous: 42L, add: t1, result: t1 -> 42L\r\n    // previous: t1,  add: t2, result: t2 -> 42L (NOT t2 -> t1 -> 42L)\r\n    // previous: t2,  add: t3, result: t3 -> 42L (NOT t3 -> t2 -> t1 -> 42L)\r\n    // `getPreviousValue` recursively traverses server timestamps to find the\r\n    // least recent Value.\r\n    if (previousValue && isServerTimestamp(previousValue)) {\r\n        previousValue = getPreviousValue(previousValue);\r\n    }\r\n    if (previousValue) {\r\n        mapValue.fields[PREVIOUS_VALUE_KEY] = previousValue;\r\n    }\r\n    return { mapValue };\r\n}\r\n/**\r\n * Returns the value of the field before this ServerTimestamp was set.\r\n *\r\n * Preserving the previous values allows the user to display the last resoled\r\n * value until the backend responds with the timestamp.\r\n */\r\nfunction getPreviousValue(value) {\r\n    const previousValue = value.mapValue.fields[PREVIOUS_VALUE_KEY];\r\n    if (isServerTimestamp(previousValue)) {\r\n        return getPreviousValue(previousValue);\r\n    }\r\n    return previousValue;\r\n}\r\n/**\r\n * Returns the local time at which this timestamp was first set.\r\n */\r\nfunction getLocalWriteTime(value) {\r\n    const localWriteTime = normalizeTimestamp(value.mapValue.fields[LOCAL_WRITE_TIME_KEY].timestampValue);\r\n    return new Timestamp(localWriteTime.seconds, localWriteTime.nanos);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass DatabaseInfo {\r\n    /**\r\n     * Constructs a DatabaseInfo using the provided host, databaseId and\r\n     * persistenceKey.\r\n     *\r\n     * @param databaseId - The database to use.\r\n     * @param appId - The Firebase App Id.\r\n     * @param persistenceKey - A unique identifier for this Firestore's local\r\n     * storage (used in conjunction with the databaseId).\r\n     * @param host - The Firestore backend host to connect to.\r\n     * @param ssl - Whether to use SSL when connecting.\r\n     * @param forceLongPolling - Whether to use the forceLongPolling option\r\n     * when using WebChannel as the network transport.\r\n     * @param autoDetectLongPolling - Whether to use the detectBufferingProxy\r\n     * option when using WebChannel as the network transport.\r\n     * @param longPollingOptions Options that configure long-polling.\r\n     * @param useFetchStreams Whether to use the Fetch API instead of\r\n     * XMLHTTPRequest\r\n     */\r\n    constructor(databaseId, appId, persistenceKey, host, ssl, forceLongPolling, autoDetectLongPolling, longPollingOptions, useFetchStreams) {\r\n        this.databaseId = databaseId;\r\n        this.appId = appId;\r\n        this.persistenceKey = persistenceKey;\r\n        this.host = host;\r\n        this.ssl = ssl;\r\n        this.forceLongPolling = forceLongPolling;\r\n        this.autoDetectLongPolling = autoDetectLongPolling;\r\n        this.longPollingOptions = longPollingOptions;\r\n        this.useFetchStreams = useFetchStreams;\r\n    }\r\n}\r\n/** The default database name for a project. */\r\nconst DEFAULT_DATABASE_NAME = '(default)';\r\n/**\r\n * Represents the database ID a Firestore client is associated with.\r\n * @internal\r\n */\r\nclass DatabaseId {\r\n    constructor(projectId, database) {\r\n        this.projectId = projectId;\r\n        this.database = database ? database : DEFAULT_DATABASE_NAME;\r\n    }\r\n    static empty() {\r\n        return new DatabaseId('', '');\r\n    }\r\n    get isDefaultDatabase() {\r\n        return this.database === DEFAULT_DATABASE_NAME;\r\n    }\r\n    isEqual(other) {\r\n        return (other instanceof DatabaseId &&\r\n            other.projectId === this.projectId &&\r\n            other.database === this.database);\r\n    }\r\n}\r\nfunction databaseIdFromApp(app, database) {\r\n    if (!Object.prototype.hasOwnProperty.apply(app.options, ['projectId'])) {\r\n        throw new FirestoreError(Code.INVALID_ARGUMENT, '\"projectId\" not provided in firebase.initializeApp.');\r\n    }\r\n    return new DatabaseId(app.options.projectId, database);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Sentinel value that sorts before any Mutation Batch ID. */\r\nconst BATCHID_UNKNOWN = -1;\r\n/**\r\n * Returns whether a variable is either undefined or null.\r\n */\r\nfunction isNullOrUndefined(value) {\r\n    return value === null || value === undefined;\r\n}\r\n/** Returns whether the value represents -0. */\r\nfunction isNegativeZero(value) {\r\n    // Detect if the value is -0.0. Based on polyfill from\r\n    // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is\r\n    return value === 0 && 1 / value === 1 / -0;\r\n}\r\n/**\r\n * Returns whether a value is an integer and in the safe integer range\r\n * @param value - The value to test for being an integer and in the safe range\r\n */\r\nfunction isSafeInteger(value) {\r\n    return (typeof value === 'number' &&\r\n        Number.isInteger(value) &&\r\n        !isNegativeZero(value) &&\r\n        value <= Number.MAX_SAFE_INTEGER &&\r\n        value >= Number.MIN_SAFE_INTEGER);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst MAX_VALUE_TYPE = '__max__';\r\nconst MAX_VALUE = {\r\n    mapValue: {\r\n        fields: {\r\n            '__type__': { stringValue: MAX_VALUE_TYPE }\r\n        }\r\n    }\r\n};\r\nconst MIN_VALUE = {\r\n    nullValue: 'NULL_VALUE'\r\n};\r\n/** Extracts the backend's type order for the provided value. */\r\nfunction typeOrder(value) {\r\n    if ('nullValue' in value) {\r\n        return 0 /* TypeOrder.NullValue */;\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return 1 /* TypeOrder.BooleanValue */;\r\n    }\r\n    else if ('integerValue' in value || 'doubleValue' in value) {\r\n        return 2 /* TypeOrder.NumberValue */;\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return 3 /* TypeOrder.TimestampValue */;\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return 5 /* TypeOrder.StringValue */;\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return 6 /* TypeOrder.BlobValue */;\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return 7 /* TypeOrder.RefValue */;\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return 8 /* TypeOrder.GeoPointValue */;\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return 9 /* TypeOrder.ArrayValue */;\r\n    }\r\n    else if ('mapValue' in value) {\r\n        if (isServerTimestamp(value)) {\r\n            return 4 /* TypeOrder.ServerTimestampValue */;\r\n        }\r\n        else if (isMaxValue(value)) {\r\n            return 9007199254740991 /* TypeOrder.MaxValue */;\r\n        }\r\n        return 10 /* TypeOrder.ObjectValue */;\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\n/** Tests `left` and `right` for equality based on the backend semantics. */\r\nfunction valueEquals(left, right) {\r\n    if (left === right) {\r\n        return true;\r\n    }\r\n    const leftType = typeOrder(left);\r\n    const rightType = typeOrder(right);\r\n    if (leftType !== rightType) {\r\n        return false;\r\n    }\r\n    switch (leftType) {\r\n        case 0 /* TypeOrder.NullValue */:\r\n            return true;\r\n        case 1 /* TypeOrder.BooleanValue */:\r\n            return left.booleanValue === right.booleanValue;\r\n        case 4 /* TypeOrder.ServerTimestampValue */:\r\n            return getLocalWriteTime(left).isEqual(getLocalWriteTime(right));\r\n        case 3 /* TypeOrder.TimestampValue */:\r\n            return timestampEquals(left, right);\r\n        case 5 /* TypeOrder.StringValue */:\r\n            return left.stringValue === right.stringValue;\r\n        case 6 /* TypeOrder.BlobValue */:\r\n            return blobEquals(left, right);\r\n        case 7 /* TypeOrder.RefValue */:\r\n            return left.referenceValue === right.referenceValue;\r\n        case 8 /* TypeOrder.GeoPointValue */:\r\n            return geoPointEquals(left, right);\r\n        case 2 /* TypeOrder.NumberValue */:\r\n            return numberEquals(left, right);\r\n        case 9 /* TypeOrder.ArrayValue */:\r\n            return arrayEquals(left.arrayValue.values || [], right.arrayValue.values || [], valueEquals);\r\n        case 10 /* TypeOrder.ObjectValue */:\r\n            return objectEquals(left, right);\r\n        case 9007199254740991 /* TypeOrder.MaxValue */:\r\n            return true;\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction timestampEquals(left, right) {\r\n    if (typeof left.timestampValue === 'string' &&\r\n        typeof right.timestampValue === 'string' &&\r\n        left.timestampValue.length === right.timestampValue.length) {\r\n        // Use string equality for ISO 8601 timestamps\r\n        return left.timestampValue === right.timestampValue;\r\n    }\r\n    const leftTimestamp = normalizeTimestamp(left.timestampValue);\r\n    const rightTimestamp = normalizeTimestamp(right.timestampValue);\r\n    return (leftTimestamp.seconds === rightTimestamp.seconds &&\r\n        leftTimestamp.nanos === rightTimestamp.nanos);\r\n}\r\nfunction geoPointEquals(left, right) {\r\n    return (normalizeNumber(left.geoPointValue.latitude) ===\r\n        normalizeNumber(right.geoPointValue.latitude) &&\r\n        normalizeNumber(left.geoPointValue.longitude) ===\r\n            normalizeNumber(right.geoPointValue.longitude));\r\n}\r\nfunction blobEquals(left, right) {\r\n    return normalizeByteString(left.bytesValue).isEqual(normalizeByteString(right.bytesValue));\r\n}\r\nfunction numberEquals(left, right) {\r\n    if ('integerValue' in left && 'integerValue' in right) {\r\n        return (normalizeNumber(left.integerValue) === normalizeNumber(right.integerValue));\r\n    }\r\n    else if ('doubleValue' in left && 'doubleValue' in right) {\r\n        const n1 = normalizeNumber(left.doubleValue);\r\n        const n2 = normalizeNumber(right.doubleValue);\r\n        if (n1 === n2) {\r\n            return isNegativeZero(n1) === isNegativeZero(n2);\r\n        }\r\n        else {\r\n            return isNaN(n1) && isNaN(n2);\r\n        }\r\n    }\r\n    return false;\r\n}\r\nfunction objectEquals(left, right) {\r\n    const leftMap = left.mapValue.fields || {};\r\n    const rightMap = right.mapValue.fields || {};\r\n    if (objectSize(leftMap) !== objectSize(rightMap)) {\r\n        return false;\r\n    }\r\n    for (const key in leftMap) {\r\n        if (leftMap.hasOwnProperty(key)) {\r\n            if (rightMap[key] === undefined ||\r\n                !valueEquals(leftMap[key], rightMap[key])) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\r\n/** Returns true if the ArrayValue contains the specified element. */\r\nfunction arrayValueContains(haystack, needle) {\r\n    return ((haystack.values || []).find(v => valueEquals(v, needle)) !== undefined);\r\n}\r\nfunction valueCompare(left, right) {\r\n    if (left === right) {\r\n        return 0;\r\n    }\r\n    const leftType = typeOrder(left);\r\n    const rightType = typeOrder(right);\r\n    if (leftType !== rightType) {\r\n        return primitiveComparator(leftType, rightType);\r\n    }\r\n    switch (leftType) {\r\n        case 0 /* TypeOrder.NullValue */:\r\n        case 9007199254740991 /* TypeOrder.MaxValue */:\r\n            return 0;\r\n        case 1 /* TypeOrder.BooleanValue */:\r\n            return primitiveComparator(left.booleanValue, right.booleanValue);\r\n        case 2 /* TypeOrder.NumberValue */:\r\n            return compareNumbers(left, right);\r\n        case 3 /* TypeOrder.TimestampValue */:\r\n            return compareTimestamps(left.timestampValue, right.timestampValue);\r\n        case 4 /* TypeOrder.ServerTimestampValue */:\r\n            return compareTimestamps(getLocalWriteTime(left), getLocalWriteTime(right));\r\n        case 5 /* TypeOrder.StringValue */:\r\n            return primitiveComparator(left.stringValue, right.stringValue);\r\n        case 6 /* TypeOrder.BlobValue */:\r\n            return compareBlobs(left.bytesValue, right.bytesValue);\r\n        case 7 /* TypeOrder.RefValue */:\r\n            return compareReferences(left.referenceValue, right.referenceValue);\r\n        case 8 /* TypeOrder.GeoPointValue */:\r\n            return compareGeoPoints(left.geoPointValue, right.geoPointValue);\r\n        case 9 /* TypeOrder.ArrayValue */:\r\n            return compareArrays(left.arrayValue, right.arrayValue);\r\n        case 10 /* TypeOrder.ObjectValue */:\r\n            return compareMaps(left.mapValue, right.mapValue);\r\n        default:\r\n            throw fail();\r\n    }\r\n}\r\nfunction compareNumbers(left, right) {\r\n    const leftNumber = normalizeNumber(left.integerValue || left.doubleValue);\r\n    const rightNumber = normalizeNumber(right.integerValue || right.doubleValue);\r\n    if (leftNumber < rightNumber) {\r\n        return -1;\r\n    }\r\n    else if (leftNumber > rightNumber) {\r\n        return 1;\r\n    }\r\n    else if (leftNumber === rightNumber) {\r\n        return 0;\r\n    }\r\n    else {\r\n        // one or both are NaN.\r\n        if (isNaN(leftNumber)) {\r\n            return isNaN(rightNumber) ? 0 : -1;\r\n        }\r\n        else {\r\n            return 1;\r\n        }\r\n    }\r\n}\r\nfunction compareTimestamps(left, right) {\r\n    if (typeof left === 'string' &&\r\n        typeof right === 'string' &&\r\n        left.length === right.length) {\r\n        return primitiveComparator(left, right);\r\n    }\r\n    const leftTimestamp = normalizeTimestamp(left);\r\n    const rightTimestamp = normalizeTimestamp(right);\r\n    const comparison = primitiveComparator(leftTimestamp.seconds, rightTimestamp.seconds);\r\n    if (comparison !== 0) {\r\n        return comparison;\r\n    }\r\n    return primitiveComparator(leftTimestamp.nanos, rightTimestamp.nanos);\r\n}\r\nfunction compareReferences(leftPath, rightPath) {\r\n    const leftSegments = leftPath.split('/');\r\n    const rightSegments = rightPath.split('/');\r\n    for (let i = 0; i < leftSegments.length && i < rightSegments.length; i++) {\r\n        const comparison = primitiveComparator(leftSegments[i], rightSegments[i]);\r\n        if (comparison !== 0) {\r\n            return comparison;\r\n        }\r\n    }\r\n    return primitiveComparator(leftSegments.length, rightSegments.length);\r\n}\r\nfunction compareGeoPoints(left, right) {\r\n    const comparison = primitiveComparator(normalizeNumber(left.latitude), normalizeNumber(right.latitude));\r\n    if (comparison !== 0) {\r\n        return comparison;\r\n    }\r\n    return primitiveComparator(normalizeNumber(left.longitude), normalizeNumber(right.longitude));\r\n}\r\nfunction compareBlobs(left, right) {\r\n    const leftBytes = normalizeByteString(left);\r\n    const rightBytes = normalizeByteString(right);\r\n    return leftBytes.compareTo(rightBytes);\r\n}\r\nfunction compareArrays(left, right) {\r\n    const leftArray = left.values || [];\r\n    const rightArray = right.values || [];\r\n    for (let i = 0; i < leftArray.length && i < rightArray.length; ++i) {\r\n        const compare = valueCompare(leftArray[i], rightArray[i]);\r\n        if (compare) {\r\n            return compare;\r\n        }\r\n    }\r\n    return primitiveComparator(leftArray.length, rightArray.length);\r\n}\r\nfunction compareMaps(left, right) {\r\n    if (left === MAX_VALUE.mapValue && right === MAX_VALUE.mapValue) {\r\n        return 0;\r\n    }\r\n    else if (left === MAX_VALUE.mapValue) {\r\n        return 1;\r\n    }\r\n    else if (right === MAX_VALUE.mapValue) {\r\n        return -1;\r\n    }\r\n    const leftMap = left.fields || {};\r\n    const leftKeys = Object.keys(leftMap);\r\n    const rightMap = right.fields || {};\r\n    const rightKeys = Object.keys(rightMap);\r\n    // Even though MapValues are likely sorted correctly based on their insertion\r\n    // order (e.g. when received from the backend), local modifications can bring\r\n    // elements out of order. We need to re-sort the elements to ensure that\r\n    // canonical IDs are independent of insertion order.\r\n    leftKeys.sort();\r\n    rightKeys.sort();\r\n    for (let i = 0; i < leftKeys.length && i < rightKeys.length; ++i) {\r\n        const keyCompare = primitiveComparator(leftKeys[i], rightKeys[i]);\r\n        if (keyCompare !== 0) {\r\n            return keyCompare;\r\n        }\r\n        const compare = valueCompare(leftMap[leftKeys[i]], rightMap[rightKeys[i]]);\r\n        if (compare !== 0) {\r\n            return compare;\r\n        }\r\n    }\r\n    return primitiveComparator(leftKeys.length, rightKeys.length);\r\n}\r\n/**\r\n * Generates the canonical ID for the provided field value (as used in Target\r\n * serialization).\r\n */\r\nfunction canonicalId(value) {\r\n    return canonifyValue(value);\r\n}\r\nfunction canonifyValue(value) {\r\n    if ('nullValue' in value) {\r\n        return 'null';\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return '' + value.booleanValue;\r\n    }\r\n    else if ('integerValue' in value) {\r\n        return '' + value.integerValue;\r\n    }\r\n    else if ('doubleValue' in value) {\r\n        return '' + value.doubleValue;\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return canonifyTimestamp(value.timestampValue);\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return value.stringValue;\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return canonifyByteString(value.bytesValue);\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return canonifyReference(value.referenceValue);\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return canonifyGeoPoint(value.geoPointValue);\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return canonifyArray(value.arrayValue);\r\n    }\r\n    else if ('mapValue' in value) {\r\n        return canonifyMap(value.mapValue);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction canonifyByteString(byteString) {\r\n    return normalizeByteString(byteString).toBase64();\r\n}\r\nfunction canonifyTimestamp(timestamp) {\r\n    const normalizedTimestamp = normalizeTimestamp(timestamp);\r\n    return `time(${normalizedTimestamp.seconds},${normalizedTimestamp.nanos})`;\r\n}\r\nfunction canonifyGeoPoint(geoPoint) {\r\n    return `geo(${geoPoint.latitude},${geoPoint.longitude})`;\r\n}\r\nfunction canonifyReference(referenceValue) {\r\n    return DocumentKey.fromName(referenceValue).toString();\r\n}\r\nfunction canonifyMap(mapValue) {\r\n    // Iteration order in JavaScript is not guaranteed. To ensure that we generate\r\n    // matching canonical IDs for identical maps, we need to sort the keys.\r\n    const sortedKeys = Object.keys(mapValue.fields || {}).sort();\r\n    let result = '{';\r\n    let first = true;\r\n    for (const key of sortedKeys) {\r\n        if (!first) {\r\n            result += ',';\r\n        }\r\n        else {\r\n            first = false;\r\n        }\r\n        result += `${key}:${canonifyValue(mapValue.fields[key])}`;\r\n    }\r\n    return result + '}';\r\n}\r\nfunction canonifyArray(arrayValue) {\r\n    let result = '[';\r\n    let first = true;\r\n    for (const value of arrayValue.values || []) {\r\n        if (!first) {\r\n            result += ',';\r\n        }\r\n        else {\r\n            first = false;\r\n        }\r\n        result += canonifyValue(value);\r\n    }\r\n    return result + ']';\r\n}\r\n/**\r\n * Returns an approximate (and wildly inaccurate) in-memory size for the field\r\n * value.\r\n *\r\n * The memory size takes into account only the actual user data as it resides\r\n * in memory and ignores object overhead.\r\n */\r\nfunction estimateByteSize(value) {\r\n    switch (typeOrder(value)) {\r\n        case 0 /* TypeOrder.NullValue */:\r\n            return 4;\r\n        case 1 /* TypeOrder.BooleanValue */:\r\n            return 4;\r\n        case 2 /* TypeOrder.NumberValue */:\r\n            return 8;\r\n        case 3 /* TypeOrder.TimestampValue */:\r\n            // Timestamps are made up of two distinct numbers (seconds + nanoseconds)\r\n            return 16;\r\n        case 4 /* TypeOrder.ServerTimestampValue */:\r\n            const previousValue = getPreviousValue(value);\r\n            return previousValue ? 16 + estimateByteSize(previousValue) : 16;\r\n        case 5 /* TypeOrder.StringValue */:\r\n            // See https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures:\r\n            // \"JavaScript's String type is [...] a set of elements of 16-bit unsigned\r\n            // integer values\"\r\n            return value.stringValue.length * 2;\r\n        case 6 /* TypeOrder.BlobValue */:\r\n            return normalizeByteString(value.bytesValue).approximateByteSize();\r\n        case 7 /* TypeOrder.RefValue */:\r\n            return value.referenceValue.length;\r\n        case 8 /* TypeOrder.GeoPointValue */:\r\n            // GeoPoints are made up of two distinct numbers (latitude + longitude)\r\n            return 16;\r\n        case 9 /* TypeOrder.ArrayValue */:\r\n            return estimateArrayByteSize(value.arrayValue);\r\n        case 10 /* TypeOrder.ObjectValue */:\r\n            return estimateMapByteSize(value.mapValue);\r\n        default:\r\n            throw fail();\r\n    }\r\n}\r\nfunction estimateMapByteSize(mapValue) {\r\n    let size = 0;\r\n    forEach(mapValue.fields, (key, val) => {\r\n        size += key.length + estimateByteSize(val);\r\n    });\r\n    return size;\r\n}\r\nfunction estimateArrayByteSize(arrayValue) {\r\n    return (arrayValue.values || []).reduce((previousSize, value) => previousSize + estimateByteSize(value), 0);\r\n}\r\n/** Returns a reference value for the provided database and key. */\r\nfunction refValue(databaseId, key) {\r\n    return {\r\n        referenceValue: `projects/${databaseId.projectId}/databases/${databaseId.database}/documents/${key.path.canonicalString()}`\r\n    };\r\n}\r\n/** Returns true if `value` is an IntegerValue . */\r\nfunction isInteger(value) {\r\n    return !!value && 'integerValue' in value;\r\n}\r\n/** Returns true if `value` is a DoubleValue. */\r\nfunction isDouble(value) {\r\n    return !!value && 'doubleValue' in value;\r\n}\r\n/** Returns true if `value` is either an IntegerValue or a DoubleValue. */\r\nfunction isNumber(value) {\r\n    return isInteger(value) || isDouble(value);\r\n}\r\n/** Returns true if `value` is an ArrayValue. */\r\nfunction isArray(value) {\r\n    return !!value && 'arrayValue' in value;\r\n}\r\n/** Returns true if `value` is a NullValue. */\r\nfunction isNullValue(value) {\r\n    return !!value && 'nullValue' in value;\r\n}\r\n/** Returns true if `value` is NaN. */\r\nfunction isNanValue(value) {\r\n    return !!value && 'doubleValue' in value && isNaN(Number(value.doubleValue));\r\n}\r\n/** Returns true if `value` is a MapValue. */\r\nfunction isMapValue(value) {\r\n    return !!value && 'mapValue' in value;\r\n}\r\n/** Creates a deep copy of `source`. */\r\nfunction deepClone(source) {\r\n    if (source.geoPointValue) {\r\n        return { geoPointValue: Object.assign({}, source.geoPointValue) };\r\n    }\r\n    else if (source.timestampValue &&\r\n        typeof source.timestampValue === 'object') {\r\n        return { timestampValue: Object.assign({}, source.timestampValue) };\r\n    }\r\n    else if (source.mapValue) {\r\n        const target = { mapValue: { fields: {} } };\r\n        forEach(source.mapValue.fields, (key, val) => (target.mapValue.fields[key] = deepClone(val)));\r\n        return target;\r\n    }\r\n    else if (source.arrayValue) {\r\n        const target = { arrayValue: { values: [] } };\r\n        for (let i = 0; i < (source.arrayValue.values || []).length; ++i) {\r\n            target.arrayValue.values[i] = deepClone(source.arrayValue.values[i]);\r\n        }\r\n        return target;\r\n    }\r\n    else {\r\n        return Object.assign({}, source);\r\n    }\r\n}\r\n/** Returns true if the Value represents the canonical {@link #MAX_VALUE} . */\r\nfunction isMaxValue(value) {\r\n    return ((((value.mapValue || {}).fields || {})['__type__'] || {}).stringValue ===\r\n        MAX_VALUE_TYPE);\r\n}\r\n/** Returns the lowest value for the given value type (inclusive). */\r\nfunction valuesGetLowerBound(value) {\r\n    if ('nullValue' in value) {\r\n        return MIN_VALUE;\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return { booleanValue: false };\r\n    }\r\n    else if ('integerValue' in value || 'doubleValue' in value) {\r\n        return { doubleValue: NaN };\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return { timestampValue: { seconds: Number.MIN_SAFE_INTEGER } };\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return { stringValue: '' };\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return { bytesValue: '' };\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return refValue(DatabaseId.empty(), DocumentKey.empty());\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return { geoPointValue: { latitude: -90, longitude: -180 } };\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return { arrayValue: {} };\r\n    }\r\n    else if ('mapValue' in value) {\r\n        return { mapValue: {} };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\n/** Returns the largest value for the given value type (exclusive). */\r\nfunction valuesGetUpperBound(value) {\r\n    if ('nullValue' in value) {\r\n        return { booleanValue: false };\r\n    }\r\n    else if ('booleanValue' in value) {\r\n        return { doubleValue: NaN };\r\n    }\r\n    else if ('integerValue' in value || 'doubleValue' in value) {\r\n        return { timestampValue: { seconds: Number.MIN_SAFE_INTEGER } };\r\n    }\r\n    else if ('timestampValue' in value) {\r\n        return { stringValue: '' };\r\n    }\r\n    else if ('stringValue' in value) {\r\n        return { bytesValue: '' };\r\n    }\r\n    else if ('bytesValue' in value) {\r\n        return refValue(DatabaseId.empty(), DocumentKey.empty());\r\n    }\r\n    else if ('referenceValue' in value) {\r\n        return { geoPointValue: { latitude: -90, longitude: -180 } };\r\n    }\r\n    else if ('geoPointValue' in value) {\r\n        return { arrayValue: {} };\r\n    }\r\n    else if ('arrayValue' in value) {\r\n        return { mapValue: {} };\r\n    }\r\n    else if ('mapValue' in value) {\r\n        return MAX_VALUE;\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction lowerBoundCompare(left, right) {\r\n    const cmp = valueCompare(left.value, right.value);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    if (left.inclusive && !right.inclusive) {\r\n        return -1;\r\n    }\r\n    else if (!left.inclusive && right.inclusive) {\r\n        return 1;\r\n    }\r\n    return 0;\r\n}\r\nfunction upperBoundCompare(left, right) {\r\n    const cmp = valueCompare(left.value, right.value);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    if (left.inclusive && !right.inclusive) {\r\n        return 1;\r\n    }\r\n    else if (!left.inclusive && right.inclusive) {\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An ObjectValue represents a MapValue in the Firestore Proto and offers the\r\n * ability to add and remove fields (via the ObjectValueBuilder).\r\n */\r\nclass ObjectValue {\r\n    constructor(value) {\r\n        this.value = value;\r\n    }\r\n    static empty() {\r\n        return new ObjectValue({ mapValue: {} });\r\n    }\r\n    /**\r\n     * Returns the value at the given path or null.\r\n     *\r\n     * @param path - the path to search\r\n     * @returns The value at the path or null if the path is not set.\r\n     */\r\n    field(path) {\r\n        if (path.isEmpty()) {\r\n            return this.value;\r\n        }\r\n        else {\r\n            let currentLevel = this.value;\r\n            for (let i = 0; i < path.length - 1; ++i) {\r\n                currentLevel = (currentLevel.mapValue.fields || {})[path.get(i)];\r\n                if (!isMapValue(currentLevel)) {\r\n                    return null;\r\n                }\r\n            }\r\n            currentLevel = (currentLevel.mapValue.fields || {})[path.lastSegment()];\r\n            return currentLevel || null;\r\n        }\r\n    }\r\n    /**\r\n     * Sets the field to the provided value.\r\n     *\r\n     * @param path - The field path to set.\r\n     * @param value - The value to set.\r\n     */\r\n    set(path, value) {\r\n        const fieldsMap = this.getFieldsMap(path.popLast());\r\n        fieldsMap[path.lastSegment()] = deepClone(value);\r\n    }\r\n    /**\r\n     * Sets the provided fields to the provided values.\r\n     *\r\n     * @param data - A map of fields to values (or null for deletes).\r\n     */\r\n    setAll(data) {\r\n        let parent = FieldPath$1.emptyPath();\r\n        let upserts = {};\r\n        let deletes = [];\r\n        data.forEach((value, path) => {\r\n            if (!parent.isImmediateParentOf(path)) {\r\n                // Insert the accumulated changes at this parent location\r\n                const fieldsMap = this.getFieldsMap(parent);\r\n                this.applyChanges(fieldsMap, upserts, deletes);\r\n                upserts = {};\r\n                deletes = [];\r\n                parent = path.popLast();\r\n            }\r\n            if (value) {\r\n                upserts[path.lastSegment()] = deepClone(value);\r\n            }\r\n            else {\r\n                deletes.push(path.lastSegment());\r\n            }\r\n        });\r\n        const fieldsMap = this.getFieldsMap(parent);\r\n        this.applyChanges(fieldsMap, upserts, deletes);\r\n    }\r\n    /**\r\n     * Removes the field at the specified path. If there is no field at the\r\n     * specified path, nothing is changed.\r\n     *\r\n     * @param path - The field path to remove.\r\n     */\r\n    delete(path) {\r\n        const nestedValue = this.field(path.popLast());\r\n        if (isMapValue(nestedValue) && nestedValue.mapValue.fields) {\r\n            delete nestedValue.mapValue.fields[path.lastSegment()];\r\n        }\r\n    }\r\n    isEqual(other) {\r\n        return valueEquals(this.value, other.value);\r\n    }\r\n    /**\r\n     * Returns the map that contains the leaf element of `path`. If the parent\r\n     * entry does not yet exist, or if it is not a map, a new map will be created.\r\n     */\r\n    getFieldsMap(path) {\r\n        let current = this.value;\r\n        if (!current.mapValue.fields) {\r\n            current.mapValue = { fields: {} };\r\n        }\r\n        for (let i = 0; i < path.length; ++i) {\r\n            let next = current.mapValue.fields[path.get(i)];\r\n            if (!isMapValue(next) || !next.mapValue.fields) {\r\n                next = { mapValue: { fields: {} } };\r\n                current.mapValue.fields[path.get(i)] = next;\r\n            }\r\n            current = next;\r\n        }\r\n        return current.mapValue.fields;\r\n    }\r\n    /**\r\n     * Modifies `fieldsMap` by adding, replacing or deleting the specified\r\n     * entries.\r\n     */\r\n    applyChanges(fieldsMap, inserts, deletes) {\r\n        forEach(inserts, (key, val) => (fieldsMap[key] = val));\r\n        for (const field of deletes) {\r\n            delete fieldsMap[field];\r\n        }\r\n    }\r\n    clone() {\r\n        return new ObjectValue(deepClone(this.value));\r\n    }\r\n}\r\n/**\r\n * Returns a FieldMask built from all fields in a MapValue.\r\n */\r\nfunction extractFieldMask(value) {\r\n    const fields = [];\r\n    forEach(value.fields, (key, value) => {\r\n        const currentPath = new FieldPath$1([key]);\r\n        if (isMapValue(value)) {\r\n            const nestedMask = extractFieldMask(value.mapValue);\r\n            const nestedFields = nestedMask.fields;\r\n            if (nestedFields.length === 0) {\r\n                // Preserve the empty map by adding it to the FieldMask.\r\n                fields.push(currentPath);\r\n            }\r\n            else {\r\n                // For nested and non-empty ObjectValues, add the FieldPath of the\r\n                // leaf nodes.\r\n                for (const nestedPath of nestedFields) {\r\n                    fields.push(currentPath.child(nestedPath));\r\n                }\r\n            }\r\n        }\r\n        else {\r\n            // For nested and non-empty ObjectValues, add the FieldPath of the leaf\r\n            // nodes.\r\n            fields.push(currentPath);\r\n        }\r\n    });\r\n    return new FieldMask(fields);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a document in Firestore with a key, version, data and whether it\r\n * has local mutations applied to it.\r\n *\r\n * Documents can transition between states via `convertToFoundDocument()`,\r\n * `convertToNoDocument()` and `convertToUnknownDocument()`. If a document does\r\n * not transition to one of these states even after all mutations have been\r\n * applied, `isValidDocument()` returns false and the document should be removed\r\n * from all views.\r\n */\r\nclass MutableDocument {\r\n    constructor(key, documentType, version, readTime, createTime, data, documentState) {\r\n        this.key = key;\r\n        this.documentType = documentType;\r\n        this.version = version;\r\n        this.readTime = readTime;\r\n        this.createTime = createTime;\r\n        this.data = data;\r\n        this.documentState = documentState;\r\n    }\r\n    /**\r\n     * Creates a document with no known version or data, but which can serve as\r\n     * base document for mutations.\r\n     */\r\n    static newInvalidDocument(documentKey) {\r\n        return new MutableDocument(documentKey, 0 /* DocumentType.INVALID */, \r\n        /* version */ SnapshotVersion.min(), \r\n        /* readTime */ SnapshotVersion.min(), \r\n        /* createTime */ SnapshotVersion.min(), ObjectValue.empty(), 0 /* DocumentState.SYNCED */);\r\n    }\r\n    /**\r\n     * Creates a new document that is known to exist with the given data at the\r\n     * given version.\r\n     */\r\n    static newFoundDocument(documentKey, version, createTime, value) {\r\n        return new MutableDocument(documentKey, 1 /* DocumentType.FOUND_DOCUMENT */, \r\n        /* version */ version, \r\n        /* readTime */ SnapshotVersion.min(), \r\n        /* createTime */ createTime, value, 0 /* DocumentState.SYNCED */);\r\n    }\r\n    /** Creates a new document that is known to not exist at the given version. */\r\n    static newNoDocument(documentKey, version) {\r\n        return new MutableDocument(documentKey, 2 /* DocumentType.NO_DOCUMENT */, \r\n        /* version */ version, \r\n        /* readTime */ SnapshotVersion.min(), \r\n        /* createTime */ SnapshotVersion.min(), ObjectValue.empty(), 0 /* DocumentState.SYNCED */);\r\n    }\r\n    /**\r\n     * Creates a new document that is known to exist at the given version but\r\n     * whose data is not known (e.g. a document that was updated without a known\r\n     * base document).\r\n     */\r\n    static newUnknownDocument(documentKey, version) {\r\n        return new MutableDocument(documentKey, 3 /* DocumentType.UNKNOWN_DOCUMENT */, \r\n        /* version */ version, \r\n        /* readTime */ SnapshotVersion.min(), \r\n        /* createTime */ SnapshotVersion.min(), ObjectValue.empty(), 2 /* DocumentState.HAS_COMMITTED_MUTATIONS */);\r\n    }\r\n    /**\r\n     * Changes the document type to indicate that it exists and that its version\r\n     * and data are known.\r\n     */\r\n    convertToFoundDocument(version, value) {\r\n        // If a document is switching state from being an invalid or deleted\r\n        // document to a valid (FOUND_DOCUMENT) document, either due to receiving an\r\n        // update from Watch or due to applying a local set mutation on top\r\n        // of a deleted document, our best guess about its createTime would be the\r\n        // version at which the document transitioned to a FOUND_DOCUMENT.\r\n        if (this.createTime.isEqual(SnapshotVersion.min()) &&\r\n            (this.documentType === 2 /* DocumentType.NO_DOCUMENT */ ||\r\n                this.documentType === 0 /* DocumentType.INVALID */)) {\r\n            this.createTime = version;\r\n        }\r\n        this.version = version;\r\n        this.documentType = 1 /* DocumentType.FOUND_DOCUMENT */;\r\n        this.data = value;\r\n        this.documentState = 0 /* DocumentState.SYNCED */;\r\n        return this;\r\n    }\r\n    /**\r\n     * Changes the document type to indicate that it doesn't exist at the given\r\n     * version.\r\n     */\r\n    convertToNoDocument(version) {\r\n        this.version = version;\r\n        this.documentType = 2 /* DocumentType.NO_DOCUMENT */;\r\n        this.data = ObjectValue.empty();\r\n        this.documentState = 0 /* DocumentState.SYNCED */;\r\n        return this;\r\n    }\r\n    /**\r\n     * Changes the document type to indicate that it exists at a given version but\r\n     * that its data is not known (e.g. a document that was updated without a known\r\n     * base document).\r\n     */\r\n    convertToUnknownDocument(version) {\r\n        this.version = version;\r\n        this.documentType = 3 /* DocumentType.UNKNOWN_DOCUMENT */;\r\n        this.data = ObjectValue.empty();\r\n        this.documentState = 2 /* DocumentState.HAS_COMMITTED_MUTATIONS */;\r\n        return this;\r\n    }\r\n    setHasCommittedMutations() {\r\n        this.documentState = 2 /* DocumentState.HAS_COMMITTED_MUTATIONS */;\r\n        return this;\r\n    }\r\n    setHasLocalMutations() {\r\n        this.documentState = 1 /* DocumentState.HAS_LOCAL_MUTATIONS */;\r\n        this.version = SnapshotVersion.min();\r\n        return this;\r\n    }\r\n    setReadTime(readTime) {\r\n        this.readTime = readTime;\r\n        return this;\r\n    }\r\n    get hasLocalMutations() {\r\n        return this.documentState === 1 /* DocumentState.HAS_LOCAL_MUTATIONS */;\r\n    }\r\n    get hasCommittedMutations() {\r\n        return this.documentState === 2 /* DocumentState.HAS_COMMITTED_MUTATIONS */;\r\n    }\r\n    get hasPendingWrites() {\r\n        return this.hasLocalMutations || this.hasCommittedMutations;\r\n    }\r\n    isValidDocument() {\r\n        return this.documentType !== 0 /* DocumentType.INVALID */;\r\n    }\r\n    isFoundDocument() {\r\n        return this.documentType === 1 /* DocumentType.FOUND_DOCUMENT */;\r\n    }\r\n    isNoDocument() {\r\n        return this.documentType === 2 /* DocumentType.NO_DOCUMENT */;\r\n    }\r\n    isUnknownDocument() {\r\n        return this.documentType === 3 /* DocumentType.UNKNOWN_DOCUMENT */;\r\n    }\r\n    isEqual(other) {\r\n        return (other instanceof MutableDocument &&\r\n            this.key.isEqual(other.key) &&\r\n            this.version.isEqual(other.version) &&\r\n            this.documentType === other.documentType &&\r\n            this.documentState === other.documentState &&\r\n            this.data.isEqual(other.data));\r\n    }\r\n    mutableCopy() {\r\n        return new MutableDocument(this.key, this.documentType, this.version, this.readTime, this.createTime, this.data.clone(), this.documentState);\r\n    }\r\n    toString() {\r\n        return (`Document(${this.key}, ${this.version}, ${JSON.stringify(this.data.value)}, ` +\r\n            `{createTime: ${this.createTime}}), ` +\r\n            `{documentType: ${this.documentType}}), ` +\r\n            `{documentState: ${this.documentState}})`);\r\n    }\r\n}\r\n/**\r\n * Compares the value for field `field` in the provided documents. Throws if\r\n * the field does not exist in both documents.\r\n */\r\nfunction compareDocumentsByField(field, d1, d2) {\r\n    const v1 = d1.data.field(field);\r\n    const v2 = d2.data.field(field);\r\n    if (v1 !== null && v2 !== null) {\r\n        return valueCompare(v1, v2);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a bound of a query.\r\n *\r\n * The bound is specified with the given components representing a position and\r\n * whether it's just before or just after the position (relative to whatever the\r\n * query order is).\r\n *\r\n * The position represents a logical index position for a query. It's a prefix\r\n * of values for the (potentially implicit) order by clauses of a query.\r\n *\r\n * Bound provides a function to determine whether a document comes before or\r\n * after a bound. This is influenced by whether the position is just before or\r\n * just after the provided values.\r\n */\r\nclass Bound {\r\n    constructor(position, inclusive) {\r\n        this.position = position;\r\n        this.inclusive = inclusive;\r\n    }\r\n}\r\nfunction boundCompareToDocument(bound, orderBy, doc) {\r\n    let comparison = 0;\r\n    for (let i = 0; i < bound.position.length; i++) {\r\n        const orderByComponent = orderBy[i];\r\n        const component = bound.position[i];\r\n        if (orderByComponent.field.isKeyField()) {\r\n            comparison = DocumentKey.comparator(DocumentKey.fromName(component.referenceValue), doc.key);\r\n        }\r\n        else {\r\n            const docValue = doc.data.field(orderByComponent.field);\r\n            comparison = valueCompare(component, docValue);\r\n        }\r\n        if (orderByComponent.dir === \"desc\" /* Direction.DESCENDING */) {\r\n            comparison = comparison * -1;\r\n        }\r\n        if (comparison !== 0) {\r\n            break;\r\n        }\r\n    }\r\n    return comparison;\r\n}\r\n/**\r\n * Returns true if a document sorts after a bound using the provided sort\r\n * order.\r\n */\r\nfunction boundSortsAfterDocument(bound, orderBy, doc) {\r\n    const comparison = boundCompareToDocument(bound, orderBy, doc);\r\n    return bound.inclusive ? comparison >= 0 : comparison > 0;\r\n}\r\n/**\r\n * Returns true if a document sorts before a bound using the provided sort\r\n * order.\r\n */\r\nfunction boundSortsBeforeDocument(bound, orderBy, doc) {\r\n    const comparison = boundCompareToDocument(bound, orderBy, doc);\r\n    return bound.inclusive ? comparison <= 0 : comparison < 0;\r\n}\r\nfunction boundEquals(left, right) {\r\n    if (left === null) {\r\n        return right === null;\r\n    }\r\n    else if (right === null) {\r\n        return false;\r\n    }\r\n    if (left.inclusive !== right.inclusive ||\r\n        left.position.length !== right.position.length) {\r\n        return false;\r\n    }\r\n    for (let i = 0; i < left.position.length; i++) {\r\n        const leftPosition = left.position[i];\r\n        const rightPosition = right.position[i];\r\n        if (!valueEquals(leftPosition, rightPosition)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An ordering on a field, in some Direction. Direction defaults to ASCENDING.\r\n */\r\nclass OrderBy {\r\n    constructor(field, dir = \"asc\" /* Direction.ASCENDING */) {\r\n        this.field = field;\r\n        this.dir = dir;\r\n    }\r\n}\r\nfunction canonifyOrderBy(orderBy) {\r\n    // TODO(b/29183165): Make this collision robust.\r\n    return orderBy.field.canonicalString() + orderBy.dir;\r\n}\r\nfunction stringifyOrderBy(orderBy) {\r\n    return `${orderBy.field.canonicalString()} (${orderBy.dir})`;\r\n}\r\nfunction orderByEquals(left, right) {\r\n    return left.dir === right.dir && left.field.isEqual(right.field);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass Filter {\r\n}\r\nclass FieldFilter extends Filter {\r\n    constructor(field, op, value) {\r\n        super();\r\n        this.field = field;\r\n        this.op = op;\r\n        this.value = value;\r\n    }\r\n    /**\r\n     * Creates a filter based on the provided arguments.\r\n     */\r\n    static create(field, op, value) {\r\n        if (field.isKeyField()) {\r\n            if (op === \"in\" /* Operator.IN */ || op === \"not-in\" /* Operator.NOT_IN */) {\r\n                return this.createKeyFieldInFilter(field, op, value);\r\n            }\r\n            else {\r\n                return new KeyFieldFilter(field, op, value);\r\n            }\r\n        }\r\n        else if (op === \"array-contains\" /* Operator.ARRAY_CONTAINS */) {\r\n            return new ArrayContainsFilter(field, value);\r\n        }\r\n        else if (op === \"in\" /* Operator.IN */) {\r\n            return new InFilter(field, value);\r\n        }\r\n        else if (op === \"not-in\" /* Operator.NOT_IN */) {\r\n            return new NotInFilter(field, value);\r\n        }\r\n        else if (op === \"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */) {\r\n            return new ArrayContainsAnyFilter(field, value);\r\n        }\r\n        else {\r\n            return new FieldFilter(field, op, value);\r\n        }\r\n    }\r\n    static createKeyFieldInFilter(field, op, value) {\r\n        return op === \"in\" /* Operator.IN */\r\n            ? new KeyFieldInFilter(field, value)\r\n            : new KeyFieldNotInFilter(field, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        // Types do not have to match in NOT_EQUAL filters.\r\n        if (this.op === \"!=\" /* Operator.NOT_EQUAL */) {\r\n            return (other !== null &&\r\n                this.matchesComparison(valueCompare(other, this.value)));\r\n        }\r\n        // Only compare types with matching backend order (such as double and int).\r\n        return (other !== null &&\r\n            typeOrder(this.value) === typeOrder(other) &&\r\n            this.matchesComparison(valueCompare(other, this.value)));\r\n    }\r\n    matchesComparison(comparison) {\r\n        switch (this.op) {\r\n            case \"<\" /* Operator.LESS_THAN */:\r\n                return comparison < 0;\r\n            case \"<=\" /* Operator.LESS_THAN_OR_EQUAL */:\r\n                return comparison <= 0;\r\n            case \"==\" /* Operator.EQUAL */:\r\n                return comparison === 0;\r\n            case \"!=\" /* Operator.NOT_EQUAL */:\r\n                return comparison !== 0;\r\n            case \">\" /* Operator.GREATER_THAN */:\r\n                return comparison > 0;\r\n            case \">=\" /* Operator.GREATER_THAN_OR_EQUAL */:\r\n                return comparison >= 0;\r\n            default:\r\n                return fail();\r\n        }\r\n    }\r\n    isInequality() {\r\n        return ([\r\n            \"<\" /* Operator.LESS_THAN */,\r\n            \"<=\" /* Operator.LESS_THAN_OR_EQUAL */,\r\n            \">\" /* Operator.GREATER_THAN */,\r\n            \">=\" /* Operator.GREATER_THAN_OR_EQUAL */,\r\n            \"!=\" /* Operator.NOT_EQUAL */,\r\n            \"not-in\" /* Operator.NOT_IN */\r\n        ].indexOf(this.op) >= 0);\r\n    }\r\n    getFlattenedFilters() {\r\n        return [this];\r\n    }\r\n    getFilters() {\r\n        return [this];\r\n    }\r\n    getFirstInequalityField() {\r\n        if (this.isInequality()) {\r\n            return this.field;\r\n        }\r\n        return null;\r\n    }\r\n}\r\nclass CompositeFilter extends Filter {\r\n    constructor(filters, op) {\r\n        super();\r\n        this.filters = filters;\r\n        this.op = op;\r\n        this.memoizedFlattenedFilters = null;\r\n    }\r\n    /**\r\n     * Creates a filter based on the provided arguments.\r\n     */\r\n    static create(filters, op) {\r\n        return new CompositeFilter(filters, op);\r\n    }\r\n    matches(doc) {\r\n        if (compositeFilterIsConjunction(this)) {\r\n            // For conjunctions, all filters must match, so return false if any filter doesn't match.\r\n            return this.filters.find(filter => !filter.matches(doc)) === undefined;\r\n        }\r\n        else {\r\n            // For disjunctions, at least one filter should match.\r\n            return this.filters.find(filter => filter.matches(doc)) !== undefined;\r\n        }\r\n    }\r\n    getFlattenedFilters() {\r\n        if (this.memoizedFlattenedFilters !== null) {\r\n            return this.memoizedFlattenedFilters;\r\n        }\r\n        this.memoizedFlattenedFilters = this.filters.reduce((result, subfilter) => {\r\n            return result.concat(subfilter.getFlattenedFilters());\r\n        }, []);\r\n        return this.memoizedFlattenedFilters;\r\n    }\r\n    // Returns a mutable copy of `this.filters`\r\n    getFilters() {\r\n        return Object.assign([], this.filters);\r\n    }\r\n    getFirstInequalityField() {\r\n        const found = this.findFirstMatchingFilter(filter => filter.isInequality());\r\n        if (found !== null) {\r\n            return found.field;\r\n        }\r\n        return null;\r\n    }\r\n    // Performs a depth-first search to find and return the first FieldFilter in the composite filter\r\n    // that satisfies the predicate. Returns `null` if none of the FieldFilters satisfy the\r\n    // predicate.\r\n    findFirstMatchingFilter(predicate) {\r\n        for (const fieldFilter of this.getFlattenedFilters()) {\r\n            if (predicate(fieldFilter)) {\r\n                return fieldFilter;\r\n            }\r\n        }\r\n        return null;\r\n    }\r\n}\r\nfunction compositeFilterIsConjunction(compositeFilter) {\r\n    return compositeFilter.op === \"and\" /* CompositeOperator.AND */;\r\n}\r\nfunction compositeFilterIsDisjunction(compositeFilter) {\r\n    return compositeFilter.op === \"or\" /* CompositeOperator.OR */;\r\n}\r\n/**\r\n * Returns true if this filter is a conjunction of field filters only. Returns false otherwise.\r\n */\r\nfunction compositeFilterIsFlatConjunction(compositeFilter) {\r\n    return (compositeFilterIsFlat(compositeFilter) &&\r\n        compositeFilterIsConjunction(compositeFilter));\r\n}\r\n/**\r\n * Returns true if this filter does not contain any composite filters. Returns false otherwise.\r\n */\r\nfunction compositeFilterIsFlat(compositeFilter) {\r\n    for (const filter of compositeFilter.filters) {\r\n        if (filter instanceof CompositeFilter) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\r\nfunction canonifyFilter(filter) {\r\n    if (filter instanceof FieldFilter) {\r\n        // TODO(b/29183165): Technically, this won't be unique if two values have\r\n        // the same description, such as the int 3 and the string \"3\". So we should\r\n        // add the types in here somehow, too.\r\n        return (filter.field.canonicalString() +\r\n            filter.op.toString() +\r\n            canonicalId(filter.value));\r\n    }\r\n    else if (compositeFilterIsFlatConjunction(filter)) {\r\n        // Older SDK versions use an implicit AND operation between their filters.\r\n        // In the new SDK versions, the developer may use an explicit AND filter.\r\n        // To stay consistent with the old usages, we add a special case to ensure\r\n        // the canonical ID for these two are the same. For example:\r\n        // `col.whereEquals(\"a\", 1).whereEquals(\"b\", 2)` should have the same\r\n        // canonical ID as `col.where(and(equals(\"a\",1), equals(\"b\",2)))`.\r\n        return filter.filters.map(filter => canonifyFilter(filter)).join(',');\r\n    }\r\n    else {\r\n        // filter instanceof CompositeFilter\r\n        const canonicalIdsString = filter.filters\r\n            .map(filter => canonifyFilter(filter))\r\n            .join(',');\r\n        return `${filter.op}(${canonicalIdsString})`;\r\n    }\r\n}\r\nfunction filterEquals(f1, f2) {\r\n    if (f1 instanceof FieldFilter) {\r\n        return fieldFilterEquals(f1, f2);\r\n    }\r\n    else if (f1 instanceof CompositeFilter) {\r\n        return compositeFilterEquals(f1, f2);\r\n    }\r\n    else {\r\n        fail();\r\n    }\r\n}\r\nfunction fieldFilterEquals(f1, f2) {\r\n    return (f2 instanceof FieldFilter &&\r\n        f1.op === f2.op &&\r\n        f1.field.isEqual(f2.field) &&\r\n        valueEquals(f1.value, f2.value));\r\n}\r\nfunction compositeFilterEquals(f1, f2) {\r\n    if (f2 instanceof CompositeFilter &&\r\n        f1.op === f2.op &&\r\n        f1.filters.length === f2.filters.length) {\r\n        const subFiltersMatch = f1.filters.reduce((result, f1Filter, index) => result && filterEquals(f1Filter, f2.filters[index]), true);\r\n        return subFiltersMatch;\r\n    }\r\n    return false;\r\n}\r\n/**\r\n * Returns a new composite filter that contains all filter from\r\n * `compositeFilter` plus all the given filters in `otherFilters`.\r\n */\r\nfunction compositeFilterWithAddedFilters(compositeFilter, otherFilters) {\r\n    const mergedFilters = compositeFilter.filters.concat(otherFilters);\r\n    return CompositeFilter.create(mergedFilters, compositeFilter.op);\r\n}\r\n/** Returns a debug description for `filter`. */\r\nfunction stringifyFilter(filter) {\r\n    if (filter instanceof FieldFilter) {\r\n        return stringifyFieldFilter(filter);\r\n    }\r\n    else if (filter instanceof CompositeFilter) {\r\n        return stringifyCompositeFilter(filter);\r\n    }\r\n    else {\r\n        return 'Filter';\r\n    }\r\n}\r\nfunction stringifyCompositeFilter(filter) {\r\n    return (filter.op.toString() +\r\n        ` {` +\r\n        filter.getFilters().map(stringifyFilter).join(' ,') +\r\n        '}');\r\n}\r\nfunction stringifyFieldFilter(filter) {\r\n    return `${filter.field.canonicalString()} ${filter.op} ${canonicalId(filter.value)}`;\r\n}\r\n/** Filter that matches on key fields (i.e. '__name__'). */\r\nclass KeyFieldFilter extends FieldFilter {\r\n    constructor(field, op, value) {\r\n        super(field, op, value);\r\n        this.key = DocumentKey.fromName(value.referenceValue);\r\n    }\r\n    matches(doc) {\r\n        const comparison = DocumentKey.comparator(doc.key, this.key);\r\n        return this.matchesComparison(comparison);\r\n    }\r\n}\r\n/** Filter that matches on key fields within an array. */\r\nclass KeyFieldInFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"in\" /* Operator.IN */, value);\r\n        this.keys = extractDocumentKeysFromArrayValue(\"in\" /* Operator.IN */, value);\r\n    }\r\n    matches(doc) {\r\n        return this.keys.some(key => key.isEqual(doc.key));\r\n    }\r\n}\r\n/** Filter that matches on key fields not present within an array. */\r\nclass KeyFieldNotInFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"not-in\" /* Operator.NOT_IN */, value);\r\n        this.keys = extractDocumentKeysFromArrayValue(\"not-in\" /* Operator.NOT_IN */, value);\r\n    }\r\n    matches(doc) {\r\n        return !this.keys.some(key => key.isEqual(doc.key));\r\n    }\r\n}\r\nfunction extractDocumentKeysFromArrayValue(op, value) {\r\n    var _a;\r\n    return (((_a = value.arrayValue) === null || _a === void 0 ? void 0 : _a.values) || []).map(v => {\r\n        return DocumentKey.fromName(v.referenceValue);\r\n    });\r\n}\r\n/** A Filter that implements the array-contains operator. */\r\nclass ArrayContainsFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"array-contains\" /* Operator.ARRAY_CONTAINS */, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        return isArray(other) && arrayValueContains(other.arrayValue, this.value);\r\n    }\r\n}\r\n/** A Filter that implements the IN operator. */\r\nclass InFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"in\" /* Operator.IN */, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        return other !== null && arrayValueContains(this.value.arrayValue, other);\r\n    }\r\n}\r\n/** A Filter that implements the not-in operator. */\r\nclass NotInFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"not-in\" /* Operator.NOT_IN */, value);\r\n    }\r\n    matches(doc) {\r\n        if (arrayValueContains(this.value.arrayValue, { nullValue: 'NULL_VALUE' })) {\r\n            return false;\r\n        }\r\n        const other = doc.data.field(this.field);\r\n        return other !== null && !arrayValueContains(this.value.arrayValue, other);\r\n    }\r\n}\r\n/** A Filter that implements the array-contains-any operator. */\r\nclass ArrayContainsAnyFilter extends FieldFilter {\r\n    constructor(field, value) {\r\n        super(field, \"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */, value);\r\n    }\r\n    matches(doc) {\r\n        const other = doc.data.field(this.field);\r\n        if (!isArray(other) || !other.arrayValue.values) {\r\n            return false;\r\n        }\r\n        return other.arrayValue.values.some(val => arrayValueContains(this.value.arrayValue, val));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// Visible for testing\r\nclass TargetImpl {\r\n    constructor(path, collectionGroup = null, orderBy = [], filters = [], limit = null, startAt = null, endAt = null) {\r\n        this.path = path;\r\n        this.collectionGroup = collectionGroup;\r\n        this.orderBy = orderBy;\r\n        this.filters = filters;\r\n        this.limit = limit;\r\n        this.startAt = startAt;\r\n        this.endAt = endAt;\r\n        this.memoizedCanonicalId = null;\r\n    }\r\n}\r\n/**\r\n * Initializes a Target with a path and optional additional query constraints.\r\n * Path must currently be empty if this is a collection group query.\r\n *\r\n * NOTE: you should always construct `Target` from `Query.toTarget` instead of\r\n * using this factory method, because `Query` provides an implicit `orderBy`\r\n * property.\r\n */\r\nfunction newTarget(path, collectionGroup = null, orderBy = [], filters = [], limit = null, startAt = null, endAt = null) {\r\n    return new TargetImpl(path, collectionGroup, orderBy, filters, limit, startAt, endAt);\r\n}\r\nfunction canonifyTarget(target) {\r\n    const targetImpl = debugCast(target);\r\n    if (targetImpl.memoizedCanonicalId === null) {\r\n        let str = targetImpl.path.canonicalString();\r\n        if (targetImpl.collectionGroup !== null) {\r\n            str += '|cg:' + targetImpl.collectionGroup;\r\n        }\r\n        str += '|f:';\r\n        str += targetImpl.filters.map(f => canonifyFilter(f)).join(',');\r\n        str += '|ob:';\r\n        str += targetImpl.orderBy.map(o => canonifyOrderBy(o)).join(',');\r\n        if (!isNullOrUndefined(targetImpl.limit)) {\r\n            str += '|l:';\r\n            str += targetImpl.limit;\r\n        }\r\n        if (targetImpl.startAt) {\r\n            str += '|lb:';\r\n            str += targetImpl.startAt.inclusive ? 'b:' : 'a:';\r\n            str += targetImpl.startAt.position.map(p => canonicalId(p)).join(',');\r\n        }\r\n        if (targetImpl.endAt) {\r\n            str += '|ub:';\r\n            str += targetImpl.endAt.inclusive ? 'a:' : 'b:';\r\n            str += targetImpl.endAt.position.map(p => canonicalId(p)).join(',');\r\n        }\r\n        targetImpl.memoizedCanonicalId = str;\r\n    }\r\n    return targetImpl.memoizedCanonicalId;\r\n}\r\nfunction stringifyTarget(target) {\r\n    let str = target.path.canonicalString();\r\n    if (target.collectionGroup !== null) {\r\n        str += ' collectionGroup=' + target.collectionGroup;\r\n    }\r\n    if (target.filters.length > 0) {\r\n        str += `, filters: [${target.filters\r\n            .map(f => stringifyFilter(f))\r\n            .join(', ')}]`;\r\n    }\r\n    if (!isNullOrUndefined(target.limit)) {\r\n        str += ', limit: ' + target.limit;\r\n    }\r\n    if (target.orderBy.length > 0) {\r\n        str += `, orderBy: [${target.orderBy\r\n            .map(o => stringifyOrderBy(o))\r\n            .join(', ')}]`;\r\n    }\r\n    if (target.startAt) {\r\n        str += ', startAt: ';\r\n        str += target.startAt.inclusive ? 'b:' : 'a:';\r\n        str += target.startAt.position.map(p => canonicalId(p)).join(',');\r\n    }\r\n    if (target.endAt) {\r\n        str += ', endAt: ';\r\n        str += target.endAt.inclusive ? 'a:' : 'b:';\r\n        str += target.endAt.position.map(p => canonicalId(p)).join(',');\r\n    }\r\n    return `Target(${str})`;\r\n}\r\nfunction targetEquals(left, right) {\r\n    if (left.limit !== right.limit) {\r\n        return false;\r\n    }\r\n    if (left.orderBy.length !== right.orderBy.length) {\r\n        return false;\r\n    }\r\n    for (let i = 0; i < left.orderBy.length; i++) {\r\n        if (!orderByEquals(left.orderBy[i], right.orderBy[i])) {\r\n            return false;\r\n        }\r\n    }\r\n    if (left.filters.length !== right.filters.length) {\r\n        return false;\r\n    }\r\n    for (let i = 0; i < left.filters.length; i++) {\r\n        if (!filterEquals(left.filters[i], right.filters[i])) {\r\n            return false;\r\n        }\r\n    }\r\n    if (left.collectionGroup !== right.collectionGroup) {\r\n        return false;\r\n    }\r\n    if (!left.path.isEqual(right.path)) {\r\n        return false;\r\n    }\r\n    if (!boundEquals(left.startAt, right.startAt)) {\r\n        return false;\r\n    }\r\n    return boundEquals(left.endAt, right.endAt);\r\n}\r\nfunction targetIsDocumentTarget(target) {\r\n    return (DocumentKey.isDocumentKey(target.path) &&\r\n        target.collectionGroup === null &&\r\n        target.filters.length === 0);\r\n}\r\n/** Returns the field filters that target the given field path. */\r\nfunction targetGetFieldFiltersForPath(target, path) {\r\n    return target.filters.filter(f => f instanceof FieldFilter && f.field.isEqual(path));\r\n}\r\n/**\r\n * Returns the values that are used in ARRAY_CONTAINS or ARRAY_CONTAINS_ANY\r\n * filters. Returns `null` if there are no such filters.\r\n */\r\nfunction targetGetArrayValues(target, fieldIndex) {\r\n    const segment = fieldIndexGetArraySegment(fieldIndex);\r\n    if (segment === undefined) {\r\n        return null;\r\n    }\r\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, segment.fieldPath)) {\r\n        switch (fieldFilter.op) {\r\n            case \"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */:\r\n                return fieldFilter.value.arrayValue.values || [];\r\n            case \"array-contains\" /* Operator.ARRAY_CONTAINS */:\r\n                return [fieldFilter.value];\r\n            // Remaining filters are not array filters.\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Returns the list of values that are used in != or NOT_IN filters. Returns\r\n * `null` if there are no such filters.\r\n */\r\nfunction targetGetNotInValues(target, fieldIndex) {\r\n    const values = new Map();\r\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n        for (const fieldFilter of targetGetFieldFiltersForPath(target, segment.fieldPath)) {\r\n            switch (fieldFilter.op) {\r\n                case \"==\" /* Operator.EQUAL */:\r\n                case \"in\" /* Operator.IN */:\r\n                    // Encode equality prefix, which is encoded in the index value before\r\n                    // the inequality (e.g. `a == 'a' && b != 'b'` is encoded to\r\n                    // `value != 'ab'`).\r\n                    values.set(segment.fieldPath.canonicalString(), fieldFilter.value);\r\n                    break;\r\n                case \"not-in\" /* Operator.NOT_IN */:\r\n                case \"!=\" /* Operator.NOT_EQUAL */:\r\n                    // NotIn/NotEqual is always a suffix. There cannot be any remaining\r\n                    // segments and hence we can return early here.\r\n                    values.set(segment.fieldPath.canonicalString(), fieldFilter.value);\r\n                    return Array.from(values.values());\r\n                // Remaining filters cannot be used as notIn bounds.\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Returns a lower bound of field values that can be used as a starting point to\r\n * scan the index defined by `fieldIndex`. Returns `MIN_VALUE` if no lower bound\r\n * exists.\r\n */\r\nfunction targetGetLowerBound(target, fieldIndex) {\r\n    const values = [];\r\n    let inclusive = true;\r\n    // For each segment, retrieve a lower bound if there is a suitable filter or\r\n    // startAt.\r\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n        const segmentBound = segment.kind === 0 /* IndexKind.ASCENDING */\r\n            ? targetGetAscendingBound(target, segment.fieldPath, target.startAt)\r\n            : targetGetDescendingBound(target, segment.fieldPath, target.startAt);\r\n        values.push(segmentBound.value);\r\n        inclusive && (inclusive = segmentBound.inclusive);\r\n    }\r\n    return new Bound(values, inclusive);\r\n}\r\n/**\r\n * Returns an upper bound of field values that can be used as an ending point\r\n * when scanning the index defined by `fieldIndex`. Returns `MAX_VALUE` if no\r\n * upper bound exists.\r\n */\r\nfunction targetGetUpperBound(target, fieldIndex) {\r\n    const values = [];\r\n    let inclusive = true;\r\n    // For each segment, retrieve an upper bound if there is a suitable filter or\r\n    // endAt.\r\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n        const segmentBound = segment.kind === 0 /* IndexKind.ASCENDING */\r\n            ? targetGetDescendingBound(target, segment.fieldPath, target.endAt)\r\n            : targetGetAscendingBound(target, segment.fieldPath, target.endAt);\r\n        values.push(segmentBound.value);\r\n        inclusive && (inclusive = segmentBound.inclusive);\r\n    }\r\n    return new Bound(values, inclusive);\r\n}\r\n/**\r\n * Returns the value to use as the lower bound for ascending index segment at\r\n * the provided `fieldPath` (or the upper bound for an descending segment).\r\n */\r\nfunction targetGetAscendingBound(target, fieldPath, bound) {\r\n    let value = MIN_VALUE;\r\n    let inclusive = true;\r\n    // Process all filters to find a value for the current field segment\r\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, fieldPath)) {\r\n        let filterValue = MIN_VALUE;\r\n        let filterInclusive = true;\r\n        switch (fieldFilter.op) {\r\n            case \"<\" /* Operator.LESS_THAN */:\r\n            case \"<=\" /* Operator.LESS_THAN_OR_EQUAL */:\r\n                filterValue = valuesGetLowerBound(fieldFilter.value);\r\n                break;\r\n            case \"==\" /* Operator.EQUAL */:\r\n            case \"in\" /* Operator.IN */:\r\n            case \">=\" /* Operator.GREATER_THAN_OR_EQUAL */:\r\n                filterValue = fieldFilter.value;\r\n                break;\r\n            case \">\" /* Operator.GREATER_THAN */:\r\n                filterValue = fieldFilter.value;\r\n                filterInclusive = false;\r\n                break;\r\n            case \"!=\" /* Operator.NOT_EQUAL */:\r\n            case \"not-in\" /* Operator.NOT_IN */:\r\n                filterValue = MIN_VALUE;\r\n                break;\r\n            // Remaining filters cannot be used as lower bounds.\r\n        }\r\n        if (lowerBoundCompare({ value, inclusive }, { value: filterValue, inclusive: filterInclusive }) < 0) {\r\n            value = filterValue;\r\n            inclusive = filterInclusive;\r\n        }\r\n    }\r\n    // If there is an additional bound, compare the values against the existing\r\n    // range to see if we can narrow the scope.\r\n    if (bound !== null) {\r\n        for (let i = 0; i < target.orderBy.length; ++i) {\r\n            const orderBy = target.orderBy[i];\r\n            if (orderBy.field.isEqual(fieldPath)) {\r\n                const cursorValue = bound.position[i];\r\n                if (lowerBoundCompare({ value, inclusive }, { value: cursorValue, inclusive: bound.inclusive }) < 0) {\r\n                    value = cursorValue;\r\n                    inclusive = bound.inclusive;\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return { value, inclusive };\r\n}\r\n/**\r\n * Returns the value to use as the upper bound for ascending index segment at\r\n * the provided `fieldPath` (or the lower bound for a descending segment).\r\n */\r\nfunction targetGetDescendingBound(target, fieldPath, bound) {\r\n    let value = MAX_VALUE;\r\n    let inclusive = true;\r\n    // Process all filters to find a value for the current field segment\r\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, fieldPath)) {\r\n        let filterValue = MAX_VALUE;\r\n        let filterInclusive = true;\r\n        switch (fieldFilter.op) {\r\n            case \">=\" /* Operator.GREATER_THAN_OR_EQUAL */:\r\n            case \">\" /* Operator.GREATER_THAN */:\r\n                filterValue = valuesGetUpperBound(fieldFilter.value);\r\n                filterInclusive = false;\r\n                break;\r\n            case \"==\" /* Operator.EQUAL */:\r\n            case \"in\" /* Operator.IN */:\r\n            case \"<=\" /* Operator.LESS_THAN_OR_EQUAL */:\r\n                filterValue = fieldFilter.value;\r\n                break;\r\n            case \"<\" /* Operator.LESS_THAN */:\r\n                filterValue = fieldFilter.value;\r\n                filterInclusive = false;\r\n                break;\r\n            case \"!=\" /* Operator.NOT_EQUAL */:\r\n            case \"not-in\" /* Operator.NOT_IN */:\r\n                filterValue = MAX_VALUE;\r\n                break;\r\n            // Remaining filters cannot be used as upper bounds.\r\n        }\r\n        if (upperBoundCompare({ value, inclusive }, { value: filterValue, inclusive: filterInclusive }) > 0) {\r\n            value = filterValue;\r\n            inclusive = filterInclusive;\r\n        }\r\n    }\r\n    // If there is an additional bound, compare the values against the existing\r\n    // range to see if we can narrow the scope.\r\n    if (bound !== null) {\r\n        for (let i = 0; i < target.orderBy.length; ++i) {\r\n            const orderBy = target.orderBy[i];\r\n            if (orderBy.field.isEqual(fieldPath)) {\r\n                const cursorValue = bound.position[i];\r\n                if (upperBoundCompare({ value, inclusive }, { value: cursorValue, inclusive: bound.inclusive }) > 0) {\r\n                    value = cursorValue;\r\n                    inclusive = bound.inclusive;\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return { value, inclusive };\r\n}\r\n/** Returns the number of segments of a perfect index for this target. */\r\nfunction targetGetSegmentCount(target) {\r\n    let fields = new SortedSet(FieldPath$1.comparator);\r\n    let hasArraySegment = false;\r\n    for (const filter of target.filters) {\r\n        for (const subFilter of filter.getFlattenedFilters()) {\r\n            // __name__ is not an explicit segment of any index, so we don't need to\r\n            // count it.\r\n            if (subFilter.field.isKeyField()) {\r\n                continue;\r\n            }\r\n            // ARRAY_CONTAINS or ARRAY_CONTAINS_ANY filters must be counted separately.\r\n            // For instance, it is possible to have an index for \"a ARRAY a ASC\". Even\r\n            // though these are on the same field, they should be counted as two\r\n            // separate segments in an index.\r\n            if (subFilter.op === \"array-contains\" /* Operator.ARRAY_CONTAINS */ ||\r\n                subFilter.op === \"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */) {\r\n                hasArraySegment = true;\r\n            }\r\n            else {\r\n                fields = fields.add(subFilter.field);\r\n            }\r\n        }\r\n    }\r\n    for (const orderBy of target.orderBy) {\r\n        // __name__ is not an explicit segment of any index, so we don't need to\r\n        // count it.\r\n        if (!orderBy.field.isKeyField()) {\r\n            fields = fields.add(orderBy.field);\r\n        }\r\n    }\r\n    return fields.size + (hasArraySegment ? 1 : 0);\r\n}\r\nfunction targetHasLimit(target) {\r\n    return target.limit !== null;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Query encapsulates all the query attributes we support in the SDK. It can\r\n * be run against the LocalStore, as well as be converted to a `Target` to\r\n * query the RemoteStore results.\r\n *\r\n * Visible for testing.\r\n */\r\nclass QueryImpl {\r\n    /**\r\n     * Initializes a Query with a path and optional additional query constraints.\r\n     * Path must currently be empty if this is a collection group query.\r\n     */\r\n    constructor(path, collectionGroup = null, explicitOrderBy = [], filters = [], limit = null, limitType = \"F\" /* LimitType.First */, startAt = null, endAt = null) {\r\n        this.path = path;\r\n        this.collectionGroup = collectionGroup;\r\n        this.explicitOrderBy = explicitOrderBy;\r\n        this.filters = filters;\r\n        this.limit = limit;\r\n        this.limitType = limitType;\r\n        this.startAt = startAt;\r\n        this.endAt = endAt;\r\n        this.memoizedOrderBy = null;\r\n        // The corresponding `Target` of this `Query` instance.\r\n        this.memoizedTarget = null;\r\n        if (this.startAt) ;\r\n        if (this.endAt) ;\r\n    }\r\n}\r\n/** Creates a new Query instance with the options provided. */\r\nfunction newQuery(path, collectionGroup, explicitOrderBy, filters, limit, limitType, startAt, endAt) {\r\n    return new QueryImpl(path, collectionGroup, explicitOrderBy, filters, limit, limitType, startAt, endAt);\r\n}\r\n/** Creates a new Query for a query that matches all documents at `path` */\r\nfunction newQueryForPath(path) {\r\n    return new QueryImpl(path);\r\n}\r\n/**\r\n * Helper to convert a collection group query into a collection query at a\r\n * specific path. This is used when executing collection group queries, since\r\n * we have to split the query into a set of collection queries at multiple\r\n * paths.\r\n */\r\nfunction asCollectionQueryAtPath(query, path) {\r\n    return new QueryImpl(path, \r\n    /*collectionGroup=*/ null, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, query.startAt, query.endAt);\r\n}\r\n/**\r\n * Returns true if this query does not specify any query constraints that\r\n * could remove results.\r\n */\r\nfunction queryMatchesAllDocuments(query) {\r\n    return (query.filters.length === 0 &&\r\n        query.limit === null &&\r\n        query.startAt == null &&\r\n        query.endAt == null &&\r\n        (query.explicitOrderBy.length === 0 ||\r\n            (query.explicitOrderBy.length === 1 &&\r\n                query.explicitOrderBy[0].field.isKeyField())));\r\n}\r\nfunction getFirstOrderByField(query) {\r\n    return query.explicitOrderBy.length > 0\r\n        ? query.explicitOrderBy[0].field\r\n        : null;\r\n}\r\nfunction getInequalityFilterField(query) {\r\n    for (const filter of query.filters) {\r\n        const result = filter.getFirstInequalityField();\r\n        if (result !== null) {\r\n            return result;\r\n        }\r\n    }\r\n    return null;\r\n}\r\n/**\r\n * Creates a new Query for a collection group query that matches all documents\r\n * within the provided collection group.\r\n */\r\nfunction newQueryForCollectionGroup(collectionId) {\r\n    return new QueryImpl(ResourcePath.emptyPath(), collectionId);\r\n}\r\n/**\r\n * Returns whether the query matches a single document by path (rather than a\r\n * collection).\r\n */\r\nfunction isDocumentQuery$1(query) {\r\n    return (DocumentKey.isDocumentKey(query.path) &&\r\n        query.collectionGroup === null &&\r\n        query.filters.length === 0);\r\n}\r\n/**\r\n * Returns whether the query matches a collection group rather than a specific\r\n * collection.\r\n */\r\nfunction isCollectionGroupQuery(query) {\r\n    return query.collectionGroup !== null;\r\n}\r\n/**\r\n * Returns the implicit order by constraint that is used to execute the Query,\r\n * which can be different from the order by constraints the user provided (e.g.\r\n * the SDK and backend always orders by `__name__`).\r\n */\r\nfunction queryOrderBy(query) {\r\n    const queryImpl = debugCast(query);\r\n    if (queryImpl.memoizedOrderBy === null) {\r\n        queryImpl.memoizedOrderBy = [];\r\n        const inequalityField = getInequalityFilterField(queryImpl);\r\n        const firstOrderByField = getFirstOrderByField(queryImpl);\r\n        if (inequalityField !== null && firstOrderByField === null) {\r\n            // In order to implicitly add key ordering, we must also add the\r\n            // inequality filter field for it to be a valid query.\r\n            // Note that the default inequality field and key ordering is ascending.\r\n            if (!inequalityField.isKeyField()) {\r\n                queryImpl.memoizedOrderBy.push(new OrderBy(inequalityField));\r\n            }\r\n            queryImpl.memoizedOrderBy.push(new OrderBy(FieldPath$1.keyField(), \"asc\" /* Direction.ASCENDING */));\r\n        }\r\n        else {\r\n            let foundKeyOrdering = false;\r\n            for (const orderBy of queryImpl.explicitOrderBy) {\r\n                queryImpl.memoizedOrderBy.push(orderBy);\r\n                if (orderBy.field.isKeyField()) {\r\n                    foundKeyOrdering = true;\r\n                }\r\n            }\r\n            if (!foundKeyOrdering) {\r\n                // The order of the implicit key ordering always matches the last\r\n                // explicit order by\r\n                const lastDirection = queryImpl.explicitOrderBy.length > 0\r\n                    ? queryImpl.explicitOrderBy[queryImpl.explicitOrderBy.length - 1]\r\n                        .dir\r\n                    : \"asc\" /* Direction.ASCENDING */;\r\n                queryImpl.memoizedOrderBy.push(new OrderBy(FieldPath$1.keyField(), lastDirection));\r\n            }\r\n        }\r\n    }\r\n    return queryImpl.memoizedOrderBy;\r\n}\r\n/**\r\n * Converts this `Query` instance to it's corresponding `Target` representation.\r\n */\r\nfunction queryToTarget(query) {\r\n    const queryImpl = debugCast(query);\r\n    if (!queryImpl.memoizedTarget) {\r\n        if (queryImpl.limitType === \"F\" /* LimitType.First */) {\r\n            queryImpl.memoizedTarget = newTarget(queryImpl.path, queryImpl.collectionGroup, queryOrderBy(queryImpl), queryImpl.filters, queryImpl.limit, queryImpl.startAt, queryImpl.endAt);\r\n        }\r\n        else {\r\n            // Flip the orderBy directions since we want the last results\r\n            const orderBys = [];\r\n            for (const orderBy of queryOrderBy(queryImpl)) {\r\n                const dir = orderBy.dir === \"desc\" /* Direction.DESCENDING */\r\n                    ? \"asc\" /* Direction.ASCENDING */\r\n                    : \"desc\" /* Direction.DESCENDING */;\r\n                orderBys.push(new OrderBy(orderBy.field, dir));\r\n            }\r\n            // We need to swap the cursors to match the now-flipped query ordering.\r\n            const startAt = queryImpl.endAt\r\n                ? new Bound(queryImpl.endAt.position, queryImpl.endAt.inclusive)\r\n                : null;\r\n            const endAt = queryImpl.startAt\r\n                ? new Bound(queryImpl.startAt.position, queryImpl.startAt.inclusive)\r\n                : null;\r\n            // Now return as a LimitType.First query.\r\n            queryImpl.memoizedTarget = newTarget(queryImpl.path, queryImpl.collectionGroup, orderBys, queryImpl.filters, queryImpl.limit, startAt, endAt);\r\n        }\r\n    }\r\n    return queryImpl.memoizedTarget;\r\n}\r\nfunction queryWithAddedFilter(query, filter) {\r\n    filter.getFirstInequalityField();\r\n    getInequalityFilterField(query);\r\n    const newFilters = query.filters.concat([filter]);\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), newFilters, query.limit, query.limitType, query.startAt, query.endAt);\r\n}\r\nfunction queryWithAddedOrderBy(query, orderBy) {\r\n    // TODO(dimond): validate that orderBy does not list the same key twice.\r\n    const newOrderBy = query.explicitOrderBy.concat([orderBy]);\r\n    return new QueryImpl(query.path, query.collectionGroup, newOrderBy, query.filters.slice(), query.limit, query.limitType, query.startAt, query.endAt);\r\n}\r\nfunction queryWithLimit(query, limit, limitType) {\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), limit, limitType, query.startAt, query.endAt);\r\n}\r\nfunction queryWithStartAt(query, bound) {\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, bound, query.endAt);\r\n}\r\nfunction queryWithEndAt(query, bound) {\r\n    return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, query.startAt, bound);\r\n}\r\nfunction queryEquals(left, right) {\r\n    return (targetEquals(queryToTarget(left), queryToTarget(right)) &&\r\n        left.limitType === right.limitType);\r\n}\r\n// TODO(b/29183165): This is used to get a unique string from a query to, for\r\n// example, use as a dictionary key, but the implementation is subject to\r\n// collisions. Make it collision-free.\r\nfunction canonifyQuery(query) {\r\n    return `${canonifyTarget(queryToTarget(query))}|lt:${query.limitType}`;\r\n}\r\nfunction stringifyQuery(query) {\r\n    return `Query(target=${stringifyTarget(queryToTarget(query))}; limitType=${query.limitType})`;\r\n}\r\n/** Returns whether `doc` matches the constraints of `query`. */\r\nfunction queryMatches(query, doc) {\r\n    return (doc.isFoundDocument() &&\r\n        queryMatchesPathAndCollectionGroup(query, doc) &&\r\n        queryMatchesOrderBy(query, doc) &&\r\n        queryMatchesFilters(query, doc) &&\r\n        queryMatchesBounds(query, doc));\r\n}\r\nfunction queryMatchesPathAndCollectionGroup(query, doc) {\r\n    const docPath = doc.key.path;\r\n    if (query.collectionGroup !== null) {\r\n        // NOTE: this.path is currently always empty since we don't expose Collection\r\n        // Group queries rooted at a document path yet.\r\n        return (doc.key.hasCollectionId(query.collectionGroup) &&\r\n            query.path.isPrefixOf(docPath));\r\n    }\r\n    else if (DocumentKey.isDocumentKey(query.path)) {\r\n        // exact match for document queries\r\n        return query.path.isEqual(docPath);\r\n    }\r\n    else {\r\n        // shallow ancestor queries by default\r\n        return query.path.isImmediateParentOf(docPath);\r\n    }\r\n}\r\n/**\r\n * A document must have a value for every ordering clause in order to show up\r\n * in the results.\r\n */\r\nfunction queryMatchesOrderBy(query, doc) {\r\n    // We must use `queryOrderBy()` to get the list of all orderBys (both implicit and explicit).\r\n    // Note that for OR queries, orderBy applies to all disjunction terms and implicit orderBys must\r\n    // be taken into account. For example, the query \"a > 1 || b==1\" has an implicit \"orderBy a\" due\r\n    // to the inequality, and is evaluated as \"a > 1 orderBy a || b==1 orderBy a\".\r\n    // A document with content of {b:1} matches the filters, but does not match the orderBy because\r\n    // it's missing the field 'a'.\r\n    for (const orderBy of queryOrderBy(query)) {\r\n        // order by key always matches\r\n        if (!orderBy.field.isKeyField() && doc.data.field(orderBy.field) === null) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\r\nfunction queryMatchesFilters(query, doc) {\r\n    for (const filter of query.filters) {\r\n        if (!filter.matches(doc)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\r\n/** Makes sure a document is within the bounds, if provided. */\r\nfunction queryMatchesBounds(query, doc) {\r\n    if (query.startAt &&\r\n        !boundSortsBeforeDocument(query.startAt, queryOrderBy(query), doc)) {\r\n        return false;\r\n    }\r\n    if (query.endAt &&\r\n        !boundSortsAfterDocument(query.endAt, queryOrderBy(query), doc)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\r\n/**\r\n * Returns the collection group that this query targets.\r\n *\r\n * PORTING NOTE: This is only used in the Web SDK to facilitate multi-tab\r\n * synchronization for query results.\r\n */\r\nfunction queryCollectionGroup(query) {\r\n    return (query.collectionGroup ||\r\n        (query.path.length % 2 === 1\r\n            ? query.path.lastSegment()\r\n            : query.path.get(query.path.length - 2)));\r\n}\r\n/**\r\n * Returns a new comparator function that can be used to compare two documents\r\n * based on the Query's ordering constraint.\r\n */\r\nfunction newQueryComparator(query) {\r\n    return (d1, d2) => {\r\n        let comparedOnKeyField = false;\r\n        for (const orderBy of queryOrderBy(query)) {\r\n            const comp = compareDocs(orderBy, d1, d2);\r\n            if (comp !== 0) {\r\n                return comp;\r\n            }\r\n            comparedOnKeyField = comparedOnKeyField || orderBy.field.isKeyField();\r\n        }\r\n        return 0;\r\n    };\r\n}\r\nfunction compareDocs(orderBy, d1, d2) {\r\n    const comparison = orderBy.field.isKeyField()\r\n        ? DocumentKey.comparator(d1.key, d2.key)\r\n        : compareDocumentsByField(orderBy.field, d1, d2);\r\n    switch (orderBy.dir) {\r\n        case \"asc\" /* Direction.ASCENDING */:\r\n            return comparison;\r\n        case \"desc\" /* Direction.DESCENDING */:\r\n            return -1 * comparison;\r\n        default:\r\n            return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A map implementation that uses objects as keys. Objects must have an\r\n * associated equals function and must be immutable. Entries in the map are\r\n * stored together with the key being produced from the mapKeyFn. This map\r\n * automatically handles collisions of keys.\r\n */\r\nclass ObjectMap {\r\n    constructor(mapKeyFn, equalsFn) {\r\n        this.mapKeyFn = mapKeyFn;\r\n        this.equalsFn = equalsFn;\r\n        /**\r\n         * The inner map for a key/value pair. Due to the possibility of collisions we\r\n         * keep a list of entries that we do a linear search through to find an actual\r\n         * match. Note that collisions should be rare, so we still expect near\r\n         * constant time lookups in practice.\r\n         */\r\n        this.inner = {};\r\n        /** The number of entries stored in the map */\r\n        this.innerSize = 0;\r\n    }\r\n    /** Get a value for this key, or undefined if it does not exist. */\r\n    get(key) {\r\n        const id = this.mapKeyFn(key);\r\n        const matches = this.inner[id];\r\n        if (matches === undefined) {\r\n            return undefined;\r\n        }\r\n        for (const [otherKey, value] of matches) {\r\n            if (this.equalsFn(otherKey, key)) {\r\n                return value;\r\n            }\r\n        }\r\n        return undefined;\r\n    }\r\n    has(key) {\r\n        return this.get(key) !== undefined;\r\n    }\r\n    /** Put this key and value in the map. */\r\n    set(key, value) {\r\n        const id = this.mapKeyFn(key);\r\n        const matches = this.inner[id];\r\n        if (matches === undefined) {\r\n            this.inner[id] = [[key, value]];\r\n            this.innerSize++;\r\n            return;\r\n        }\r\n        for (let i = 0; i < matches.length; i++) {\r\n            if (this.equalsFn(matches[i][0], key)) {\r\n                // This is updating an existing entry and does not increase `innerSize`.\r\n                matches[i] = [key, value];\r\n                return;\r\n            }\r\n        }\r\n        matches.push([key, value]);\r\n        this.innerSize++;\r\n    }\r\n    /**\r\n     * Remove this key from the map. Returns a boolean if anything was deleted.\r\n     */\r\n    delete(key) {\r\n        const id = this.mapKeyFn(key);\r\n        const matches = this.inner[id];\r\n        if (matches === undefined) {\r\n            return false;\r\n        }\r\n        for (let i = 0; i < matches.length; i++) {\r\n            if (this.equalsFn(matches[i][0], key)) {\r\n                if (matches.length === 1) {\r\n                    delete this.inner[id];\r\n                }\r\n                else {\r\n                    matches.splice(i, 1);\r\n                }\r\n                this.innerSize--;\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n    forEach(fn) {\r\n        forEach(this.inner, (_, entries) => {\r\n            for (const [k, v] of entries) {\r\n                fn(k, v);\r\n            }\r\n        });\r\n    }\r\n    isEmpty() {\r\n        return isEmpty(this.inner);\r\n    }\r\n    size() {\r\n        return this.innerSize;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst EMPTY_MUTABLE_DOCUMENT_MAP = new SortedMap(DocumentKey.comparator);\r\nfunction mutableDocumentMap() {\r\n    return EMPTY_MUTABLE_DOCUMENT_MAP;\r\n}\r\nconst EMPTY_DOCUMENT_MAP = new SortedMap(DocumentKey.comparator);\r\nfunction documentMap(...docs) {\r\n    let map = EMPTY_DOCUMENT_MAP;\r\n    for (const doc of docs) {\r\n        map = map.insert(doc.key, doc);\r\n    }\r\n    return map;\r\n}\r\nfunction newOverlayedDocumentMap() {\r\n    return newDocumentKeyMap();\r\n}\r\nfunction convertOverlayedDocumentMapToDocumentMap(collection) {\r\n    let documents = EMPTY_DOCUMENT_MAP;\r\n    collection.forEach((k, v) => (documents = documents.insert(k, v.overlayedDocument)));\r\n    return documents;\r\n}\r\nfunction newOverlayMap() {\r\n    return newDocumentKeyMap();\r\n}\r\nfunction newMutationMap() {\r\n    return newDocumentKeyMap();\r\n}\r\nfunction newDocumentKeyMap() {\r\n    return new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\r\n}\r\nconst EMPTY_DOCUMENT_VERSION_MAP = new SortedMap(DocumentKey.comparator);\r\nfunction documentVersionMap() {\r\n    return EMPTY_DOCUMENT_VERSION_MAP;\r\n}\r\nconst EMPTY_DOCUMENT_KEY_SET = new SortedSet(DocumentKey.comparator);\r\nfunction documentKeySet(...keys) {\r\n    let set = EMPTY_DOCUMENT_KEY_SET;\r\n    for (const key of keys) {\r\n        set = set.add(key);\r\n    }\r\n    return set;\r\n}\r\nconst EMPTY_TARGET_ID_SET = new SortedSet(primitiveComparator);\r\nfunction targetIdSet() {\r\n    return EMPTY_TARGET_ID_SET;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Returns an DoubleValue for `value` that is encoded based the serializer's\r\n * `useProto3Json` setting.\r\n */\r\nfunction toDouble(serializer, value) {\r\n    if (serializer.useProto3Json) {\r\n        if (isNaN(value)) {\r\n            return { doubleValue: 'NaN' };\r\n        }\r\n        else if (value === Infinity) {\r\n            return { doubleValue: 'Infinity' };\r\n        }\r\n        else if (value === -Infinity) {\r\n            return { doubleValue: '-Infinity' };\r\n        }\r\n    }\r\n    return { doubleValue: isNegativeZero(value) ? '-0' : value };\r\n}\r\n/**\r\n * Returns an IntegerValue for `value`.\r\n */\r\nfunction toInteger(value) {\r\n    return { integerValue: '' + value };\r\n}\r\n/**\r\n * Returns a value for a number that's appropriate to put into a proto.\r\n * The return value is an IntegerValue if it can safely represent the value,\r\n * otherwise a DoubleValue is returned.\r\n */\r\nfunction toNumber(serializer, value) {\r\n    return isSafeInteger(value) ? toInteger(value) : toDouble(serializer, value);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Used to represent a field transform on a mutation. */\r\nclass TransformOperation {\r\n    constructor() {\r\n        // Make sure that the structural type of `TransformOperation` is unique.\r\n        // See https://github.com/microsoft/TypeScript/issues/5451\r\n        this._ = undefined;\r\n    }\r\n}\r\n/**\r\n * Computes the local transform result against the provided `previousValue`,\r\n * optionally using the provided localWriteTime.\r\n */\r\nfunction applyTransformOperationToLocalView(transform, previousValue, localWriteTime) {\r\n    if (transform instanceof ServerTimestampTransform) {\r\n        return serverTimestamp$1(localWriteTime, previousValue);\r\n    }\r\n    else if (transform instanceof ArrayUnionTransformOperation) {\r\n        return applyArrayUnionTransformOperation(transform, previousValue);\r\n    }\r\n    else if (transform instanceof ArrayRemoveTransformOperation) {\r\n        return applyArrayRemoveTransformOperation(transform, previousValue);\r\n    }\r\n    else {\r\n        return applyNumericIncrementTransformOperationToLocalView(transform, previousValue);\r\n    }\r\n}\r\n/**\r\n * Computes a final transform result after the transform has been acknowledged\r\n * by the server, potentially using the server-provided transformResult.\r\n */\r\nfunction applyTransformOperationToRemoteDocument(transform, previousValue, transformResult) {\r\n    // The server just sends null as the transform result for array operations,\r\n    // so we have to calculate a result the same as we do for local\r\n    // applications.\r\n    if (transform instanceof ArrayUnionTransformOperation) {\r\n        return applyArrayUnionTransformOperation(transform, previousValue);\r\n    }\r\n    else if (transform instanceof ArrayRemoveTransformOperation) {\r\n        return applyArrayRemoveTransformOperation(transform, previousValue);\r\n    }\r\n    return transformResult;\r\n}\r\n/**\r\n * If this transform operation is not idempotent, returns the base value to\r\n * persist for this transform. If a base value is returned, the transform\r\n * operation is always applied to this base value, even if document has\r\n * already been updated.\r\n *\r\n * Base values provide consistent behavior for non-idempotent transforms and\r\n * allow us to return the same latency-compensated value even if the backend\r\n * has already applied the transform operation. The base value is null for\r\n * idempotent transforms, as they can be re-played even if the backend has\r\n * already applied them.\r\n *\r\n * @returns a base value to store along with the mutation, or null for\r\n * idempotent transforms.\r\n */\r\nfunction computeTransformOperationBaseValue(transform, previousValue) {\r\n    if (transform instanceof NumericIncrementTransformOperation) {\r\n        return isNumber(previousValue) ? previousValue : { integerValue: 0 };\r\n    }\r\n    return null;\r\n}\r\nfunction transformOperationEquals(left, right) {\r\n    if (left instanceof ArrayUnionTransformOperation &&\r\n        right instanceof ArrayUnionTransformOperation) {\r\n        return arrayEquals(left.elements, right.elements, valueEquals);\r\n    }\r\n    else if (left instanceof ArrayRemoveTransformOperation &&\r\n        right instanceof ArrayRemoveTransformOperation) {\r\n        return arrayEquals(left.elements, right.elements, valueEquals);\r\n    }\r\n    else if (left instanceof NumericIncrementTransformOperation &&\r\n        right instanceof NumericIncrementTransformOperation) {\r\n        return valueEquals(left.operand, right.operand);\r\n    }\r\n    return (left instanceof ServerTimestampTransform &&\r\n        right instanceof ServerTimestampTransform);\r\n}\r\n/** Transforms a value into a server-generated timestamp. */\r\nclass ServerTimestampTransform extends TransformOperation {\r\n}\r\n/** Transforms an array value via a union operation. */\r\nclass ArrayUnionTransformOperation extends TransformOperation {\r\n    constructor(elements) {\r\n        super();\r\n        this.elements = elements;\r\n    }\r\n}\r\nfunction applyArrayUnionTransformOperation(transform, previousValue) {\r\n    const values = coercedFieldValuesArray(previousValue);\r\n    for (const toUnion of transform.elements) {\r\n        if (!values.some(element => valueEquals(element, toUnion))) {\r\n            values.push(toUnion);\r\n        }\r\n    }\r\n    return { arrayValue: { values } };\r\n}\r\n/** Transforms an array value via a remove operation. */\r\nclass ArrayRemoveTransformOperation extends TransformOperation {\r\n    constructor(elements) {\r\n        super();\r\n        this.elements = elements;\r\n    }\r\n}\r\nfunction applyArrayRemoveTransformOperation(transform, previousValue) {\r\n    let values = coercedFieldValuesArray(previousValue);\r\n    for (const toRemove of transform.elements) {\r\n        values = values.filter(element => !valueEquals(element, toRemove));\r\n    }\r\n    return { arrayValue: { values } };\r\n}\r\n/**\r\n * Implements the backend semantics for locally computed NUMERIC_ADD (increment)\r\n * transforms. Converts all field values to integers or doubles, but unlike the\r\n * backend does not cap integer values at 2^63. Instead, JavaScript number\r\n * arithmetic is used and precision loss can occur for values greater than 2^53.\r\n */\r\nclass NumericIncrementTransformOperation extends TransformOperation {\r\n    constructor(serializer, operand) {\r\n        super();\r\n        this.serializer = serializer;\r\n        this.operand = operand;\r\n    }\r\n}\r\nfunction applyNumericIncrementTransformOperationToLocalView(transform, previousValue) {\r\n    // PORTING NOTE: Since JavaScript's integer arithmetic is limited to 53 bit\r\n    // precision and resolves overflows by reducing precision, we do not\r\n    // manually cap overflows at 2^63.\r\n    const baseValue = computeTransformOperationBaseValue(transform, previousValue);\r\n    const sum = asNumber(baseValue) + asNumber(transform.operand);\r\n    if (isInteger(baseValue) && isInteger(transform.operand)) {\r\n        return toInteger(sum);\r\n    }\r\n    else {\r\n        return toDouble(transform.serializer, sum);\r\n    }\r\n}\r\nfunction asNumber(value) {\r\n    return normalizeNumber(value.integerValue || value.doubleValue);\r\n}\r\nfunction coercedFieldValuesArray(value) {\r\n    return isArray(value) && value.arrayValue.values\r\n        ? value.arrayValue.values.slice()\r\n        : [];\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** A field path and the TransformOperation to perform upon it. */\r\nclass FieldTransform {\r\n    constructor(field, transform) {\r\n        this.field = field;\r\n        this.transform = transform;\r\n    }\r\n}\r\nfunction fieldTransformEquals(left, right) {\r\n    return (left.field.isEqual(right.field) &&\r\n        transformOperationEquals(left.transform, right.transform));\r\n}\r\nfunction fieldTransformsAreEqual(left, right) {\r\n    if (left === undefined && right === undefined) {\r\n        return true;\r\n    }\r\n    if (left && right) {\r\n        return arrayEquals(left, right, (l, r) => fieldTransformEquals(l, r));\r\n    }\r\n    return false;\r\n}\r\n/** The result of successfully applying a mutation to the backend. */\r\nclass MutationResult {\r\n    constructor(\r\n    /**\r\n     * The version at which the mutation was committed:\r\n     *\r\n     * - For most operations, this is the updateTime in the WriteResult.\r\n     * - For deletes, the commitTime of the WriteResponse (because deletes are\r\n     *   not stored and have no updateTime).\r\n     *\r\n     * Note that these versions can be different: No-op writes will not change\r\n     * the updateTime even though the commitTime advances.\r\n     */\r\n    version, \r\n    /**\r\n     * The resulting fields returned from the backend after a mutation\r\n     * containing field transforms has been committed. Contains one FieldValue\r\n     * for each FieldTransform that was in the mutation.\r\n     *\r\n     * Will be empty if the mutation did not contain any field transforms.\r\n     */\r\n    transformResults) {\r\n        this.version = version;\r\n        this.transformResults = transformResults;\r\n    }\r\n}\r\n/**\r\n * Encodes a precondition for a mutation. This follows the model that the\r\n * backend accepts with the special case of an explicit \"empty\" precondition\r\n * (meaning no precondition).\r\n */\r\nclass Precondition {\r\n    constructor(updateTime, exists) {\r\n        this.updateTime = updateTime;\r\n        this.exists = exists;\r\n    }\r\n    /** Creates a new empty Precondition. */\r\n    static none() {\r\n        return new Precondition();\r\n    }\r\n    /** Creates a new Precondition with an exists flag. */\r\n    static exists(exists) {\r\n        return new Precondition(undefined, exists);\r\n    }\r\n    /** Creates a new Precondition based on a version a document exists at. */\r\n    static updateTime(version) {\r\n        return new Precondition(version);\r\n    }\r\n    /** Returns whether this Precondition is empty. */\r\n    get isNone() {\r\n        return this.updateTime === undefined && this.exists === undefined;\r\n    }\r\n    isEqual(other) {\r\n        return (this.exists === other.exists &&\r\n            (this.updateTime\r\n                ? !!other.updateTime && this.updateTime.isEqual(other.updateTime)\r\n                : !other.updateTime));\r\n    }\r\n}\r\n/** Returns true if the preconditions is valid for the given document. */\r\nfunction preconditionIsValidForDocument(precondition, document) {\r\n    if (precondition.updateTime !== undefined) {\r\n        return (document.isFoundDocument() &&\r\n            document.version.isEqual(precondition.updateTime));\r\n    }\r\n    else if (precondition.exists !== undefined) {\r\n        return precondition.exists === document.isFoundDocument();\r\n    }\r\n    else {\r\n        return true;\r\n    }\r\n}\r\n/**\r\n * A mutation describes a self-contained change to a document. Mutations can\r\n * create, replace, delete, and update subsets of documents.\r\n *\r\n * Mutations not only act on the value of the document but also its version.\r\n *\r\n * For local mutations (mutations that haven't been committed yet), we preserve\r\n * the existing version for Set and Patch mutations. For Delete mutations, we\r\n * reset the version to 0.\r\n *\r\n * Here's the expected transition table.\r\n *\r\n * MUTATION           APPLIED TO            RESULTS IN\r\n *\r\n * SetMutation        Document(v3)          Document(v3)\r\n * SetMutation        NoDocument(v3)        Document(v0)\r\n * SetMutation        InvalidDocument(v0)   Document(v0)\r\n * PatchMutation      Document(v3)          Document(v3)\r\n * PatchMutation      NoDocument(v3)        NoDocument(v3)\r\n * PatchMutation      InvalidDocument(v0)   UnknownDocument(v3)\r\n * DeleteMutation     Document(v3)          NoDocument(v0)\r\n * DeleteMutation     NoDocument(v3)        NoDocument(v0)\r\n * DeleteMutation     InvalidDocument(v0)   NoDocument(v0)\r\n *\r\n * For acknowledged mutations, we use the updateTime of the WriteResponse as\r\n * the resulting version for Set and Patch mutations. As deletes have no\r\n * explicit update time, we use the commitTime of the WriteResponse for\r\n * Delete mutations.\r\n *\r\n * If a mutation is acknowledged by the backend but fails the precondition check\r\n * locally, we transition to an `UnknownDocument` and rely on Watch to send us\r\n * the updated version.\r\n *\r\n * Field transforms are used only with Patch and Set Mutations. We use the\r\n * `updateTransforms` message to store transforms, rather than the `transforms`s\r\n * messages.\r\n *\r\n * ## Subclassing Notes\r\n *\r\n * Every type of mutation needs to implement its own applyToRemoteDocument() and\r\n * applyToLocalView() to implement the actual behavior of applying the mutation\r\n * to some source document (see `setMutationApplyToRemoteDocument()` for an\r\n * example).\r\n */\r\nclass Mutation {\r\n}\r\n/**\r\n * A utility method to calculate a `Mutation` representing the overlay from the\r\n * final state of the document, and a `FieldMask` representing the fields that\r\n * are mutated by the local mutations.\r\n */\r\nfunction calculateOverlayMutation(doc, mask) {\r\n    if (!doc.hasLocalMutations || (mask && mask.fields.length === 0)) {\r\n        return null;\r\n    }\r\n    // mask is null when sets or deletes are applied to the current document.\r\n    if (mask === null) {\r\n        if (doc.isNoDocument()) {\r\n            return new DeleteMutation(doc.key, Precondition.none());\r\n        }\r\n        else {\r\n            return new SetMutation(doc.key, doc.data, Precondition.none());\r\n        }\r\n    }\r\n    else {\r\n        const docValue = doc.data;\r\n        const patchValue = ObjectValue.empty();\r\n        let maskSet = new SortedSet(FieldPath$1.comparator);\r\n        for (let path of mask.fields) {\r\n            if (!maskSet.has(path)) {\r\n                let value = docValue.field(path);\r\n                // If we are deleting a nested field, we take the immediate parent as\r\n                // the mask used to construct the resulting mutation.\r\n                // Justification: Nested fields can create parent fields implicitly. If\r\n                // only a leaf entry is deleted in later mutations, the parent field\r\n                // should still remain, but we may have lost this information.\r\n                // Consider mutation (foo.bar 1), then mutation (foo.bar delete()).\r\n                // This leaves the final result (foo, {}). Despite the fact that `doc`\r\n                // has the correct result, `foo` is not in `mask`, and the resulting\r\n                // mutation would miss `foo`.\r\n                if (value === null && path.length > 1) {\r\n                    path = path.popLast();\r\n                    value = docValue.field(path);\r\n                }\r\n                if (value === null) {\r\n                    patchValue.delete(path);\r\n                }\r\n                else {\r\n                    patchValue.set(path, value);\r\n                }\r\n                maskSet = maskSet.add(path);\r\n            }\r\n        }\r\n        return new PatchMutation(doc.key, patchValue, new FieldMask(maskSet.toArray()), Precondition.none());\r\n    }\r\n}\r\n/**\r\n * Applies this mutation to the given document for the purposes of computing a\r\n * new remote document. If the input document doesn't match the expected state\r\n * (e.g. it is invalid or outdated), the document type may transition to\r\n * unknown.\r\n *\r\n * @param mutation - The mutation to apply.\r\n * @param document - The document to mutate. The input document can be an\r\n *     invalid document if the client has no knowledge of the pre-mutation state\r\n *     of the document.\r\n * @param mutationResult - The result of applying the mutation from the backend.\r\n */\r\nfunction mutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    if (mutation instanceof SetMutation) {\r\n        setMutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n    }\r\n    else if (mutation instanceof PatchMutation) {\r\n        patchMutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n    }\r\n    else {\r\n        deleteMutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n    }\r\n}\r\n/**\r\n * Applies this mutation to the given document for the purposes of computing\r\n * the new local view of a document. If the input document doesn't match the\r\n * expected state, the document is not modified.\r\n *\r\n * @param mutation - The mutation to apply.\r\n * @param document - The document to mutate. The input document can be an\r\n *     invalid document if the client has no knowledge of the pre-mutation state\r\n *     of the document.\r\n * @param previousMask - The fields that have been updated before applying this mutation.\r\n * @param localWriteTime - A timestamp indicating the local write time of the\r\n *     batch this mutation is a part of.\r\n * @returns A `FieldMask` representing the fields that are changed by applying this mutation.\r\n */\r\nfunction mutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\r\n    if (mutation instanceof SetMutation) {\r\n        return setMutationApplyToLocalView(mutation, document, previousMask, localWriteTime);\r\n    }\r\n    else if (mutation instanceof PatchMutation) {\r\n        return patchMutationApplyToLocalView(mutation, document, previousMask, localWriteTime);\r\n    }\r\n    else {\r\n        return deleteMutationApplyToLocalView(mutation, document, previousMask);\r\n    }\r\n}\r\n/**\r\n * If this mutation is not idempotent, returns the base value to persist with\r\n * this mutation. If a base value is returned, the mutation is always applied\r\n * to this base value, even if document has already been updated.\r\n *\r\n * The base value is a sparse object that consists of only the document\r\n * fields for which this mutation contains a non-idempotent transformation\r\n * (e.g. a numeric increment). The provided value guarantees consistent\r\n * behavior for non-idempotent transforms and allow us to return the same\r\n * latency-compensated value even if the backend has already applied the\r\n * mutation. The base value is null for idempotent mutations, as they can be\r\n * re-played even if the backend has already applied them.\r\n *\r\n * @returns a base value to store along with the mutation, or null for\r\n * idempotent mutations.\r\n */\r\nfunction mutationExtractBaseValue(mutation, document) {\r\n    let baseObject = null;\r\n    for (const fieldTransform of mutation.fieldTransforms) {\r\n        const existingValue = document.data.field(fieldTransform.field);\r\n        const coercedValue = computeTransformOperationBaseValue(fieldTransform.transform, existingValue || null);\r\n        if (coercedValue != null) {\r\n            if (baseObject === null) {\r\n                baseObject = ObjectValue.empty();\r\n            }\r\n            baseObject.set(fieldTransform.field, coercedValue);\r\n        }\r\n    }\r\n    return baseObject ? baseObject : null;\r\n}\r\nfunction mutationEquals(left, right) {\r\n    if (left.type !== right.type) {\r\n        return false;\r\n    }\r\n    if (!left.key.isEqual(right.key)) {\r\n        return false;\r\n    }\r\n    if (!left.precondition.isEqual(right.precondition)) {\r\n        return false;\r\n    }\r\n    if (!fieldTransformsAreEqual(left.fieldTransforms, right.fieldTransforms)) {\r\n        return false;\r\n    }\r\n    if (left.type === 0 /* MutationType.Set */) {\r\n        return left.value.isEqual(right.value);\r\n    }\r\n    if (left.type === 1 /* MutationType.Patch */) {\r\n        return (left.data.isEqual(right.data) &&\r\n            left.fieldMask.isEqual(right.fieldMask));\r\n    }\r\n    return true;\r\n}\r\n/**\r\n * A mutation that creates or replaces the document at the given key with the\r\n * object value contents.\r\n */\r\nclass SetMutation extends Mutation {\r\n    constructor(key, value, precondition, fieldTransforms = []) {\r\n        super();\r\n        this.key = key;\r\n        this.value = value;\r\n        this.precondition = precondition;\r\n        this.fieldTransforms = fieldTransforms;\r\n        this.type = 0 /* MutationType.Set */;\r\n    }\r\n    getFieldMask() {\r\n        return null;\r\n    }\r\n}\r\nfunction setMutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    // Unlike setMutationApplyToLocalView, if we're applying a mutation to a\r\n    // remote document the server has accepted the mutation so the precondition\r\n    // must have held.\r\n    const newData = mutation.value.clone();\r\n    const transformResults = serverTransformResults(mutation.fieldTransforms, document, mutationResult.transformResults);\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(mutationResult.version, newData)\r\n        .setHasCommittedMutations();\r\n}\r\nfunction setMutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\r\n    if (!preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        // The mutation failed to apply (e.g. a document ID created with add()\r\n        // caused a name collision).\r\n        return previousMask;\r\n    }\r\n    const newData = mutation.value.clone();\r\n    const transformResults = localTransformResults(mutation.fieldTransforms, localWriteTime, document);\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(document.version, newData)\r\n        .setHasLocalMutations();\r\n    return null; // SetMutation overwrites all fields.\r\n}\r\n/**\r\n * A mutation that modifies fields of the document at the given key with the\r\n * given values. The values are applied through a field mask:\r\n *\r\n *  * When a field is in both the mask and the values, the corresponding field\r\n *    is updated.\r\n *  * When a field is in neither the mask nor the values, the corresponding\r\n *    field is unmodified.\r\n *  * When a field is in the mask but not in the values, the corresponding field\r\n *    is deleted.\r\n *  * When a field is not in the mask but is in the values, the values map is\r\n *    ignored.\r\n */\r\nclass PatchMutation extends Mutation {\r\n    constructor(key, data, fieldMask, precondition, fieldTransforms = []) {\r\n        super();\r\n        this.key = key;\r\n        this.data = data;\r\n        this.fieldMask = fieldMask;\r\n        this.precondition = precondition;\r\n        this.fieldTransforms = fieldTransforms;\r\n        this.type = 1 /* MutationType.Patch */;\r\n    }\r\n    getFieldMask() {\r\n        return this.fieldMask;\r\n    }\r\n}\r\nfunction patchMutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    if (!preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        // Since the mutation was not rejected, we know that the precondition\r\n        // matched on the backend. We therefore must not have the expected version\r\n        // of the document in our cache and convert to an UnknownDocument with a\r\n        // known updateTime.\r\n        document.convertToUnknownDocument(mutationResult.version);\r\n        return;\r\n    }\r\n    const transformResults = serverTransformResults(mutation.fieldTransforms, document, mutationResult.transformResults);\r\n    const newData = document.data;\r\n    newData.setAll(getPatch(mutation));\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(mutationResult.version, newData)\r\n        .setHasCommittedMutations();\r\n}\r\nfunction patchMutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\r\n    if (!preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        return previousMask;\r\n    }\r\n    const transformResults = localTransformResults(mutation.fieldTransforms, localWriteTime, document);\r\n    const newData = document.data;\r\n    newData.setAll(getPatch(mutation));\r\n    newData.setAll(transformResults);\r\n    document\r\n        .convertToFoundDocument(document.version, newData)\r\n        .setHasLocalMutations();\r\n    if (previousMask === null) {\r\n        return null;\r\n    }\r\n    return previousMask\r\n        .unionWith(mutation.fieldMask.fields)\r\n        .unionWith(mutation.fieldTransforms.map(transform => transform.field));\r\n}\r\n/**\r\n * Returns a FieldPath/Value map with the content of the PatchMutation.\r\n */\r\nfunction getPatch(mutation) {\r\n    const result = new Map();\r\n    mutation.fieldMask.fields.forEach(fieldPath => {\r\n        if (!fieldPath.isEmpty()) {\r\n            const newValue = mutation.data.field(fieldPath);\r\n            result.set(fieldPath, newValue);\r\n        }\r\n    });\r\n    return result;\r\n}\r\n/**\r\n * Creates a list of \"transform results\" (a transform result is a field value\r\n * representing the result of applying a transform) for use after a mutation\r\n * containing transforms has been acknowledged by the server.\r\n *\r\n * @param fieldTransforms - The field transforms to apply the result to.\r\n * @param mutableDocument - The current state of the document after applying all\r\n * previous mutations.\r\n * @param serverTransformResults - The transform results received by the server.\r\n * @returns The transform results list.\r\n */\r\nfunction serverTransformResults(fieldTransforms, mutableDocument, serverTransformResults) {\r\n    const transformResults = new Map();\r\n    hardAssert(fieldTransforms.length === serverTransformResults.length);\r\n    for (let i = 0; i < serverTransformResults.length; i++) {\r\n        const fieldTransform = fieldTransforms[i];\r\n        const transform = fieldTransform.transform;\r\n        const previousValue = mutableDocument.data.field(fieldTransform.field);\r\n        transformResults.set(fieldTransform.field, applyTransformOperationToRemoteDocument(transform, previousValue, serverTransformResults[i]));\r\n    }\r\n    return transformResults;\r\n}\r\n/**\r\n * Creates a list of \"transform results\" (a transform result is a field value\r\n * representing the result of applying a transform) for use when applying a\r\n * transform locally.\r\n *\r\n * @param fieldTransforms - The field transforms to apply the result to.\r\n * @param localWriteTime - The local time of the mutation (used to\r\n *     generate ServerTimestampValues).\r\n * @param mutableDocument - The document to apply transforms on.\r\n * @returns The transform results list.\r\n */\r\nfunction localTransformResults(fieldTransforms, localWriteTime, mutableDocument) {\r\n    const transformResults = new Map();\r\n    for (const fieldTransform of fieldTransforms) {\r\n        const transform = fieldTransform.transform;\r\n        const previousValue = mutableDocument.data.field(fieldTransform.field);\r\n        transformResults.set(fieldTransform.field, applyTransformOperationToLocalView(transform, previousValue, localWriteTime));\r\n    }\r\n    return transformResults;\r\n}\r\n/** A mutation that deletes the document at the given key. */\r\nclass DeleteMutation extends Mutation {\r\n    constructor(key, precondition) {\r\n        super();\r\n        this.key = key;\r\n        this.precondition = precondition;\r\n        this.type = 2 /* MutationType.Delete */;\r\n        this.fieldTransforms = [];\r\n    }\r\n    getFieldMask() {\r\n        return null;\r\n    }\r\n}\r\nfunction deleteMutationApplyToRemoteDocument(mutation, document, mutationResult) {\r\n    // Unlike applyToLocalView, if we're applying a mutation to a remote\r\n    // document the server has accepted the mutation so the precondition must\r\n    // have held.\r\n    document\r\n        .convertToNoDocument(mutationResult.version)\r\n        .setHasCommittedMutations();\r\n}\r\nfunction deleteMutationApplyToLocalView(mutation, document, previousMask) {\r\n    if (preconditionIsValidForDocument(mutation.precondition, document)) {\r\n        document.convertToNoDocument(document.version).setHasLocalMutations();\r\n        return null;\r\n    }\r\n    return previousMask;\r\n}\r\n/**\r\n * A mutation that verifies the existence of the document at the given key with\r\n * the provided precondition.\r\n *\r\n * The `verify` operation is only used in Transactions, and this class serves\r\n * primarily to facilitate serialization into protos.\r\n */\r\nclass VerifyMutation extends Mutation {\r\n    constructor(key, precondition) {\r\n        super();\r\n        this.key = key;\r\n        this.precondition = precondition;\r\n        this.type = 3 /* MutationType.Verify */;\r\n        this.fieldTransforms = [];\r\n    }\r\n    getFieldMask() {\r\n        return null;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A batch of mutations that will be sent as one unit to the backend.\r\n */\r\nclass MutationBatch {\r\n    /**\r\n     * @param batchId - The unique ID of this mutation batch.\r\n     * @param localWriteTime - The original write time of this mutation.\r\n     * @param baseMutations - Mutations that are used to populate the base\r\n     * values when this mutation is applied locally. This can be used to locally\r\n     * overwrite values that are persisted in the remote document cache. Base\r\n     * mutations are never sent to the backend.\r\n     * @param mutations - The user-provided mutations in this mutation batch.\r\n     * User-provided mutations are applied both locally and remotely on the\r\n     * backend.\r\n     */\r\n    constructor(batchId, localWriteTime, baseMutations, mutations) {\r\n        this.batchId = batchId;\r\n        this.localWriteTime = localWriteTime;\r\n        this.baseMutations = baseMutations;\r\n        this.mutations = mutations;\r\n    }\r\n    /**\r\n     * Applies all the mutations in this MutationBatch to the specified document\r\n     * to compute the state of the remote document\r\n     *\r\n     * @param document - The document to apply mutations to.\r\n     * @param batchResult - The result of applying the MutationBatch to the\r\n     * backend.\r\n     */\r\n    applyToRemoteDocument(document, batchResult) {\r\n        const mutationResults = batchResult.mutationResults;\r\n        for (let i = 0; i < this.mutations.length; i++) {\r\n            const mutation = this.mutations[i];\r\n            if (mutation.key.isEqual(document.key)) {\r\n                const mutationResult = mutationResults[i];\r\n                mutationApplyToRemoteDocument(mutation, document, mutationResult);\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Computes the local view of a document given all the mutations in this\r\n     * batch.\r\n     *\r\n     * @param document - The document to apply mutations to.\r\n     * @param mutatedFields - Fields that have been updated before applying this mutation batch.\r\n     * @returns A `FieldMask` representing all the fields that are mutated.\r\n     */\r\n    applyToLocalView(document, mutatedFields) {\r\n        // First, apply the base state. This allows us to apply non-idempotent\r\n        // transform against a consistent set of values.\r\n        for (const mutation of this.baseMutations) {\r\n            if (mutation.key.isEqual(document.key)) {\r\n                mutatedFields = mutationApplyToLocalView(mutation, document, mutatedFields, this.localWriteTime);\r\n            }\r\n        }\r\n        // Second, apply all user-provided mutations.\r\n        for (const mutation of this.mutations) {\r\n            if (mutation.key.isEqual(document.key)) {\r\n                mutatedFields = mutationApplyToLocalView(mutation, document, mutatedFields, this.localWriteTime);\r\n            }\r\n        }\r\n        return mutatedFields;\r\n    }\r\n    /**\r\n     * Computes the local view for all provided documents given the mutations in\r\n     * this batch. Returns a `DocumentKey` to `Mutation` map which can be used to\r\n     * replace all the mutation applications.\r\n     */\r\n    applyToLocalDocumentSet(documentMap, documentsWithoutRemoteVersion) {\r\n        // TODO(mrschmidt): This implementation is O(n^2). If we apply the mutations\r\n        // directly (as done in `applyToLocalView()`), we can reduce the complexity\r\n        // to O(n).\r\n        const overlays = newMutationMap();\r\n        this.mutations.forEach(m => {\r\n            const overlayedDocument = documentMap.get(m.key);\r\n            // TODO(mutabledocuments): This method should take a MutableDocumentMap\r\n            // and we should remove this cast.\r\n            const mutableDocument = overlayedDocument.overlayedDocument;\r\n            let mutatedFields = this.applyToLocalView(mutableDocument, overlayedDocument.mutatedFields);\r\n            // Set mutatedFields to null if the document is only from local mutations.\r\n            // This creates a Set or Delete mutation, instead of trying to create a\r\n            // patch mutation as the overlay.\r\n            mutatedFields = documentsWithoutRemoteVersion.has(m.key)\r\n                ? null\r\n                : mutatedFields;\r\n            const overlay = calculateOverlayMutation(mutableDocument, mutatedFields);\r\n            if (overlay !== null) {\r\n                overlays.set(m.key, overlay);\r\n            }\r\n            if (!mutableDocument.isValidDocument()) {\r\n                mutableDocument.convertToNoDocument(SnapshotVersion.min());\r\n            }\r\n        });\r\n        return overlays;\r\n    }\r\n    keys() {\r\n        return this.mutations.reduce((keys, m) => keys.add(m.key), documentKeySet());\r\n    }\r\n    isEqual(other) {\r\n        return (this.batchId === other.batchId &&\r\n            arrayEquals(this.mutations, other.mutations, (l, r) => mutationEquals(l, r)) &&\r\n            arrayEquals(this.baseMutations, other.baseMutations, (l, r) => mutationEquals(l, r)));\r\n    }\r\n}\r\n/** The result of applying a mutation batch to the backend. */\r\nclass MutationBatchResult {\r\n    constructor(batch, commitVersion, mutationResults, \r\n    /**\r\n     * A pre-computed mapping from each mutated document to the resulting\r\n     * version.\r\n     */\r\n    docVersions) {\r\n        this.batch = batch;\r\n        this.commitVersion = commitVersion;\r\n        this.mutationResults = mutationResults;\r\n        this.docVersions = docVersions;\r\n    }\r\n    /**\r\n     * Creates a new MutationBatchResult for the given batch and results. There\r\n     * must be one result for each mutation in the batch. This static factory\r\n     * caches a document=&gt;version mapping (docVersions).\r\n     */\r\n    static from(batch, commitVersion, results) {\r\n        hardAssert(batch.mutations.length === results.length);\r\n        let versionMap = documentVersionMap();\r\n        const mutations = batch.mutations;\r\n        for (let i = 0; i < mutations.length; i++) {\r\n            versionMap = versionMap.insert(mutations[i].key, results[i].version);\r\n        }\r\n        return new MutationBatchResult(batch, commitVersion, results, versionMap);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Representation of an overlay computed by Firestore.\r\n *\r\n * Holds information about a mutation and the largest batch id in Firestore when\r\n * the mutation was created.\r\n */\r\nclass Overlay {\r\n    constructor(largestBatchId, mutation) {\r\n        this.largestBatchId = largestBatchId;\r\n        this.mutation = mutation;\r\n    }\r\n    getKey() {\r\n        return this.mutation.key;\r\n    }\r\n    isEqual(other) {\r\n        return other !== null && this.mutation === other.mutation;\r\n    }\r\n    toString() {\r\n        return `Overlay{\n      largestBatchId: ${this.largestBatchId},\n      mutation: ${this.mutation.toString()}\n    }`;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass ExistenceFilter {\r\n    constructor(count, unchangedNames) {\r\n        this.count = count;\r\n        this.unchangedNames = unchangedNames;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Error Codes describing the different ways GRPC can fail. These are copied\r\n * directly from GRPC's sources here:\r\n *\r\n * https://github.com/grpc/grpc/blob/bceec94ea4fc5f0085d81235d8e1c06798dc341a/include/grpc%2B%2B/impl/codegen/status_code_enum.h\r\n *\r\n * Important! The names of these identifiers matter because the string forms\r\n * are used for reverse lookups from the webchannel stream. Do NOT change the\r\n * names of these identifiers or change this into a const enum.\r\n */\r\nvar RpcCode;\r\n(function (RpcCode) {\r\n    RpcCode[RpcCode[\"OK\"] = 0] = \"OK\";\r\n    RpcCode[RpcCode[\"CANCELLED\"] = 1] = \"CANCELLED\";\r\n    RpcCode[RpcCode[\"UNKNOWN\"] = 2] = \"UNKNOWN\";\r\n    RpcCode[RpcCode[\"INVALID_ARGUMENT\"] = 3] = \"INVALID_ARGUMENT\";\r\n    RpcCode[RpcCode[\"DEADLINE_EXCEEDED\"] = 4] = \"DEADLINE_EXCEEDED\";\r\n    RpcCode[RpcCode[\"NOT_FOUND\"] = 5] = \"NOT_FOUND\";\r\n    RpcCode[RpcCode[\"ALREADY_EXISTS\"] = 6] = \"ALREADY_EXISTS\";\r\n    RpcCode[RpcCode[\"PERMISSION_DENIED\"] = 7] = \"PERMISSION_DENIED\";\r\n    RpcCode[RpcCode[\"UNAUTHENTICATED\"] = 16] = \"UNAUTHENTICATED\";\r\n    RpcCode[RpcCode[\"RESOURCE_EXHAUSTED\"] = 8] = \"RESOURCE_EXHAUSTED\";\r\n    RpcCode[RpcCode[\"FAILED_PRECONDITION\"] = 9] = \"FAILED_PRECONDITION\";\r\n    RpcCode[RpcCode[\"ABORTED\"] = 10] = \"ABORTED\";\r\n    RpcCode[RpcCode[\"OUT_OF_RANGE\"] = 11] = \"OUT_OF_RANGE\";\r\n    RpcCode[RpcCode[\"UNIMPLEMENTED\"] = 12] = \"UNIMPLEMENTED\";\r\n    RpcCode[RpcCode[\"INTERNAL\"] = 13] = \"INTERNAL\";\r\n    RpcCode[RpcCode[\"UNAVAILABLE\"] = 14] = \"UNAVAILABLE\";\r\n    RpcCode[RpcCode[\"DATA_LOSS\"] = 15] = \"DATA_LOSS\";\r\n})(RpcCode || (RpcCode = {}));\r\n/**\r\n * Determines whether an error code represents a permanent error when received\r\n * in response to a non-write operation.\r\n *\r\n * See isPermanentWriteError for classifying write errors.\r\n */\r\nfunction isPermanentError(code) {\r\n    switch (code) {\r\n        case Code.OK:\r\n            return fail();\r\n        case Code.CANCELLED:\r\n        case Code.UNKNOWN:\r\n        case Code.DEADLINE_EXCEEDED:\r\n        case Code.RESOURCE_EXHAUSTED:\r\n        case Code.INTERNAL:\r\n        case Code.UNAVAILABLE:\r\n        // Unauthenticated means something went wrong with our token and we need\r\n        // to retry with new credentials which will happen automatically.\r\n        case Code.UNAUTHENTICATED:\r\n            return false;\r\n        case Code.INVALID_ARGUMENT:\r\n        case Code.NOT_FOUND:\r\n        case Code.ALREADY_EXISTS:\r\n        case Code.PERMISSION_DENIED:\r\n        case Code.FAILED_PRECONDITION:\r\n        // Aborted might be retried in some scenarios, but that is dependant on\r\n        // the context and should handled individually by the calling code.\r\n        // See https://cloud.google.com/apis/design/errors.\r\n        case Code.ABORTED:\r\n        case Code.OUT_OF_RANGE:\r\n        case Code.UNIMPLEMENTED:\r\n        case Code.DATA_LOSS:\r\n            return true;\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\n/**\r\n * Determines whether an error code represents a permanent error when received\r\n * in response to a write operation.\r\n *\r\n * Write operations must be handled specially because as of b/119437764, ABORTED\r\n * errors on the write stream should be retried too (even though ABORTED errors\r\n * are not generally retryable).\r\n *\r\n * Note that during the initial handshake on the write stream an ABORTED error\r\n * signals that we should discard our stream token (i.e. it is permanent). This\r\n * means a handshake error should be classified with isPermanentError, above.\r\n */\r\nfunction isPermanentWriteError(code) {\r\n    return isPermanentError(code) && code !== Code.ABORTED;\r\n}\r\n/**\r\n * Maps an error Code from GRPC status code number, like 0, 1, or 14. These\r\n * are not the same as HTTP status codes.\r\n *\r\n * @returns The Code equivalent to the given GRPC status code. Fails if there\r\n *     is no match.\r\n */\r\nfunction mapCodeFromRpcCode(code) {\r\n    if (code === undefined) {\r\n        // This shouldn't normally happen, but in certain error cases (like trying\r\n        // to send invalid proto messages) we may get an error with no GRPC code.\r\n        logError('GRPC error has no .code');\r\n        return Code.UNKNOWN;\r\n    }\r\n    switch (code) {\r\n        case RpcCode.OK:\r\n            return Code.OK;\r\n        case RpcCode.CANCELLED:\r\n            return Code.CANCELLED;\r\n        case RpcCode.UNKNOWN:\r\n            return Code.UNKNOWN;\r\n        case RpcCode.DEADLINE_EXCEEDED:\r\n            return Code.DEADLINE_EXCEEDED;\r\n        case RpcCode.RESOURCE_EXHAUSTED:\r\n            return Code.RESOURCE_EXHAUSTED;\r\n        case RpcCode.INTERNAL:\r\n            return Code.INTERNAL;\r\n        case RpcCode.UNAVAILABLE:\r\n            return Code.UNAVAILABLE;\r\n        case RpcCode.UNAUTHENTICATED:\r\n            return Code.UNAUTHENTICATED;\r\n        case RpcCode.INVALID_ARGUMENT:\r\n            return Code.INVALID_ARGUMENT;\r\n        case RpcCode.NOT_FOUND:\r\n            return Code.NOT_FOUND;\r\n        case RpcCode.ALREADY_EXISTS:\r\n            return Code.ALREADY_EXISTS;\r\n        case RpcCode.PERMISSION_DENIED:\r\n            return Code.PERMISSION_DENIED;\r\n        case RpcCode.FAILED_PRECONDITION:\r\n            return Code.FAILED_PRECONDITION;\r\n        case RpcCode.ABORTED:\r\n            return Code.ABORTED;\r\n        case RpcCode.OUT_OF_RANGE:\r\n            return Code.OUT_OF_RANGE;\r\n        case RpcCode.UNIMPLEMENTED:\r\n            return Code.UNIMPLEMENTED;\r\n        case RpcCode.DATA_LOSS:\r\n            return Code.DATA_LOSS;\r\n        default:\r\n            return fail();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An error encountered while decoding base64 string.\r\n */\r\nclass Base64DecodeError extends Error {\r\n    constructor() {\r\n        super(...arguments);\r\n        this.name = 'Base64DecodeError';\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Manages \"testing hooks\", hooks into the internals of the SDK to verify\r\n * internal state and events during integration tests. Do not use this class\r\n * except for testing purposes.\r\n *\r\n * There are two ways to retrieve the global singleton instance of this class:\r\n * 1. The `instance` property, which returns null if the global singleton\r\n *      instance has not been created. Use this property if the caller should\r\n *      \"do nothing\" if there are no testing hooks registered, such as when\r\n *      delivering an event to notify registered callbacks.\r\n * 2. The `getOrCreateInstance()` method, which creates the global singleton\r\n *      instance if it has not been created. Use this method if the instance is\r\n *      needed to, for example, register a callback.\r\n *\r\n * @internal\r\n */\r\nclass TestingHooks {\r\n    constructor() {\r\n        this.onExistenceFilterMismatchCallbacks = new Map();\r\n    }\r\n    /**\r\n     * Returns the singleton instance of this class, or null if it has not been\r\n     * initialized.\r\n     */\r\n    static get instance() {\r\n        return gTestingHooksSingletonInstance;\r\n    }\r\n    /**\r\n     * Returns the singleton instance of this class, creating it if is has never\r\n     * been created before.\r\n     */\r\n    static getOrCreateInstance() {\r\n        if (gTestingHooksSingletonInstance === null) {\r\n            gTestingHooksSingletonInstance = new TestingHooks();\r\n        }\r\n        return gTestingHooksSingletonInstance;\r\n    }\r\n    /**\r\n     * Registers a callback to be notified when an existence filter mismatch\r\n     * occurs in the Watch listen stream.\r\n     *\r\n     * The relative order in which callbacks are notified is unspecified; do not\r\n     * rely on any particular ordering. If a given callback is registered multiple\r\n     * times then it will be notified multiple times, once per registration.\r\n     *\r\n     * @param callback the callback to invoke upon existence filter mismatch.\r\n     *\r\n     * @return a function that, when called, unregisters the given callback; only\r\n     * the first invocation of the returned function does anything; all subsequent\r\n     * invocations do nothing.\r\n     */\r\n    onExistenceFilterMismatch(callback) {\r\n        const key = Symbol();\r\n        this.onExistenceFilterMismatchCallbacks.set(key, callback);\r\n        return () => this.onExistenceFilterMismatchCallbacks.delete(key);\r\n    }\r\n    /**\r\n     * Invokes all currently-registered `onExistenceFilterMismatch` callbacks.\r\n     * @param info Information about the existence filter mismatch.\r\n     */\r\n    notifyOnExistenceFilterMismatch(info) {\r\n        this.onExistenceFilterMismatchCallbacks.forEach(callback => callback(info));\r\n    }\r\n}\r\n/** The global singleton instance of `TestingHooks`. */\r\nlet gTestingHooksSingletonInstance = null;\n\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An instance of the Platform's 'TextEncoder' implementation.\r\n */\r\nfunction newTextEncoder() {\r\n    return new TextEncoder();\r\n}\r\n/**\r\n * An instance of the Platform's 'TextDecoder' implementation.\r\n */\r\nfunction newTextDecoder() {\r\n    return new TextDecoder('utf-8');\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst MAX_64_BIT_UNSIGNED_INTEGER = new Integer([0xffffffff, 0xffffffff], 0);\r\n// Hash a string using md5 hashing algorithm.\r\nfunction getMd5HashValue(value) {\r\n    const encodedValue = newTextEncoder().encode(value);\r\n    const md5 = new Md5();\r\n    md5.update(encodedValue);\r\n    return new Uint8Array(md5.digest());\r\n}\r\n// Interpret the 16 bytes array as two 64-bit unsigned integers, encoded using\r\n// 2’s complement using little endian.\r\nfunction get64BitUints(Bytes) {\r\n    const dataView = new DataView(Bytes.buffer);\r\n    const chunk1 = dataView.getUint32(0, /* littleEndian= */ true);\r\n    const chunk2 = dataView.getUint32(4, /* littleEndian= */ true);\r\n    const chunk3 = dataView.getUint32(8, /* littleEndian= */ true);\r\n    const chunk4 = dataView.getUint32(12, /* littleEndian= */ true);\r\n    const integer1 = new Integer([chunk1, chunk2], 0);\r\n    const integer2 = new Integer([chunk3, chunk4], 0);\r\n    return [integer1, integer2];\r\n}\r\nclass BloomFilter {\r\n    constructor(bitmap, padding, hashCount) {\r\n        this.bitmap = bitmap;\r\n        this.padding = padding;\r\n        this.hashCount = hashCount;\r\n        if (padding < 0 || padding >= 8) {\r\n            throw new BloomFilterError(`Invalid padding: ${padding}`);\r\n        }\r\n        if (hashCount < 0) {\r\n            throw new BloomFilterError(`Invalid hash count: ${hashCount}`);\r\n        }\r\n        if (bitmap.length > 0 && this.hashCount === 0) {\r\n            // Only empty bloom filter can have 0 hash count.\r\n            throw new BloomFilterError(`Invalid hash count: ${hashCount}`);\r\n        }\r\n        if (bitmap.length === 0 && padding !== 0) {\r\n            // Empty bloom filter should have 0 padding.\r\n            throw new BloomFilterError(`Invalid padding when bitmap length is 0: ${padding}`);\r\n        }\r\n        this.bitCount = bitmap.length * 8 - padding;\r\n        // Set the bit count in Integer to avoid repetition in mightContain().\r\n        this.bitCountInInteger = Integer.fromNumber(this.bitCount);\r\n    }\r\n    // Calculate the ith hash value based on the hashed 64bit integers,\r\n    // and calculate its corresponding bit index in the bitmap to be checked.\r\n    getBitIndex(num1, num2, hashIndex) {\r\n        // Calculate hashed value h(i) = h1 + (i * h2).\r\n        let hashValue = num1.add(num2.multiply(Integer.fromNumber(hashIndex)));\r\n        // Wrap if hash value overflow 64bit.\r\n        if (hashValue.compare(MAX_64_BIT_UNSIGNED_INTEGER) === 1) {\r\n            hashValue = new Integer([hashValue.getBits(0), hashValue.getBits(1)], 0);\r\n        }\r\n        return hashValue.modulo(this.bitCountInInteger).toNumber();\r\n    }\r\n    // Return whether the bit on the given index in the bitmap is set to 1.\r\n    isBitSet(index) {\r\n        // To retrieve bit n, calculate: (bitmap[n / 8] & (0x01 << (n % 8))).\r\n        const byte = this.bitmap[Math.floor(index / 8)];\r\n        const offset = index % 8;\r\n        return (byte & (0x01 << offset)) !== 0;\r\n    }\r\n    mightContain(value) {\r\n        // Empty bitmap should always return false on membership check.\r\n        if (this.bitCount === 0) {\r\n            return false;\r\n        }\r\n        const md5HashedValue = getMd5HashValue(value);\r\n        const [hash1, hash2] = get64BitUints(md5HashedValue);\r\n        for (let i = 0; i < this.hashCount; i++) {\r\n            const index = this.getBitIndex(hash1, hash2, i);\r\n            if (!this.isBitSet(index)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    /** Create bloom filter for testing purposes only. */\r\n    static create(bitCount, hashCount, contains) {\r\n        const padding = bitCount % 8 === 0 ? 0 : 8 - (bitCount % 8);\r\n        const bitmap = new Uint8Array(Math.ceil(bitCount / 8));\r\n        const bloomFilter = new BloomFilter(bitmap, padding, hashCount);\r\n        contains.forEach(item => bloomFilter.insert(item));\r\n        return bloomFilter;\r\n    }\r\n    insert(value) {\r\n        if (this.bitCount === 0) {\r\n            return;\r\n        }\r\n        const md5HashedValue = getMd5HashValue(value);\r\n        const [hash1, hash2] = get64BitUints(md5HashedValue);\r\n        for (let i = 0; i < this.hashCount; i++) {\r\n            const index = this.getBitIndex(hash1, hash2, i);\r\n            this.setBit(index);\r\n        }\r\n    }\r\n    setBit(index) {\r\n        const indexOfByte = Math.floor(index / 8);\r\n        const offset = index % 8;\r\n        this.bitmap[indexOfByte] |= 0x01 << offset;\r\n    }\r\n}\r\nclass BloomFilterError extends Error {\r\n    constructor() {\r\n        super(...arguments);\r\n        this.name = 'BloomFilterError';\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An event from the RemoteStore. It is split into targetChanges (changes to the\r\n * state or the set of documents in our watched targets) and documentUpdates\r\n * (changes to the actual documents).\r\n */\r\nclass RemoteEvent {\r\n    constructor(\r\n    /**\r\n     * The snapshot version this event brings us up to, or MIN if not set.\r\n     */\r\n    snapshotVersion, \r\n    /**\r\n     * A map from target to changes to the target. See TargetChange.\r\n     */\r\n    targetChanges, \r\n    /**\r\n     * A map of targets that is known to be inconsistent, and the purpose for\r\n     * re-listening. Listens for these targets should be re-established without\r\n     * resume tokens.\r\n     */\r\n    targetMismatches, \r\n    /**\r\n     * A set of which documents have changed or been deleted, along with the\r\n     * doc's new values (if not deleted).\r\n     */\r\n    documentUpdates, \r\n    /**\r\n     * A set of which document updates are due only to limbo resolution targets.\r\n     */\r\n    resolvedLimboDocuments) {\r\n        this.snapshotVersion = snapshotVersion;\r\n        this.targetChanges = targetChanges;\r\n        this.targetMismatches = targetMismatches;\r\n        this.documentUpdates = documentUpdates;\r\n        this.resolvedLimboDocuments = resolvedLimboDocuments;\r\n    }\r\n    /**\r\n     * HACK: Views require RemoteEvents in order to determine whether the view is\r\n     * CURRENT, but secondary tabs don't receive remote events. So this method is\r\n     * used to create a synthesized RemoteEvent that can be used to apply a\r\n     * CURRENT status change to a View, for queries executed in a different tab.\r\n     */\r\n    // PORTING NOTE: Multi-tab only\r\n    static createSynthesizedRemoteEventForCurrentChange(targetId, current, resumeToken) {\r\n        const targetChanges = new Map();\r\n        targetChanges.set(targetId, TargetChange.createSynthesizedTargetChangeForCurrentChange(targetId, current, resumeToken));\r\n        return new RemoteEvent(SnapshotVersion.min(), targetChanges, new SortedMap(primitiveComparator), mutableDocumentMap(), documentKeySet());\r\n    }\r\n}\r\n/**\r\n * A TargetChange specifies the set of changes for a specific target as part of\r\n * a RemoteEvent. These changes track which documents are added, modified or\r\n * removed, as well as the target's resume token and whether the target is\r\n * marked CURRENT.\r\n * The actual changes *to* documents are not part of the TargetChange since\r\n * documents may be part of multiple targets.\r\n */\r\nclass TargetChange {\r\n    constructor(\r\n    /**\r\n     * An opaque, server-assigned token that allows watching a query to be resumed\r\n     * after disconnecting without retransmitting all the data that matches the\r\n     * query. The resume token essentially identifies a point in time from which\r\n     * the server should resume sending results.\r\n     */\r\n    resumeToken, \r\n    /**\r\n     * The \"current\" (synced) status of this target. Note that \"current\"\r\n     * has special meaning in the RPC protocol that implies that a target is\r\n     * both up-to-date and consistent with the rest of the watch stream.\r\n     */\r\n    current, \r\n    /**\r\n     * The set of documents that were newly assigned to this target as part of\r\n     * this remote event.\r\n     */\r\n    addedDocuments, \r\n    /**\r\n     * The set of documents that were already assigned to this target but received\r\n     * an update during this remote event.\r\n     */\r\n    modifiedDocuments, \r\n    /**\r\n     * The set of documents that were removed from this target as part of this\r\n     * remote event.\r\n     */\r\n    removedDocuments) {\r\n        this.resumeToken = resumeToken;\r\n        this.current = current;\r\n        this.addedDocuments = addedDocuments;\r\n        this.modifiedDocuments = modifiedDocuments;\r\n        this.removedDocuments = removedDocuments;\r\n    }\r\n    /**\r\n     * This method is used to create a synthesized TargetChanges that can be used to\r\n     * apply a CURRENT status change to a View (for queries executed in a different\r\n     * tab) or for new queries (to raise snapshots with correct CURRENT status).\r\n     */\r\n    static createSynthesizedTargetChangeForCurrentChange(targetId, current, resumeToken) {\r\n        return new TargetChange(resumeToken, current, documentKeySet(), documentKeySet(), documentKeySet());\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a changed document and a list of target ids to which this change\r\n * applies.\r\n *\r\n * If document has been deleted NoDocument will be provided.\r\n */\r\nclass DocumentWatchChange {\r\n    constructor(\r\n    /** The new document applies to all of these targets. */\r\n    updatedTargetIds, \r\n    /** The new document is removed from all of these targets. */\r\n    removedTargetIds, \r\n    /** The key of the document for this change. */\r\n    key, \r\n    /**\r\n     * The new document or NoDocument if it was deleted. Is null if the\r\n     * document went out of view without the server sending a new document.\r\n     */\r\n    newDoc) {\r\n        this.updatedTargetIds = updatedTargetIds;\r\n        this.removedTargetIds = removedTargetIds;\r\n        this.key = key;\r\n        this.newDoc = newDoc;\r\n    }\r\n}\r\nclass ExistenceFilterChange {\r\n    constructor(targetId, existenceFilter) {\r\n        this.targetId = targetId;\r\n        this.existenceFilter = existenceFilter;\r\n    }\r\n}\r\nclass WatchTargetChange {\r\n    constructor(\r\n    /** What kind of change occurred to the watch target. */\r\n    state, \r\n    /** The target IDs that were added/removed/set. */\r\n    targetIds, \r\n    /**\r\n     * An opaque, server-assigned token that allows watching a target to be\r\n     * resumed after disconnecting without retransmitting all the data that\r\n     * matches the target. The resume token essentially identifies a point in\r\n     * time from which the server should resume sending results.\r\n     */\r\n    resumeToken = ByteString.EMPTY_BYTE_STRING, \r\n    /** An RPC error indicating why the watch failed. */\r\n    cause = null) {\r\n        this.state = state;\r\n        this.targetIds = targetIds;\r\n        this.resumeToken = resumeToken;\r\n        this.cause = cause;\r\n    }\r\n}\r\n/** Tracks the internal state of a Watch target. */\r\nclass TargetState {\r\n    constructor() {\r\n        /**\r\n         * The number of pending responses (adds or removes) that we are waiting on.\r\n         * We only consider targets active that have no pending responses.\r\n         */\r\n        this.pendingResponses = 0;\r\n        /**\r\n         * Keeps track of the document changes since the last raised snapshot.\r\n         *\r\n         * These changes are continuously updated as we receive document updates and\r\n         * always reflect the current set of changes against the last issued snapshot.\r\n         */\r\n        this.documentChanges = snapshotChangesMap();\r\n        /** See public getters for explanations of these fields. */\r\n        this._resumeToken = ByteString.EMPTY_BYTE_STRING;\r\n        this._current = false;\r\n        /**\r\n         * Whether this target state should be included in the next snapshot. We\r\n         * initialize to true so that newly-added targets are included in the next\r\n         * RemoteEvent.\r\n         */\r\n        this._hasPendingChanges = true;\r\n    }\r\n    /**\r\n     * Whether this target has been marked 'current'.\r\n     *\r\n     * 'Current' has special meaning in the RPC protocol: It implies that the\r\n     * Watch backend has sent us all changes up to the point at which the target\r\n     * was added and that the target is consistent with the rest of the watch\r\n     * stream.\r\n     */\r\n    get current() {\r\n        return this._current;\r\n    }\r\n    /** The last resume token sent to us for this target. */\r\n    get resumeToken() {\r\n        return this._resumeToken;\r\n    }\r\n    /** Whether this target has pending target adds or target removes. */\r\n    get isPending() {\r\n        return this.pendingResponses !== 0;\r\n    }\r\n    /** Whether we have modified any state that should trigger a snapshot. */\r\n    get hasPendingChanges() {\r\n        return this._hasPendingChanges;\r\n    }\r\n    /**\r\n     * Applies the resume token to the TargetChange, but only when it has a new\r\n     * value. Empty resumeTokens are discarded.\r\n     */\r\n    updateResumeToken(resumeToken) {\r\n        if (resumeToken.approximateByteSize() > 0) {\r\n            this._hasPendingChanges = true;\r\n            this._resumeToken = resumeToken;\r\n        }\r\n    }\r\n    /**\r\n     * Creates a target change from the current set of changes.\r\n     *\r\n     * To reset the document changes after raising this snapshot, call\r\n     * `clearPendingChanges()`.\r\n     */\r\n    toTargetChange() {\r\n        let addedDocuments = documentKeySet();\r\n        let modifiedDocuments = documentKeySet();\r\n        let removedDocuments = documentKeySet();\r\n        this.documentChanges.forEach((key, changeType) => {\r\n            switch (changeType) {\r\n                case 0 /* ChangeType.Added */:\r\n                    addedDocuments = addedDocuments.add(key);\r\n                    break;\r\n                case 2 /* ChangeType.Modified */:\r\n                    modifiedDocuments = modifiedDocuments.add(key);\r\n                    break;\r\n                case 1 /* ChangeType.Removed */:\r\n                    removedDocuments = removedDocuments.add(key);\r\n                    break;\r\n                default:\r\n                    fail();\r\n            }\r\n        });\r\n        return new TargetChange(this._resumeToken, this._current, addedDocuments, modifiedDocuments, removedDocuments);\r\n    }\r\n    /**\r\n     * Resets the document changes and sets `hasPendingChanges` to false.\r\n     */\r\n    clearPendingChanges() {\r\n        this._hasPendingChanges = false;\r\n        this.documentChanges = snapshotChangesMap();\r\n    }\r\n    addDocumentChange(key, changeType) {\r\n        this._hasPendingChanges = true;\r\n        this.documentChanges = this.documentChanges.insert(key, changeType);\r\n    }\r\n    removeDocumentChange(key) {\r\n        this._hasPendingChanges = true;\r\n        this.documentChanges = this.documentChanges.remove(key);\r\n    }\r\n    recordPendingTargetRequest() {\r\n        this.pendingResponses += 1;\r\n    }\r\n    recordTargetResponse() {\r\n        this.pendingResponses -= 1;\r\n    }\r\n    markCurrent() {\r\n        this._hasPendingChanges = true;\r\n        this._current = true;\r\n    }\r\n}\r\nconst LOG_TAG$g = 'WatchChangeAggregator';\r\n/**\r\n * A helper class to accumulate watch changes into a RemoteEvent.\r\n */\r\nclass WatchChangeAggregator {\r\n    constructor(metadataProvider) {\r\n        this.metadataProvider = metadataProvider;\r\n        /** The internal state of all tracked targets. */\r\n        this.targetStates = new Map();\r\n        /** Keeps track of the documents to update since the last raised snapshot. */\r\n        this.pendingDocumentUpdates = mutableDocumentMap();\r\n        /** A mapping of document keys to their set of target IDs. */\r\n        this.pendingDocumentTargetMapping = documentTargetMap();\r\n        /**\r\n         * A map of targets with existence filter mismatches. These targets are\r\n         * known to be inconsistent and their listens needs to be re-established by\r\n         * RemoteStore.\r\n         */\r\n        this.pendingTargetResets = new SortedMap(primitiveComparator);\r\n    }\r\n    /**\r\n     * Processes and adds the DocumentWatchChange to the current set of changes.\r\n     */\r\n    handleDocumentChange(docChange) {\r\n        for (const targetId of docChange.updatedTargetIds) {\r\n            if (docChange.newDoc && docChange.newDoc.isFoundDocument()) {\r\n                this.addDocumentToTarget(targetId, docChange.newDoc);\r\n            }\r\n            else {\r\n                this.removeDocumentFromTarget(targetId, docChange.key, docChange.newDoc);\r\n            }\r\n        }\r\n        for (const targetId of docChange.removedTargetIds) {\r\n            this.removeDocumentFromTarget(targetId, docChange.key, docChange.newDoc);\r\n        }\r\n    }\r\n    /** Processes and adds the WatchTargetChange to the current set of changes. */\r\n    handleTargetChange(targetChange) {\r\n        this.forEachTarget(targetChange, targetId => {\r\n            const targetState = this.ensureTargetState(targetId);\r\n            switch (targetChange.state) {\r\n                case 0 /* WatchTargetChangeState.NoChange */:\r\n                    if (this.isActiveTarget(targetId)) {\r\n                        targetState.updateResumeToken(targetChange.resumeToken);\r\n                    }\r\n                    break;\r\n                case 1 /* WatchTargetChangeState.Added */:\r\n                    // We need to decrement the number of pending acks needed from watch\r\n                    // for this targetId.\r\n                    targetState.recordTargetResponse();\r\n                    if (!targetState.isPending) {\r\n                        // We have a freshly added target, so we need to reset any state\r\n                        // that we had previously. This can happen e.g. when remove and add\r\n                        // back a target for existence filter mismatches.\r\n                        targetState.clearPendingChanges();\r\n                    }\r\n                    targetState.updateResumeToken(targetChange.resumeToken);\r\n                    break;\r\n                case 2 /* WatchTargetChangeState.Removed */:\r\n                    // We need to keep track of removed targets to we can post-filter and\r\n                    // remove any target changes.\r\n                    // We need to decrement the number of pending acks needed from watch\r\n                    // for this targetId.\r\n                    targetState.recordTargetResponse();\r\n                    if (!targetState.isPending) {\r\n                        this.removeTarget(targetId);\r\n                    }\r\n                    break;\r\n                case 3 /* WatchTargetChangeState.Current */:\r\n                    if (this.isActiveTarget(targetId)) {\r\n                        targetState.markCurrent();\r\n                        targetState.updateResumeToken(targetChange.resumeToken);\r\n                    }\r\n                    break;\r\n                case 4 /* WatchTargetChangeState.Reset */:\r\n                    if (this.isActiveTarget(targetId)) {\r\n                        // Reset the target and synthesizes removes for all existing\r\n                        // documents. The backend will re-add any documents that still\r\n                        // match the target before it sends the next global snapshot.\r\n                        this.resetTarget(targetId);\r\n                        targetState.updateResumeToken(targetChange.resumeToken);\r\n                    }\r\n                    break;\r\n                default:\r\n                    fail();\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Iterates over all targetIds that the watch change applies to: either the\r\n     * targetIds explicitly listed in the change or the targetIds of all currently\r\n     * active targets.\r\n     */\r\n    forEachTarget(targetChange, fn) {\r\n        if (targetChange.targetIds.length > 0) {\r\n            targetChange.targetIds.forEach(fn);\r\n        }\r\n        else {\r\n            this.targetStates.forEach((_, targetId) => {\r\n                if (this.isActiveTarget(targetId)) {\r\n                    fn(targetId);\r\n                }\r\n            });\r\n        }\r\n    }\r\n    /**\r\n     * Handles existence filters and synthesizes deletes for filter mismatches.\r\n     * Targets that are invalidated by filter mismatches are added to\r\n     * `pendingTargetResets`.\r\n     */\r\n    handleExistenceFilter(watchChange) {\r\n        var _a;\r\n        const targetId = watchChange.targetId;\r\n        const expectedCount = watchChange.existenceFilter.count;\r\n        const targetData = this.targetDataForActiveTarget(targetId);\r\n        if (targetData) {\r\n            const target = targetData.target;\r\n            if (targetIsDocumentTarget(target)) {\r\n                if (expectedCount === 0) {\r\n                    // The existence filter told us the document does not exist. We deduce\r\n                    // that this document does not exist and apply a deleted document to\r\n                    // our updates. Without applying this deleted document there might be\r\n                    // another query that will raise this document as part of a snapshot\r\n                    // until it is resolved, essentially exposing inconsistency between\r\n                    // queries.\r\n                    const key = new DocumentKey(target.path);\r\n                    this.removeDocumentFromTarget(targetId, key, MutableDocument.newNoDocument(key, SnapshotVersion.min()));\r\n                }\r\n                else {\r\n                    hardAssert(expectedCount === 1);\r\n                }\r\n            }\r\n            else {\r\n                const currentSize = this.getCurrentDocumentCountForTarget(targetId);\r\n                // Existence filter mismatch. Mark the documents as being in limbo, and\r\n                // raise a snapshot with `isFromCache:true`.\r\n                if (currentSize !== expectedCount) {\r\n                    // Apply bloom filter to identify and mark removed documents.\r\n                    const status = this.applyBloomFilter(watchChange, currentSize);\r\n                    if (status !== 0 /* BloomFilterApplicationStatus.Success */) {\r\n                        // If bloom filter application fails, we reset the mapping and\r\n                        // trigger re-run of the query.\r\n                        this.resetTarget(targetId);\r\n                        const purpose = status === 2 /* BloomFilterApplicationStatus.FalsePositive */\r\n                            ? \"TargetPurposeExistenceFilterMismatchBloom\" /* TargetPurpose.ExistenceFilterMismatchBloom */\r\n                            : \"TargetPurposeExistenceFilterMismatch\" /* TargetPurpose.ExistenceFilterMismatch */;\r\n                        this.pendingTargetResets = this.pendingTargetResets.insert(targetId, purpose);\r\n                    }\r\n                    (_a = TestingHooks.instance) === null || _a === void 0 ? void 0 : _a.notifyOnExistenceFilterMismatch(createExistenceFilterMismatchInfoForTestingHooks(status, currentSize, watchChange.existenceFilter));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Apply bloom filter to remove the deleted documents, and return the\r\n     * application status.\r\n     */\r\n    applyBloomFilter(watchChange, currentCount) {\r\n        const { unchangedNames, count: expectedCount } = watchChange.existenceFilter;\r\n        if (!unchangedNames || !unchangedNames.bits) {\r\n            return 1 /* BloomFilterApplicationStatus.Skipped */;\r\n        }\r\n        const { bits: { bitmap = '', padding = 0 }, hashCount = 0 } = unchangedNames;\r\n        let normalizedBitmap;\r\n        try {\r\n            normalizedBitmap = normalizeByteString(bitmap).toUint8Array();\r\n        }\r\n        catch (err) {\r\n            if (err instanceof Base64DecodeError) {\r\n                logWarn('Decoding the base64 bloom filter in existence filter failed (' +\r\n                    err.message +\r\n                    '); ignoring the bloom filter and falling back to full re-query.');\r\n                return 1 /* BloomFilterApplicationStatus.Skipped */;\r\n            }\r\n            else {\r\n                throw err;\r\n            }\r\n        }\r\n        let bloomFilter;\r\n        try {\r\n            // BloomFilter throws error if the inputs are invalid.\r\n            bloomFilter = new BloomFilter(normalizedBitmap, padding, hashCount);\r\n        }\r\n        catch (err) {\r\n            if (err instanceof BloomFilterError) {\r\n                logWarn('BloomFilter error: ', err);\r\n            }\r\n            else {\r\n                logWarn('Applying bloom filter failed: ', err);\r\n            }\r\n            return 1 /* BloomFilterApplicationStatus.Skipped */;\r\n        }\r\n        if (bloomFilter.bitCount === 0) {\r\n            return 1 /* BloomFilterApplicationStatus.Skipped */;\r\n        }\r\n        const removedDocumentCount = this.filterRemovedDocuments(watchChange.targetId, bloomFilter);\r\n        if (expectedCount !== currentCount - removedDocumentCount) {\r\n            return 2 /* BloomFilterApplicationStatus.FalsePositive */;\r\n        }\r\n        return 0 /* BloomFilterApplicationStatus.Success */;\r\n    }\r\n    /**\r\n     * Filter out removed documents based on bloom filter membership result and\r\n     * return number of documents removed.\r\n     */\r\n    filterRemovedDocuments(targetId, bloomFilter) {\r\n        const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\r\n        let removalCount = 0;\r\n        existingKeys.forEach(key => {\r\n            const databaseId = this.metadataProvider.getDatabaseId();\r\n            const documentPath = `projects/${databaseId.projectId}/databases/${databaseId.database}/documents/${key.path.canonicalString()}`;\r\n            if (!bloomFilter.mightContain(documentPath)) {\r\n                this.removeDocumentFromTarget(targetId, key, /*updatedDocument=*/ null);\r\n                removalCount++;\r\n            }\r\n        });\r\n        return removalCount;\r\n    }\r\n    /**\r\n     * Converts the currently accumulated state into a remote event at the\r\n     * provided snapshot version. Resets the accumulated changes before returning.\r\n     */\r\n    createRemoteEvent(snapshotVersion) {\r\n        const targetChanges = new Map();\r\n        this.targetStates.forEach((targetState, targetId) => {\r\n            const targetData = this.targetDataForActiveTarget(targetId);\r\n            if (targetData) {\r\n                if (targetState.current && targetIsDocumentTarget(targetData.target)) {\r\n                    // Document queries for document that don't exist can produce an empty\r\n                    // result set. To update our local cache, we synthesize a document\r\n                    // delete if we have not previously received the document. This\r\n                    // resolves the limbo state of the document, removing it from\r\n                    // limboDocumentRefs.\r\n                    //\r\n                    // TODO(dimond): Ideally we would have an explicit lookup target\r\n                    // instead resulting in an explicit delete message and we could\r\n                    // remove this special logic.\r\n                    const key = new DocumentKey(targetData.target.path);\r\n                    if (this.pendingDocumentUpdates.get(key) === null &&\r\n                        !this.targetContainsDocument(targetId, key)) {\r\n                        this.removeDocumentFromTarget(targetId, key, MutableDocument.newNoDocument(key, snapshotVersion));\r\n                    }\r\n                }\r\n                if (targetState.hasPendingChanges) {\r\n                    targetChanges.set(targetId, targetState.toTargetChange());\r\n                    targetState.clearPendingChanges();\r\n                }\r\n            }\r\n        });\r\n        let resolvedLimboDocuments = documentKeySet();\r\n        // We extract the set of limbo-only document updates as the GC logic\r\n        // special-cases documents that do not appear in the target cache.\r\n        //\r\n        // TODO(gsoltis): Expand on this comment once GC is available in the JS\r\n        // client.\r\n        this.pendingDocumentTargetMapping.forEach((key, targets) => {\r\n            let isOnlyLimboTarget = true;\r\n            targets.forEachWhile(targetId => {\r\n                const targetData = this.targetDataForActiveTarget(targetId);\r\n                if (targetData &&\r\n                    targetData.purpose !== \"TargetPurposeLimboResolution\" /* TargetPurpose.LimboResolution */) {\r\n                    isOnlyLimboTarget = false;\r\n                    return false;\r\n                }\r\n                return true;\r\n            });\r\n            if (isOnlyLimboTarget) {\r\n                resolvedLimboDocuments = resolvedLimboDocuments.add(key);\r\n            }\r\n        });\r\n        this.pendingDocumentUpdates.forEach((_, doc) => doc.setReadTime(snapshotVersion));\r\n        const remoteEvent = new RemoteEvent(snapshotVersion, targetChanges, this.pendingTargetResets, this.pendingDocumentUpdates, resolvedLimboDocuments);\r\n        this.pendingDocumentUpdates = mutableDocumentMap();\r\n        this.pendingDocumentTargetMapping = documentTargetMap();\r\n        this.pendingTargetResets = new SortedMap(primitiveComparator);\r\n        return remoteEvent;\r\n    }\r\n    /**\r\n     * Adds the provided document to the internal list of document updates and\r\n     * its document key to the given target's mapping.\r\n     */\r\n    // Visible for testing.\r\n    addDocumentToTarget(targetId, document) {\r\n        if (!this.isActiveTarget(targetId)) {\r\n            return;\r\n        }\r\n        const changeType = this.targetContainsDocument(targetId, document.key)\r\n            ? 2 /* ChangeType.Modified */\r\n            : 0 /* ChangeType.Added */;\r\n        const targetState = this.ensureTargetState(targetId);\r\n        targetState.addDocumentChange(document.key, changeType);\r\n        this.pendingDocumentUpdates = this.pendingDocumentUpdates.insert(document.key, document);\r\n        this.pendingDocumentTargetMapping =\r\n            this.pendingDocumentTargetMapping.insert(document.key, this.ensureDocumentTargetMapping(document.key).add(targetId));\r\n    }\r\n    /**\r\n     * Removes the provided document from the target mapping. If the\r\n     * document no longer matches the target, but the document's state is still\r\n     * known (e.g. we know that the document was deleted or we received the change\r\n     * that caused the filter mismatch), the new document can be provided\r\n     * to update the remote document cache.\r\n     */\r\n    // Visible for testing.\r\n    removeDocumentFromTarget(targetId, key, updatedDocument) {\r\n        if (!this.isActiveTarget(targetId)) {\r\n            return;\r\n        }\r\n        const targetState = this.ensureTargetState(targetId);\r\n        if (this.targetContainsDocument(targetId, key)) {\r\n            targetState.addDocumentChange(key, 1 /* ChangeType.Removed */);\r\n        }\r\n        else {\r\n            // The document may have entered and left the target before we raised a\r\n            // snapshot, so we can just ignore the change.\r\n            targetState.removeDocumentChange(key);\r\n        }\r\n        this.pendingDocumentTargetMapping =\r\n            this.pendingDocumentTargetMapping.insert(key, this.ensureDocumentTargetMapping(key).delete(targetId));\r\n        if (updatedDocument) {\r\n            this.pendingDocumentUpdates = this.pendingDocumentUpdates.insert(key, updatedDocument);\r\n        }\r\n    }\r\n    removeTarget(targetId) {\r\n        this.targetStates.delete(targetId);\r\n    }\r\n    /**\r\n     * Returns the current count of documents in the target. This includes both\r\n     * the number of documents that the LocalStore considers to be part of the\r\n     * target as well as any accumulated changes.\r\n     */\r\n    getCurrentDocumentCountForTarget(targetId) {\r\n        const targetState = this.ensureTargetState(targetId);\r\n        const targetChange = targetState.toTargetChange();\r\n        return (this.metadataProvider.getRemoteKeysForTarget(targetId).size +\r\n            targetChange.addedDocuments.size -\r\n            targetChange.removedDocuments.size);\r\n    }\r\n    /**\r\n     * Increment the number of acks needed from watch before we can consider the\r\n     * server to be 'in-sync' with the client's active targets.\r\n     */\r\n    recordPendingTargetRequest(targetId) {\r\n        // For each request we get we need to record we need a response for it.\r\n        const targetState = this.ensureTargetState(targetId);\r\n        targetState.recordPendingTargetRequest();\r\n    }\r\n    ensureTargetState(targetId) {\r\n        let result = this.targetStates.get(targetId);\r\n        if (!result) {\r\n            result = new TargetState();\r\n            this.targetStates.set(targetId, result);\r\n        }\r\n        return result;\r\n    }\r\n    ensureDocumentTargetMapping(key) {\r\n        let targetMapping = this.pendingDocumentTargetMapping.get(key);\r\n        if (!targetMapping) {\r\n            targetMapping = new SortedSet(primitiveComparator);\r\n            this.pendingDocumentTargetMapping =\r\n                this.pendingDocumentTargetMapping.insert(key, targetMapping);\r\n        }\r\n        return targetMapping;\r\n    }\r\n    /**\r\n     * Verifies that the user is still interested in this target (by calling\r\n     * `getTargetDataForTarget()`) and that we are not waiting for pending ADDs\r\n     * from watch.\r\n     */\r\n    isActiveTarget(targetId) {\r\n        const targetActive = this.targetDataForActiveTarget(targetId) !== null;\r\n        if (!targetActive) {\r\n            logDebug(LOG_TAG$g, 'Detected inactive target', targetId);\r\n        }\r\n        return targetActive;\r\n    }\r\n    /**\r\n     * Returns the TargetData for an active target (i.e. a target that the user\r\n     * is still interested in that has no outstanding target change requests).\r\n     */\r\n    targetDataForActiveTarget(targetId) {\r\n        const targetState = this.targetStates.get(targetId);\r\n        return targetState && targetState.isPending\r\n            ? null\r\n            : this.metadataProvider.getTargetDataForTarget(targetId);\r\n    }\r\n    /**\r\n     * Resets the state of a Watch target to its initial state (e.g. sets\r\n     * 'current' to false, clears the resume token and removes its target mapping\r\n     * from all documents).\r\n     */\r\n    resetTarget(targetId) {\r\n        this.targetStates.set(targetId, new TargetState());\r\n        // Trigger removal for any documents currently mapped to this target.\r\n        // These removals will be part of the initial snapshot if Watch does not\r\n        // resend these documents.\r\n        const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\r\n        existingKeys.forEach(key => {\r\n            this.removeDocumentFromTarget(targetId, key, /*updatedDocument=*/ null);\r\n        });\r\n    }\r\n    /**\r\n     * Returns whether the LocalStore considers the document to be part of the\r\n     * specified target.\r\n     */\r\n    targetContainsDocument(targetId, key) {\r\n        const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\r\n        return existingKeys.has(key);\r\n    }\r\n}\r\nfunction documentTargetMap() {\r\n    return new SortedMap(DocumentKey.comparator);\r\n}\r\nfunction snapshotChangesMap() {\r\n    return new SortedMap(DocumentKey.comparator);\r\n}\r\nfunction createExistenceFilterMismatchInfoForTestingHooks(status, localCacheCount, existenceFilter) {\r\n    var _a, _b, _c, _d, _e, _f;\r\n    const result = {\r\n        localCacheCount,\r\n        existenceFilterCount: existenceFilter.count\r\n    };\r\n    const unchangedNames = existenceFilter.unchangedNames;\r\n    if (unchangedNames) {\r\n        result.bloomFilter = {\r\n            applied: status === 0 /* BloomFilterApplicationStatus.Success */,\r\n            hashCount: (_a = unchangedNames === null || unchangedNames === void 0 ? void 0 : unchangedNames.hashCount) !== null && _a !== void 0 ? _a : 0,\r\n            bitmapLength: (_d = (_c = (_b = unchangedNames === null || unchangedNames === void 0 ? void 0 : unchangedNames.bits) === null || _b === void 0 ? void 0 : _b.bitmap) === null || _c === void 0 ? void 0 : _c.length) !== null && _d !== void 0 ? _d : 0,\r\n            padding: (_f = (_e = unchangedNames === null || unchangedNames === void 0 ? void 0 : unchangedNames.bits) === null || _e === void 0 ? void 0 : _e.padding) !== null && _f !== void 0 ? _f : 0\r\n        };\r\n    }\r\n    return result;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst DIRECTIONS = (() => {\r\n    const dirs = {};\r\n    dirs[\"asc\" /* Direction.ASCENDING */] = 'ASCENDING';\r\n    dirs[\"desc\" /* Direction.DESCENDING */] = 'DESCENDING';\r\n    return dirs;\r\n})();\r\nconst OPERATORS = (() => {\r\n    const ops = {};\r\n    ops[\"<\" /* Operator.LESS_THAN */] = 'LESS_THAN';\r\n    ops[\"<=\" /* Operator.LESS_THAN_OR_EQUAL */] = 'LESS_THAN_OR_EQUAL';\r\n    ops[\">\" /* Operator.GREATER_THAN */] = 'GREATER_THAN';\r\n    ops[\">=\" /* Operator.GREATER_THAN_OR_EQUAL */] = 'GREATER_THAN_OR_EQUAL';\r\n    ops[\"==\" /* Operator.EQUAL */] = 'EQUAL';\r\n    ops[\"!=\" /* Operator.NOT_EQUAL */] = 'NOT_EQUAL';\r\n    ops[\"array-contains\" /* Operator.ARRAY_CONTAINS */] = 'ARRAY_CONTAINS';\r\n    ops[\"in\" /* Operator.IN */] = 'IN';\r\n    ops[\"not-in\" /* Operator.NOT_IN */] = 'NOT_IN';\r\n    ops[\"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */] = 'ARRAY_CONTAINS_ANY';\r\n    return ops;\r\n})();\r\nconst COMPOSITE_OPERATORS = (() => {\r\n    const ops = {};\r\n    ops[\"and\" /* CompositeOperator.AND */] = 'AND';\r\n    ops[\"or\" /* CompositeOperator.OR */] = 'OR';\r\n    return ops;\r\n})();\r\nfunction assertPresent(value, description) {\r\n}\r\n/**\r\n * This class generates JsonObject values for the Datastore API suitable for\r\n * sending to either GRPC stub methods or via the JSON/HTTP REST API.\r\n *\r\n * The serializer supports both Protobuf.js and Proto3 JSON formats. By\r\n * setting `useProto3Json` to true, the serializer will use the Proto3 JSON\r\n * format.\r\n *\r\n * For a description of the Proto3 JSON format check\r\n * https://developers.google.com/protocol-buffers/docs/proto3#json\r\n *\r\n * TODO(klimt): We can remove the databaseId argument if we keep the full\r\n * resource name in documents.\r\n */\r\nclass JsonProtoSerializer {\r\n    constructor(databaseId, useProto3Json) {\r\n        this.databaseId = databaseId;\r\n        this.useProto3Json = useProto3Json;\r\n    }\r\n}\r\nfunction fromRpcStatus(status) {\r\n    const code = status.code === undefined ? Code.UNKNOWN : mapCodeFromRpcCode(status.code);\r\n    return new FirestoreError(code, status.message || '');\r\n}\r\n/**\r\n * Returns a value for a number (or null) that's appropriate to put into\r\n * a google.protobuf.Int32Value proto.\r\n * DO NOT USE THIS FOR ANYTHING ELSE.\r\n * This method cheats. It's typed as returning \"number\" because that's what\r\n * our generated proto interfaces say Int32Value must be. But GRPC actually\r\n * expects a { value: <number> } struct.\r\n */\r\nfunction toInt32Proto(serializer, val) {\r\n    if (serializer.useProto3Json || isNullOrUndefined(val)) {\r\n        return val;\r\n    }\r\n    else {\r\n        return { value: val };\r\n    }\r\n}\r\n/**\r\n * Returns a number (or null) from a google.protobuf.Int32Value proto.\r\n */\r\nfunction fromInt32Proto(val) {\r\n    let result;\r\n    if (typeof val === 'object') {\r\n        result = val.value;\r\n    }\r\n    else {\r\n        result = val;\r\n    }\r\n    return isNullOrUndefined(result) ? null : result;\r\n}\r\n/**\r\n * Returns a value for a Date that's appropriate to put into a proto.\r\n */\r\nfunction toTimestamp(serializer, timestamp) {\r\n    if (serializer.useProto3Json) {\r\n        // Serialize to ISO-8601 date format, but with full nano resolution.\r\n        // Since JS Date has only millis, let's only use it for the seconds and\r\n        // then manually add the fractions to the end.\r\n        const jsDateStr = new Date(timestamp.seconds * 1000).toISOString();\r\n        // Remove .xxx frac part and Z in the end.\r\n        const strUntilSeconds = jsDateStr.replace(/\\.\\d*/, '').replace('Z', '');\r\n        // Pad the fraction out to 9 digits (nanos).\r\n        const nanoStr = ('000000000' + timestamp.nanoseconds).slice(-9);\r\n        return `${strUntilSeconds}.${nanoStr}Z`;\r\n    }\r\n    else {\r\n        return {\r\n            seconds: '' + timestamp.seconds,\r\n            nanos: timestamp.nanoseconds\r\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        };\r\n    }\r\n}\r\nfunction fromTimestamp(date) {\r\n    const timestamp = normalizeTimestamp(date);\r\n    return new Timestamp(timestamp.seconds, timestamp.nanos);\r\n}\r\n/**\r\n * Returns a value for bytes that's appropriate to put in a proto.\r\n *\r\n * Visible for testing.\r\n */\r\nfunction toBytes(serializer, bytes) {\r\n    if (serializer.useProto3Json) {\r\n        return bytes.toBase64();\r\n    }\r\n    else {\r\n        return bytes.toUint8Array();\r\n    }\r\n}\r\n/**\r\n * Returns a ByteString based on the proto string value.\r\n */\r\nfunction fromBytes(serializer, value) {\r\n    if (serializer.useProto3Json) {\r\n        hardAssert(value === undefined || typeof value === 'string');\r\n        return ByteString.fromBase64String(value ? value : '');\r\n    }\r\n    else {\r\n        hardAssert(value === undefined || value instanceof Uint8Array);\r\n        return ByteString.fromUint8Array(value ? value : new Uint8Array());\r\n    }\r\n}\r\nfunction toVersion(serializer, version) {\r\n    return toTimestamp(serializer, version.toTimestamp());\r\n}\r\nfunction fromVersion(version) {\r\n    hardAssert(!!version);\r\n    return SnapshotVersion.fromTimestamp(fromTimestamp(version));\r\n}\r\nfunction toResourceName(databaseId, path) {\r\n    return fullyQualifiedPrefixPath(databaseId)\r\n        .child('documents')\r\n        .child(path)\r\n        .canonicalString();\r\n}\r\nfunction fromResourceName(name) {\r\n    const resource = ResourcePath.fromString(name);\r\n    hardAssert(isValidResourceName(resource));\r\n    return resource;\r\n}\r\nfunction toName(serializer, key) {\r\n    return toResourceName(serializer.databaseId, key.path);\r\n}\r\nfunction fromName(serializer, name) {\r\n    const resource = fromResourceName(name);\r\n    if (resource.get(1) !== serializer.databaseId.projectId) {\r\n        throw new FirestoreError(Code.INVALID_ARGUMENT, 'Tried to deserialize key from different project: ' +\r\n            resource.get(1) +\r\n            ' vs ' +\r\n            serializer.databaseId.projectId);\r\n    }\r\n    if (resource.get(3) !== serializer.databaseId.database) {\r\n        throw new FirestoreError(Code.INVALID_ARGUMENT, 'Tried to deserialize key from different database: ' +\r\n            resource.get(3) +\r\n            ' vs ' +\r\n            serializer.databaseId.database);\r\n    }\r\n    return new DocumentKey(extractLocalPathFromResourceName(resource));\r\n}\r\nfunction toQueryPath(serializer, path) {\r\n    return toResourceName(serializer.databaseId, path);\r\n}\r\nfunction fromQueryPath(name) {\r\n    const resourceName = fromResourceName(name);\r\n    // In v1beta1 queries for collections at the root did not have a trailing\r\n    // \"/documents\". In v1 all resource paths contain \"/documents\". Preserve the\r\n    // ability to read the v1beta1 form for compatibility with queries persisted\r\n    // in the local target cache.\r\n    if (resourceName.length === 4) {\r\n        return ResourcePath.emptyPath();\r\n    }\r\n    return extractLocalPathFromResourceName(resourceName);\r\n}\r\nfunction getEncodedDatabaseId(serializer) {\r\n    const path = new ResourcePath([\r\n        'projects',\r\n        serializer.databaseId.projectId,\r\n        'databases',\r\n        serializer.databaseId.database\r\n    ]);\r\n    return path.canonicalString();\r\n}\r\nfunction fullyQualifiedPrefixPath(databaseId) {\r\n    return new ResourcePath([\r\n        'projects',\r\n        databaseId.projectId,\r\n        'databases',\r\n        databaseId.database\r\n    ]);\r\n}\r\nfunction extractLocalPathFromResourceName(resourceName) {\r\n    hardAssert(resourceName.length > 4 && resourceName.get(4) === 'documents');\r\n    return resourceName.popFirst(5);\r\n}\r\n/** Creates a Document proto from key and fields (but no create/update time) */\r\nfunction toMutationDocument(serializer, key, fields) {\r\n    return {\r\n        name: toName(serializer, key),\r\n        fields: fields.value.mapValue.fields\r\n    };\r\n}\r\nfunction toDocument(serializer, document) {\r\n    return {\r\n        name: toName(serializer, document.key),\r\n        fields: document.data.value.mapValue.fields,\r\n        updateTime: toTimestamp(serializer, document.version.toTimestamp()),\r\n        createTime: toTimestamp(serializer, document.createTime.toTimestamp())\r\n    };\r\n}\r\nfunction fromDocument(serializer, document, hasCommittedMutations) {\r\n    const key = fromName(serializer, document.name);\r\n    const version = fromVersion(document.updateTime);\r\n    // If we read a document from persistence that is missing createTime, it's due\r\n    // to older SDK versions not storing this information. In such cases, we'll\r\n    // set the createTime to zero. This can be removed in the long term.\r\n    const createTime = document.createTime\r\n        ? fromVersion(document.createTime)\r\n        : SnapshotVersion.min();\r\n    const data = new ObjectValue({ mapValue: { fields: document.fields } });\r\n    const result = MutableDocument.newFoundDocument(key, version, createTime, data);\r\n    if (hasCommittedMutations) {\r\n        result.setHasCommittedMutations();\r\n    }\r\n    return hasCommittedMutations ? result.setHasCommittedMutations() : result;\r\n}\r\nfunction fromFound(serializer, doc) {\r\n    hardAssert(!!doc.found);\r\n    assertPresent(doc.found.name);\r\n    assertPresent(doc.found.updateTime);\r\n    const key = fromName(serializer, doc.found.name);\r\n    const version = fromVersion(doc.found.updateTime);\r\n    const createTime = doc.found.createTime\r\n        ? fromVersion(doc.found.createTime)\r\n        : SnapshotVersion.min();\r\n    const data = new ObjectValue({ mapValue: { fields: doc.found.fields } });\r\n    return MutableDocument.newFoundDocument(key, version, createTime, data);\r\n}\r\nfunction fromMissing(serializer, result) {\r\n    hardAssert(!!result.missing);\r\n    hardAssert(!!result.readTime);\r\n    const key = fromName(serializer, result.missing);\r\n    const version = fromVersion(result.readTime);\r\n    return MutableDocument.newNoDocument(key, version);\r\n}\r\nfunction fromBatchGetDocumentsResponse(serializer, result) {\r\n    if ('found' in result) {\r\n        return fromFound(serializer, result);\r\n    }\r\n    else if ('missing' in result) {\r\n        return fromMissing(serializer, result);\r\n    }\r\n    return fail();\r\n}\r\nfunction fromWatchChange(serializer, change) {\r\n    let watchChange;\r\n    if ('targetChange' in change) {\r\n        assertPresent(change.targetChange);\r\n        // proto3 default value is unset in JSON (undefined), so use 'NO_CHANGE'\r\n        // if unset\r\n        const state = fromWatchTargetChangeState(change.targetChange.targetChangeType || 'NO_CHANGE');\r\n        const targetIds = change.targetChange.targetIds || [];\r\n        const resumeToken = fromBytes(serializer, change.targetChange.resumeToken);\r\n        const causeProto = change.targetChange.cause;\r\n        const cause = causeProto && fromRpcStatus(causeProto);\r\n        watchChange = new WatchTargetChange(state, targetIds, resumeToken, cause || null);\r\n    }\r\n    else if ('documentChange' in change) {\r\n        assertPresent(change.documentChange);\r\n        const entityChange = change.documentChange;\r\n        assertPresent(entityChange.document);\r\n        assertPresent(entityChange.document.name);\r\n        assertPresent(entityChange.document.updateTime);\r\n        const key = fromName(serializer, entityChange.document.name);\r\n        const version = fromVersion(entityChange.document.updateTime);\r\n        const createTime = entityChange.document.createTime\r\n            ? fromVersion(entityChange.document.createTime)\r\n            : SnapshotVersion.min();\r\n        const data = new ObjectValue({\r\n            mapValue: { fields: entityChange.document.fields }\r\n        });\r\n        const doc = MutableDocument.newFoundDocument(key, version, createTime, data);\r\n        const updatedTargetIds = entityChange.targetIds || [];\r\n        const removedTargetIds = entityChange.removedTargetIds || [];\r\n        watchChange = new DocumentWatchChange(updatedTargetIds, removedTargetIds, doc.key, doc);\r\n    }\r\n    else if ('documentDelete' in change) {\r\n        assertPresent(change.documentDelete);\r\n        const docDelete = change.documentDelete;\r\n        assertPresent(docDelete.document);\r\n        const key = fromName(serializer, docDelete.document);\r\n        const version = docDelete.readTime\r\n            ? fromVersion(docDelete.readTime)\r\n            : SnapshotVersion.min();\r\n        const doc = MutableDocument.newNoDocument(key, version);\r\n        const removedTargetIds = docDelete.removedTargetIds || [];\r\n        watchChange = new DocumentWatchChange([], removedTargetIds, doc.key, doc);\r\n    }\r\n    else if ('documentRemove' in change) {\r\n        assertPresent(change.documentRemove);\r\n        const docRemove = change.documentRemove;\r\n        assertPresent(docRemove.document);\r\n        const key = fromName(serializer, docRemove.document);\r\n        const removedTargetIds = docRemove.removedTargetIds || [];\r\n        watchChange = new DocumentWatchChange([], removedTargetIds, key, null);\r\n    }\r\n    else if ('filter' in change) {\r\n        // TODO(dimond): implement existence filter parsing with strategy.\r\n        assertPresent(change.filter);\r\n        const filter = change.filter;\r\n        assertPresent(filter.targetId);\r\n        const { count = 0, unchangedNames } = filter;\r\n        const existenceFilter = new ExistenceFilter(count, unchangedNames);\r\n        const targetId = filter.targetId;\r\n        watchChange = new ExistenceFilterChange(targetId, existenceFilter);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    return watchChange;\r\n}\r\nfunction fromWatchTargetChangeState(state) {\r\n    if (state === 'NO_CHANGE') {\r\n        return 0 /* WatchTargetChangeState.NoChange */;\r\n    }\r\n    else if (state === 'ADD') {\r\n        return 1 /* WatchTargetChangeState.Added */;\r\n    }\r\n    else if (state === 'REMOVE') {\r\n        return 2 /* WatchTargetChangeState.Removed */;\r\n    }\r\n    else if (state === 'CURRENT') {\r\n        return 3 /* WatchTargetChangeState.Current */;\r\n    }\r\n    else if (state === 'RESET') {\r\n        return 4 /* WatchTargetChangeState.Reset */;\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction versionFromListenResponse(change) {\r\n    // We have only reached a consistent snapshot for the entire stream if there\r\n    // is a read_time set and it applies to all targets (i.e. the list of\r\n    // targets is empty). The backend is guaranteed to send such responses.\r\n    if (!('targetChange' in change)) {\r\n        return SnapshotVersion.min();\r\n    }\r\n    const targetChange = change.targetChange;\r\n    if (targetChange.targetIds && targetChange.targetIds.length) {\r\n        return SnapshotVersion.min();\r\n    }\r\n    if (!targetChange.readTime) {\r\n        return SnapshotVersion.min();\r\n    }\r\n    return fromVersion(targetChange.readTime);\r\n}\r\nfunction toMutation(serializer, mutation) {\r\n    let result;\r\n    if (mutation instanceof SetMutation) {\r\n        result = {\r\n            update: toMutationDocument(serializer, mutation.key, mutation.value)\r\n        };\r\n    }\r\n    else if (mutation instanceof DeleteMutation) {\r\n        result = { delete: toName(serializer, mutation.key) };\r\n    }\r\n    else if (mutation instanceof PatchMutation) {\r\n        result = {\r\n            update: toMutationDocument(serializer, mutation.key, mutation.data),\r\n            updateMask: toDocumentMask(mutation.fieldMask)\r\n        };\r\n    }\r\n    else if (mutation instanceof VerifyMutation) {\r\n        result = {\r\n            verify: toName(serializer, mutation.key)\r\n        };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    if (mutation.fieldTransforms.length > 0) {\r\n        result.updateTransforms = mutation.fieldTransforms.map(transform => toFieldTransform(serializer, transform));\r\n    }\r\n    if (!mutation.precondition.isNone) {\r\n        result.currentDocument = toPrecondition(serializer, mutation.precondition);\r\n    }\r\n    return result;\r\n}\r\nfunction fromMutation(serializer, proto) {\r\n    const precondition = proto.currentDocument\r\n        ? fromPrecondition(proto.currentDocument)\r\n        : Precondition.none();\r\n    const fieldTransforms = proto.updateTransforms\r\n        ? proto.updateTransforms.map(transform => fromFieldTransform(serializer, transform))\r\n        : [];\r\n    if (proto.update) {\r\n        assertPresent(proto.update.name);\r\n        const key = fromName(serializer, proto.update.name);\r\n        const value = new ObjectValue({\r\n            mapValue: { fields: proto.update.fields }\r\n        });\r\n        if (proto.updateMask) {\r\n            const fieldMask = fromDocumentMask(proto.updateMask);\r\n            return new PatchMutation(key, value, fieldMask, precondition, fieldTransforms);\r\n        }\r\n        else {\r\n            return new SetMutation(key, value, precondition, fieldTransforms);\r\n        }\r\n    }\r\n    else if (proto.delete) {\r\n        const key = fromName(serializer, proto.delete);\r\n        return new DeleteMutation(key, precondition);\r\n    }\r\n    else if (proto.verify) {\r\n        const key = fromName(serializer, proto.verify);\r\n        return new VerifyMutation(key, precondition);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction toPrecondition(serializer, precondition) {\r\n    if (precondition.updateTime !== undefined) {\r\n        return {\r\n            updateTime: toVersion(serializer, precondition.updateTime)\r\n        };\r\n    }\r\n    else if (precondition.exists !== undefined) {\r\n        return { exists: precondition.exists };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction fromPrecondition(precondition) {\r\n    if (precondition.updateTime !== undefined) {\r\n        return Precondition.updateTime(fromVersion(precondition.updateTime));\r\n    }\r\n    else if (precondition.exists !== undefined) {\r\n        return Precondition.exists(precondition.exists);\r\n    }\r\n    else {\r\n        return Precondition.none();\r\n    }\r\n}\r\nfunction fromWriteResult(proto, commitTime) {\r\n    // NOTE: Deletes don't have an updateTime.\r\n    let version = proto.updateTime\r\n        ? fromVersion(proto.updateTime)\r\n        : fromVersion(commitTime);\r\n    if (version.isEqual(SnapshotVersion.min())) {\r\n        // The Firestore Emulator currently returns an update time of 0 for\r\n        // deletes of non-existing documents (rather than null). This breaks the\r\n        // test \"get deleted doc while offline with source=cache\" as NoDocuments\r\n        // with version 0 are filtered by IndexedDb's RemoteDocumentCache.\r\n        // TODO(#2149): Remove this when Emulator is fixed\r\n        version = fromVersion(commitTime);\r\n    }\r\n    return new MutationResult(version, proto.transformResults || []);\r\n}\r\nfunction fromWriteResults(protos, commitTime) {\r\n    if (protos && protos.length > 0) {\r\n        hardAssert(commitTime !== undefined);\r\n        return protos.map(proto => fromWriteResult(proto, commitTime));\r\n    }\r\n    else {\r\n        return [];\r\n    }\r\n}\r\nfunction toFieldTransform(serializer, fieldTransform) {\r\n    const transform = fieldTransform.transform;\r\n    if (transform instanceof ServerTimestampTransform) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            setToServerValue: 'REQUEST_TIME'\r\n        };\r\n    }\r\n    else if (transform instanceof ArrayUnionTransformOperation) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            appendMissingElements: {\r\n                values: transform.elements\r\n            }\r\n        };\r\n    }\r\n    else if (transform instanceof ArrayRemoveTransformOperation) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            removeAllFromArray: {\r\n                values: transform.elements\r\n            }\r\n        };\r\n    }\r\n    else if (transform instanceof NumericIncrementTransformOperation) {\r\n        return {\r\n            fieldPath: fieldTransform.field.canonicalString(),\r\n            increment: transform.operand\r\n        };\r\n    }\r\n    else {\r\n        throw fail();\r\n    }\r\n}\r\nfunction fromFieldTransform(serializer, proto) {\r\n    let transform = null;\r\n    if ('setToServerValue' in proto) {\r\n        hardAssert(proto.setToServerValue === 'REQUEST_TIME');\r\n        transform = new ServerTimestampTransform();\r\n    }\r\n    else if ('appendMissingElements' in proto) {\r\n        const values = proto.appendMissingElements.values || [];\r\n        transform = new ArrayUnionTransformOperation(values);\r\n    }\r\n    else if ('removeAllFromArray' in proto) {\r\n        const values = proto.removeAllFromArray.values || [];\r\n        transform = new ArrayRemoveTransformOperation(values);\r\n    }\r\n    else if ('increment' in proto) {\r\n        transform = new NumericIncrementTransformOperation(serializer, proto.increment);\r\n    }\r\n    else {\r\n        fail();\r\n    }\r\n    const fieldPath = FieldPath$1.fromServerFormat(proto.fieldPath);\r\n    return new FieldTransform(fieldPath, transform);\r\n}\r\nfunction toDocumentsTarget(serializer, target) {\r\n    return { documents: [toQueryPath(serializer, target.path)] };\r\n}\r\nfunction fromDocumentsTarget(documentsTarget) {\r\n    const count = documentsTarget.documents.length;\r\n    hardAssert(count === 1);\r\n    const name = documentsTarget.documents[0];\r\n    return queryToTarget(newQueryForPath(fromQueryPath(name)));\r\n}\r\nfunction toQueryTarget(serializer, target) {\r\n    // Dissect the path into parent, collectionId, and optional key filter.\r\n    const result = { structuredQuery: {} };\r\n    const path = target.path;\r\n    if (target.collectionGroup !== null) {\r\n        result.parent = toQueryPath(serializer, path);\r\n        result.structuredQuery.from = [\r\n            {\r\n                collectionId: target.collectionGroup,\r\n                allDescendants: true\r\n            }\r\n        ];\r\n    }\r\n    else {\r\n        result.parent = toQueryPath(serializer, path.popLast());\r\n        result.structuredQuery.from = [{ collectionId: path.lastSegment() }];\r\n    }\r\n    const where = toFilters(target.filters);\r\n    if (where) {\r\n        result.structuredQuery.where = where;\r\n    }\r\n    const orderBy = toOrder(target.orderBy);\r\n    if (orderBy) {\r\n        result.structuredQuery.orderBy = orderBy;\r\n    }\r\n    const limit = toInt32Proto(serializer, target.limit);\r\n    if (limit !== null) {\r\n        result.structuredQuery.limit = limit;\r\n    }\r\n    if (target.startAt) {\r\n        result.structuredQuery.startAt = toStartAtCursor(target.startAt);\r\n    }\r\n    if (target.endAt) {\r\n        result.structuredQuery.endAt = toEndAtCursor(target.endAt);\r\n    }\r\n    return result;\r\n}\r\nfunction toRunAggregationQueryRequest(serializer, target, aggregates) {\r\n    const queryTarget = toQueryTarget(serializer, target);\r\n    const aliasMap = {};\r\n    const aggregations = [];\r\n    let aggregationNum = 0;\r\n    aggregates.forEach(aggregate => {\r\n        // Map all client-side aliases to a unique short-form\r\n        // alias. This avoids issues with client-side aliases that\r\n        // exceed the 1500-byte string size limit.\r\n        const serverAlias = `aggregate_${aggregationNum++}`;\r\n        aliasMap[serverAlias] = aggregate.alias;\r\n        if (aggregate.aggregateType === 'count') {\r\n            aggregations.push({\r\n                alias: serverAlias,\r\n                count: {}\r\n            });\r\n        }\r\n        else if (aggregate.aggregateType === 'avg') {\r\n            aggregations.push({\r\n                alias: serverAlias,\r\n                avg: {\r\n                    field: toFieldPathReference(aggregate.fieldPath)\r\n                }\r\n            });\r\n        }\r\n        else if (aggregate.aggregateType === 'sum') {\r\n            aggregations.push({\r\n                alias: serverAlias,\r\n                sum: {\r\n                    field: toFieldPathReference(aggregate.fieldPath)\r\n                }\r\n            });\r\n        }\r\n    });\r\n    return {\r\n        request: {\r\n            structuredAggregationQuery: {\r\n                aggregations,\r\n                structuredQuery: queryTarget.structuredQuery\r\n            },\r\n            parent: queryTarget.parent\r\n        },\r\n        aliasMap\r\n    };\r\n}\r\nfunction convertQueryTargetToQuery(target) {\r\n    let path = fromQueryPath(target.parent);\r\n    const query = target.structuredQuery;\r\n    const fromCount = query.from ? query.from.length : 0;\r\n    let collectionGroup = null;\r\n    if (fromCount > 0) {\r\n        hardAssert(fromCount === 1);\r\n        const from = query.from[0];\r\n        if (from.allDescendants) {\r\n            collectionGroup = from.collectionId;\r\n        }\r\n        else {\r\n            path = path.child(from.collectionId);\r\n        }\r\n    }\r\n    let filterBy = [];\r\n    if (query.where) {\r\n        filterBy = fromFilters(query.where);\r\n    }\r\n    let orderBy = [];\r\n    if (query.orderBy) {\r\n        orderBy = fromOrder(query.orderBy);\r\n    }\r\n    let limit = null;\r\n    if (query.limit) {\r\n        limit = fromInt32Proto(query.limit);\r\n    }\r\n    let startAt = null;\r\n    if (query.startAt) {\r\n        startAt = fromStartAtCursor(query.startAt);\r\n    }\r\n    let endAt = null;\r\n    if (query.endAt) {\r\n        endAt = fromEndAtCursor(query.endAt);\r\n    }\r\n    return newQuery(path, collectionGroup, orderBy, filterBy, limit, \"F\" /* LimitType.First */, startAt, endAt);\r\n}\r\nfunction fromQueryTarget(target) {\r\n    return queryToTarget(convertQueryTargetToQuery(target));\r\n}\r\nfunction toListenRequestLabels(serializer, targetData) {\r\n    const value = toLabel(targetData.purpose);\r\n    if (value == null) {\r\n        return null;\r\n    }\r\n    else {\r\n        return {\r\n            'goog-listen-tags': value\r\n        };\r\n    }\r\n}\r\nfunction toLabel(purpose) {\r\n    switch (purpose) {\r\n        case \"TargetPurposeListen\" /* TargetPurpose.Listen */:\r\n            return null;\r\n        case \"TargetPurposeExistenceFilterMismatch\" /* TargetPurpose.ExistenceFilterMismatch */:\r\n            return 'existence-filter-mismatch';\r\n        case \"TargetPurposeExistenceFilterMismatchBloom\" /* TargetPurpose.ExistenceFilterMismatchBloom */:\r\n            return 'existence-filter-mismatch-bloom';\r\n        case \"TargetPurposeLimboResolution\" /* TargetPurpose.LimboResolution */:\r\n            return 'limbo-document';\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction toTarget(serializer, targetData) {\r\n    let result;\r\n    const target = targetData.target;\r\n    if (targetIsDocumentTarget(target)) {\r\n        result = { documents: toDocumentsTarget(serializer, target) };\r\n    }\r\n    else {\r\n        result = { query: toQueryTarget(serializer, target) };\r\n    }\r\n    result.targetId = targetData.targetId;\r\n    if (targetData.resumeToken.approximateByteSize() > 0) {\r\n        result.resumeToken = toBytes(serializer, targetData.resumeToken);\r\n        const expectedCount = toInt32Proto(serializer, targetData.expectedCount);\r\n        if (expectedCount !== null) {\r\n            result.expectedCount = expectedCount;\r\n        }\r\n    }\r\n    else if (targetData.snapshotVersion.compareTo(SnapshotVersion.min()) > 0) {\r\n        // TODO(wuandy): Consider removing above check because it is most likely true.\r\n        // Right now, many tests depend on this behaviour though (leaving min() out\r\n        // of serialization).\r\n        result.readTime = toTimestamp(serializer, targetData.snapshotVersion.toTimestamp());\r\n        const expectedCount = toInt32Proto(serializer, targetData.expectedCount);\r\n        if (expectedCount !== null) {\r\n            result.expectedCount = expectedCount;\r\n        }\r\n    }\r\n    return result;\r\n}\r\nfunction toFilters(filters) {\r\n    if (filters.length === 0) {\r\n        return;\r\n    }\r\n    return toFilter(CompositeFilter.create(filters, \"and\" /* CompositeOperator.AND */));\r\n}\r\nfunction fromFilters(filter) {\r\n    const result = fromFilter(filter);\r\n    if (result instanceof CompositeFilter &&\r\n        compositeFilterIsFlatConjunction(result)) {\r\n        return result.getFilters();\r\n    }\r\n    return [result];\r\n}\r\nfunction fromFilter(filter) {\r\n    if (filter.unaryFilter !== undefined) {\r\n        return fromUnaryFilter(filter);\r\n    }\r\n    else if (filter.fieldFilter !== undefined) {\r\n        return fromFieldFilter(filter);\r\n    }\r\n    else if (filter.compositeFilter !== undefined) {\r\n        return fromCompositeFilter(filter);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction toOrder(orderBys) {\r\n    if (orderBys.length === 0) {\r\n        return;\r\n    }\r\n    return orderBys.map(order => toPropertyOrder(order));\r\n}\r\nfunction fromOrder(orderBys) {\r\n    return orderBys.map(order => fromPropertyOrder(order));\r\n}\r\nfunction toStartAtCursor(cursor) {\r\n    return {\r\n        before: cursor.inclusive,\r\n        values: cursor.position\r\n    };\r\n}\r\nfunction toEndAtCursor(cursor) {\r\n    return {\r\n        before: !cursor.inclusive,\r\n        values: cursor.position\r\n    };\r\n}\r\nfunction fromStartAtCursor(cursor) {\r\n    const inclusive = !!cursor.before;\r\n    const position = cursor.values || [];\r\n    return new Bound(position, inclusive);\r\n}\r\nfunction fromEndAtCursor(cursor) {\r\n    const inclusive = !cursor.before;\r\n    const position = cursor.values || [];\r\n    return new Bound(position, inclusive);\r\n}\r\n// visible for testing\r\nfunction toDirection(dir) {\r\n    return DIRECTIONS[dir];\r\n}\r\n// visible for testing\r\nfunction fromDirection(dir) {\r\n    switch (dir) {\r\n        case 'ASCENDING':\r\n            return \"asc\" /* Direction.ASCENDING */;\r\n        case 'DESCENDING':\r\n            return \"desc\" /* Direction.DESCENDING */;\r\n        default:\r\n            return undefined;\r\n    }\r\n}\r\n// visible for testing\r\nfunction toOperatorName(op) {\r\n    return OPERATORS[op];\r\n}\r\nfunction toCompositeOperatorName(op) {\r\n    return COMPOSITE_OPERATORS[op];\r\n}\r\nfunction fromOperatorName(op) {\r\n    switch (op) {\r\n        case 'EQUAL':\r\n            return \"==\" /* Operator.EQUAL */;\r\n        case 'NOT_EQUAL':\r\n            return \"!=\" /* Operator.NOT_EQUAL */;\r\n        case 'GREATER_THAN':\r\n            return \">\" /* Operator.GREATER_THAN */;\r\n        case 'GREATER_THAN_OR_EQUAL':\r\n            return \">=\" /* Operator.GREATER_THAN_OR_EQUAL */;\r\n        case 'LESS_THAN':\r\n            return \"<\" /* Operator.LESS_THAN */;\r\n        case 'LESS_THAN_OR_EQUAL':\r\n            return \"<=\" /* Operator.LESS_THAN_OR_EQUAL */;\r\n        case 'ARRAY_CONTAINS':\r\n            return \"array-contains\" /* Operator.ARRAY_CONTAINS */;\r\n        case 'IN':\r\n            return \"in\" /* Operator.IN */;\r\n        case 'NOT_IN':\r\n            return \"not-in\" /* Operator.NOT_IN */;\r\n        case 'ARRAY_CONTAINS_ANY':\r\n            return \"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */;\r\n        case 'OPERATOR_UNSPECIFIED':\r\n            return fail();\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction fromCompositeOperatorName(op) {\r\n    switch (op) {\r\n        case 'AND':\r\n            return \"and\" /* CompositeOperator.AND */;\r\n        case 'OR':\r\n            return \"or\" /* CompositeOperator.OR */;\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction toFieldPathReference(path) {\r\n    return { fieldPath: path.canonicalString() };\r\n}\r\nfunction fromFieldPathReference(fieldReference) {\r\n    return FieldPath$1.fromServerFormat(fieldReference.fieldPath);\r\n}\r\n// visible for testing\r\nfunction toPropertyOrder(orderBy) {\r\n    return {\r\n        field: toFieldPathReference(orderBy.field),\r\n        direction: toDirection(orderBy.dir)\r\n    };\r\n}\r\nfunction fromPropertyOrder(orderBy) {\r\n    return new OrderBy(fromFieldPathReference(orderBy.field), fromDirection(orderBy.direction));\r\n}\r\n// visible for testing\r\nfunction toFilter(filter) {\r\n    if (filter instanceof FieldFilter) {\r\n        return toUnaryOrFieldFilter(filter);\r\n    }\r\n    else if (filter instanceof CompositeFilter) {\r\n        return toCompositeFilter(filter);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n}\r\nfunction toCompositeFilter(filter) {\r\n    const protos = filter.getFilters().map(filter => toFilter(filter));\r\n    if (protos.length === 1) {\r\n        return protos[0];\r\n    }\r\n    return {\r\n        compositeFilter: {\r\n            op: toCompositeOperatorName(filter.op),\r\n            filters: protos\r\n        }\r\n    };\r\n}\r\nfunction toUnaryOrFieldFilter(filter) {\r\n    if (filter.op === \"==\" /* Operator.EQUAL */) {\r\n        if (isNanValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NAN'\r\n                }\r\n            };\r\n        }\r\n        else if (isNullValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NULL'\r\n                }\r\n            };\r\n        }\r\n    }\r\n    else if (filter.op === \"!=\" /* Operator.NOT_EQUAL */) {\r\n        if (isNanValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NOT_NAN'\r\n                }\r\n            };\r\n        }\r\n        else if (isNullValue(filter.value)) {\r\n            return {\r\n                unaryFilter: {\r\n                    field: toFieldPathReference(filter.field),\r\n                    op: 'IS_NOT_NULL'\r\n                }\r\n            };\r\n        }\r\n    }\r\n    return {\r\n        fieldFilter: {\r\n            field: toFieldPathReference(filter.field),\r\n            op: toOperatorName(filter.op),\r\n            value: filter.value\r\n        }\r\n    };\r\n}\r\nfunction fromUnaryFilter(filter) {\r\n    switch (filter.unaryFilter.op) {\r\n        case 'IS_NAN':\r\n            const nanField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(nanField, \"==\" /* Operator.EQUAL */, {\r\n                doubleValue: NaN\r\n            });\r\n        case 'IS_NULL':\r\n            const nullField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(nullField, \"==\" /* Operator.EQUAL */, {\r\n                nullValue: 'NULL_VALUE'\r\n            });\r\n        case 'IS_NOT_NAN':\r\n            const notNanField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(notNanField, \"!=\" /* Operator.NOT_EQUAL */, {\r\n                doubleValue: NaN\r\n            });\r\n        case 'IS_NOT_NULL':\r\n            const notNullField = fromFieldPathReference(filter.unaryFilter.field);\r\n            return FieldFilter.create(notNullField, \"!=\" /* Operator.NOT_EQUAL */, {\r\n                nullValue: 'NULL_VALUE'\r\n            });\r\n        case 'OPERATOR_UNSPECIFIED':\r\n            return fail();\r\n        default:\r\n            return fail();\r\n    }\r\n}\r\nfunction fromFieldFilter(filter) {\r\n    return FieldFilter.create(fromFieldPathReference(filter.fieldFilter.field), fromOperatorName(filter.fieldFilter.op), filter.fieldFilter.value);\r\n}\r\nfunction fromCompositeFilter(filter) {\r\n    return CompositeFilter.create(filter.compositeFilter.filters.map(filter => fromFilter(filter)), fromCompositeOperatorName(filter.compositeFilter.op));\r\n}\r\nfunction toDocumentMask(fieldMask) {\r\n    const canonicalFields = [];\r\n    fieldMask.fields.forEach(field => canonicalFields.push(field.canonicalString()));\r\n    return {\r\n        fieldPaths: canonicalFields\r\n    };\r\n}\r\nfunction fromDocumentMask(proto) {\r\n    const paths = proto.fieldPaths || [];\r\n    return new FieldMask(paths.map(path => FieldPath$1.fromServerFormat(path)));\r\n}\r\nfunction isValidResourceName(path) {\r\n    // Resource names have at least 4 components (project ID, database ID)\r\n    return (path.length >= 4 &&\r\n        path.get(0) === 'projects' &&\r\n        path.get(2) === 'databases');\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An immutable set of metadata that the local store tracks for each target.\r\n */\r\nclass TargetData {\r\n    constructor(\r\n    /** The target being listened to. */\r\n    target, \r\n    /**\r\n     * The target ID to which the target corresponds; Assigned by the\r\n     * LocalStore for user listens and by the SyncEngine for limbo watches.\r\n     */\r\n    targetId, \r\n    /** The purpose of the target. */\r\n    purpose, \r\n    /**\r\n     * The sequence number of the last transaction during which this target data\r\n     * was modified.\r\n     */\r\n    sequenceNumber, \r\n    /** The latest snapshot version seen for this target. */\r\n    snapshotVersion = SnapshotVersion.min(), \r\n    /**\r\n     * The maximum snapshot version at which the associated view\r\n     * contained no limbo documents.\r\n     */\r\n    lastLimboFreeSnapshotVersion = SnapshotVersion.min(), \r\n    /**\r\n     * An opaque, server-assigned token that allows watching a target to be\r\n     * resumed after disconnecting without retransmitting all the data that\r\n     * matches the target. The resume token essentially identifies a point in\r\n     * time from which the server should resume sending results.\r\n     */\r\n    resumeToken = ByteString.EMPTY_BYTE_STRING, \r\n    /**\r\n     * The number of documents that last matched the query at the resume token or\r\n     * read time. Documents are counted only when making a listen request with\r\n     * resume token or read time, otherwise, keep it null.\r\n     */\r\n    expectedCount = null) {\r\n        this.target = target;\r\n        this.targetId = targetId;\r\n        this.purpose = purpose;\r\n        this.sequenceNumber = sequenceNumber;\r\n        this.snapshotVersion = snapshotVersion;\r\n        this.lastLimboFreeSnapshotVersion = lastLimboFreeSnapshotVersion;\r\n        this.resumeToken = resumeToken;\r\n        this.expectedCount = expectedCount;\r\n    }\r\n    /** Creates a new target data instance with an updated sequence number. */\r\n    withSequenceNumber(sequenceNumber) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, sequenceNumber, this.snapshotVersion, this.lastLimboFreeSnapshotVersion, this.resumeToken, this.expectedCount);\r\n    }\r\n    /**\r\n     * Creates a new target data instance with an updated resume token and\r\n     * snapshot version.\r\n     */\r\n    withResumeToken(resumeToken, snapshotVersion) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, snapshotVersion, this.lastLimboFreeSnapshotVersion, resumeToken, \r\n        /* expectedCount= */ null);\r\n    }\r\n    /**\r\n     * Creates a new target data instance with an updated expected count.\r\n     */\r\n    withExpectedCount(expectedCount) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, this.snapshotVersion, this.lastLimboFreeSnapshotVersion, this.resumeToken, expectedCount);\r\n    }\r\n    /**\r\n     * Creates a new target data instance with an updated last limbo free\r\n     * snapshot version number.\r\n     */\r\n    withLastLimboFreeSnapshotVersion(lastLimboFreeSnapshotVersion) {\r\n        return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, this.snapshotVersion, lastLimboFreeSnapshotVersion, this.resumeToken, this.expectedCount);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Serializer for values stored in the LocalStore. */\r\nclass LocalSerializer {\r\n    constructor(remoteSerializer) {\r\n        this.remoteSerializer = remoteSerializer;\r\n    }\r\n}\r\n/** Decodes a remote document from storage locally to a Document. */\r\nfunction fromDbRemoteDocument(localSerializer, remoteDoc) {\r\n    let doc;\r\n    if (remoteDoc.document) {\r\n        doc = fromDocument(localSerializer.remoteSerializer, remoteDoc.document, !!remoteDoc.hasCommittedMutations);\r\n    }\r\n    else if (remoteDoc.noDocument) {\r\n        const key = DocumentKey.fromSegments(remoteDoc.noDocument.path);\r\n        const version = fromDbTimestamp(remoteDoc.noDocument.readTime);\r\n        doc = MutableDocument.newNoDocument(key, version);\r\n        if (remoteDoc.hasCommittedMutations) {\r\n            doc.setHasCommittedMutations();\r\n        }\r\n    }\r\n    else if (remoteDoc.unknownDocument) {\r\n        const key = DocumentKey.fromSegments(remoteDoc.unknownDocument.path);\r\n        const version = fromDbTimestamp(remoteDoc.unknownDocument.version);\r\n        doc = MutableDocument.newUnknownDocument(key, version);\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    if (remoteDoc.readTime) {\r\n        doc.setReadTime(fromDbTimestampKey(remoteDoc.readTime));\r\n    }\r\n    return doc;\r\n}\r\n/** Encodes a document for storage locally. */\r\nfunction toDbRemoteDocument(localSerializer, document) {\r\n    const key = document.key;\r\n    const remoteDoc = {\r\n        prefixPath: key.getCollectionPath().popLast().toArray(),\r\n        collectionGroup: key.collectionGroup,\r\n        documentId: key.path.lastSegment(),\r\n        readTime: toDbTimestampKey(document.readTime),\r\n        hasCommittedMutations: document.hasCommittedMutations\r\n    };\r\n    if (document.isFoundDocument()) {\r\n        remoteDoc.document = toDocument(localSerializer.remoteSerializer, document);\r\n    }\r\n    else if (document.isNoDocument()) {\r\n        remoteDoc.noDocument = {\r\n            path: key.path.toArray(),\r\n            readTime: toDbTimestamp(document.version)\r\n        };\r\n    }\r\n    else if (document.isUnknownDocument()) {\r\n        remoteDoc.unknownDocument = {\r\n            path: key.path.toArray(),\r\n            version: toDbTimestamp(document.version)\r\n        };\r\n    }\r\n    else {\r\n        return fail();\r\n    }\r\n    return remoteDoc;\r\n}\r\nfunction toDbTimestampKey(snapshotVersion) {\r\n    const timestamp = snapshotVersion.toTimestamp();\r\n    return [timestamp.seconds, timestamp.nanoseconds];\r\n}\r\nfunction fromDbTimestampKey(dbTimestampKey) {\r\n    const timestamp = new Timestamp(dbTimestampKey[0], dbTimestampKey[1]);\r\n    return SnapshotVersion.fromTimestamp(timestamp);\r\n}\r\nfunction toDbTimestamp(snapshotVersion) {\r\n    const timestamp = snapshotVersion.toTimestamp();\r\n    return { seconds: timestamp.seconds, nanoseconds: timestamp.nanoseconds };\r\n}\r\nfunction fromDbTimestamp(dbTimestamp) {\r\n    const timestamp = new Timestamp(dbTimestamp.seconds, dbTimestamp.nanoseconds);\r\n    return SnapshotVersion.fromTimestamp(timestamp);\r\n}\r\n/** Encodes a batch of mutations into a DbMutationBatch for local storage. */\r\nfunction toDbMutationBatch(localSerializer, userId, batch) {\r\n    const serializedBaseMutations = batch.baseMutations.map(m => toMutation(localSerializer.remoteSerializer, m));\r\n    const serializedMutations = batch.mutations.map(m => toMutation(localSerializer.remoteSerializer, m));\r\n    return {\r\n        userId,\r\n        batchId: batch.batchId,\r\n        localWriteTimeMs: batch.localWriteTime.toMillis(),\r\n        baseMutations: serializedBaseMutations,\r\n        mutations: serializedMutations\r\n    };\r\n}\r\n/** Decodes a DbMutationBatch into a MutationBatch */\r\nfunction fromDbMutationBatch(localSerializer, dbBatch) {\r\n    const baseMutations = (dbBatch.baseMutations || []).map(m => fromMutation(localSerializer.remoteSerializer, m));\r\n    // Squash old transform mutations into existing patch or set mutations.\r\n    // The replacement of representing `transforms` with `update_transforms`\r\n    // on the SDK means that old `transform` mutations stored in IndexedDB need\r\n    // to be updated to `update_transforms`.\r\n    // TODO(b/174608374): Remove this code once we perform a schema migration.\r\n    for (let i = 0; i < dbBatch.mutations.length - 1; ++i) {\r\n        const currentMutation = dbBatch.mutations[i];\r\n        const hasTransform = i + 1 < dbBatch.mutations.length &&\r\n            dbBatch.mutations[i + 1].transform !== undefined;\r\n        if (hasTransform) {\r\n            const transformMutation = dbBatch.mutations[i + 1];\r\n            currentMutation.updateTransforms =\r\n                transformMutation.transform.fieldTransforms;\r\n            dbBatch.mutations.splice(i + 1, 1);\r\n            ++i;\r\n        }\r\n    }\r\n    const mutations = dbBatch.mutations.map(m => fromMutation(localSerializer.remoteSerializer, m));\r\n    const timestamp = Timestamp.fromMillis(dbBatch.localWriteTimeMs);\r\n    return new MutationBatch(dbBatch.batchId, timestamp, baseMutations, mutations);\r\n}\r\n/** Decodes a DbTarget into TargetData */\r\nfunction fromDbTarget(dbTarget) {\r\n    const version = fromDbTimestamp(dbTarget.readTime);\r\n    const lastLimboFreeSnapshotVersion = dbTarget.lastLimboFreeSnapshotVersion !== undefined\r\n        ? fromDbTimestamp(dbTarget.lastLimboFreeSnapshotVersion)\r\n        : SnapshotVersion.min();\r\n    let target;\r\n    if (isDocumentQuery(dbTarget.query)) {\r\n        target = fromDocumentsTarget(dbTarget.query);\r\n    }\r\n    else {\r\n        target = fromQueryTarget(dbTarget.query);\r\n    }\r\n    return new TargetData(target, dbTarget.targetId, \"TargetPurposeListen\" /* TargetPurpose.Listen */, dbTarget.lastListenSequenceNumber, version, lastLimboFreeSnapshotVersion, ByteString.fromBase64String(dbTarget.resumeToken));\r\n}\r\n/** Encodes TargetData into a DbTarget for storage locally. */\r\nfunction toDbTarget(localSerializer, targetData) {\r\n    const dbTimestamp = toDbTimestamp(targetData.snapshotVersion);\r\n    const dbLastLimboFreeTimestamp = toDbTimestamp(targetData.lastLimboFreeSnapshotVersion);\r\n    let queryProto;\r\n    if (targetIsDocumentTarget(targetData.target)) {\r\n        queryProto = toDocumentsTarget(localSerializer.remoteSerializer, targetData.target);\r\n    }\r\n    else {\r\n        queryProto = toQueryTarget(localSerializer.remoteSerializer, targetData.target);\r\n    }\r\n    // We can't store the resumeToken as a ByteString in IndexedDb, so we\r\n    // convert it to a base64 string for storage.\r\n    const resumeToken = targetData.resumeToken.toBase64();\r\n    // lastListenSequenceNumber is always 0 until we do real GC.\r\n    return {\r\n        targetId: targetData.targetId,\r\n        canonicalId: canonifyTarget(targetData.target),\r\n        readTime: dbTimestamp,\r\n        resumeToken,\r\n        lastListenSequenceNumber: targetData.sequenceNumber,\r\n        lastLimboFreeSnapshotVersion: dbLastLimboFreeTimestamp,\r\n        query: queryProto\r\n    };\r\n}\r\n/**\r\n * A helper function for figuring out what kind of query has been stored.\r\n */\r\nfunction isDocumentQuery(dbQuery) {\r\n    return dbQuery.documents !== undefined;\r\n}\r\n/** Encodes a DbBundle to a BundleMetadata object. */\r\nfunction fromDbBundle(dbBundle) {\r\n    return {\r\n        id: dbBundle.bundleId,\r\n        createTime: fromDbTimestamp(dbBundle.createTime),\r\n        version: dbBundle.version\r\n    };\r\n}\r\n/** Encodes a BundleMetadata to a DbBundle. */\r\nfunction toDbBundle(metadata) {\r\n    return {\r\n        bundleId: metadata.id,\r\n        createTime: toDbTimestamp(fromVersion(metadata.createTime)),\r\n        version: metadata.version\r\n    };\r\n}\r\n/** Encodes a DbNamedQuery to a NamedQuery. */\r\nfunction fromDbNamedQuery(dbNamedQuery) {\r\n    return {\r\n        name: dbNamedQuery.name,\r\n        query: fromBundledQuery(dbNamedQuery.bundledQuery),\r\n        readTime: fromDbTimestamp(dbNamedQuery.readTime)\r\n    };\r\n}\r\n/** Encodes a NamedQuery from a bundle proto to a DbNamedQuery. */\r\nfunction toDbNamedQuery(query) {\r\n    return {\r\n        name: query.name,\r\n        readTime: toDbTimestamp(fromVersion(query.readTime)),\r\n        bundledQuery: query.bundledQuery\r\n    };\r\n}\r\n/**\r\n * Encodes a `BundledQuery` from bundle proto to a Query object.\r\n *\r\n * This reconstructs the original query used to build the bundle being loaded,\r\n * including features exists only in SDKs (for example: limit-to-last).\r\n */\r\nfunction fromBundledQuery(bundledQuery) {\r\n    const query = convertQueryTargetToQuery({\r\n        parent: bundledQuery.parent,\r\n        structuredQuery: bundledQuery.structuredQuery\r\n    });\r\n    if (bundledQuery.limitType === 'LAST') {\r\n        return queryWithLimit(query, query.limit, \"L\" /* LimitType.Last */);\r\n    }\r\n    return query;\r\n}\r\n/** Encodes a NamedQuery proto object to a NamedQuery model object. */\r\nfunction fromProtoNamedQuery(namedQuery) {\r\n    return {\r\n        name: namedQuery.name,\r\n        query: fromBundledQuery(namedQuery.bundledQuery),\r\n        readTime: fromVersion(namedQuery.readTime)\r\n    };\r\n}\r\n/** Decodes a BundleMetadata proto into a BundleMetadata object. */\r\nfunction fromBundleMetadata(metadata) {\r\n    return {\r\n        id: metadata.id,\r\n        version: metadata.version,\r\n        createTime: fromVersion(metadata.createTime)\r\n    };\r\n}\r\n/** Encodes a DbDocumentOverlay object to an Overlay model object. */\r\nfunction fromDbDocumentOverlay(localSerializer, dbDocumentOverlay) {\r\n    return new Overlay(dbDocumentOverlay.largestBatchId, fromMutation(localSerializer.remoteSerializer, dbDocumentOverlay.overlayMutation));\r\n}\r\n/** Decodes an Overlay model object into a DbDocumentOverlay object. */\r\nfunction toDbDocumentOverlay(localSerializer, userId, overlay) {\r\n    const [_, collectionPath, documentId] = toDbDocumentOverlayKey(userId, overlay.mutation.key);\r\n    return {\r\n        userId,\r\n        collectionPath,\r\n        documentId,\r\n        collectionGroup: overlay.mutation.key.getCollectionGroup(),\r\n        largestBatchId: overlay.largestBatchId,\r\n        overlayMutation: toMutation(localSerializer.remoteSerializer, overlay.mutation)\r\n    };\r\n}\r\n/**\r\n * Returns the DbDocumentOverlayKey corresponding to the given user and\r\n * document key.\r\n */\r\nfunction toDbDocumentOverlayKey(userId, docKey) {\r\n    const docId = docKey.path.lastSegment();\r\n    const collectionPath = encodeResourcePath(docKey.path.popLast());\r\n    return [userId, collectionPath, docId];\r\n}\r\nfunction toDbIndexConfiguration(index) {\r\n    return {\r\n        indexId: index.indexId,\r\n        collectionGroup: index.collectionGroup,\r\n        fields: index.fields.map(s => [s.fieldPath.canonicalString(), s.kind])\r\n    };\r\n}\r\nfunction fromDbIndexConfiguration(index, state) {\r\n    const decodedState = state\r\n        ? new IndexState(state.sequenceNumber, new IndexOffset(fromDbTimestamp(state.readTime), new DocumentKey(decodeResourcePath(state.documentKey)), state.largestBatchId))\r\n        : IndexState.empty();\r\n    const decodedSegments = index.fields.map(([fieldPath, kind]) => new IndexSegment(FieldPath$1.fromServerFormat(fieldPath), kind));\r\n    return new FieldIndex(index.indexId, index.collectionGroup, decodedSegments, decodedState);\r\n}\r\nfunction toDbIndexState(indexId, user, sequenceNumber, offset) {\r\n    return {\r\n        indexId,\r\n        uid: user.uid || '',\r\n        sequenceNumber,\r\n        readTime: toDbTimestamp(offset.readTime),\r\n        documentKey: encodeResourcePath(offset.documentKey.path),\r\n        largestBatchId: offset.largestBatchId\r\n    };\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass IndexedDbBundleCache {\r\n    getBundleMetadata(transaction, bundleId) {\r\n        return bundlesStore(transaction)\r\n            .get(bundleId)\r\n            .next(bundle => {\r\n            if (bundle) {\r\n                return fromDbBundle(bundle);\r\n            }\r\n            return undefined;\r\n        });\r\n    }\r\n    saveBundleMetadata(transaction, bundleMetadata) {\r\n        return bundlesStore(transaction).put(toDbBundle(bundleMetadata));\r\n    }\r\n    getNamedQuery(transaction, queryName) {\r\n        return namedQueriesStore(transaction)\r\n            .get(queryName)\r\n            .next(query => {\r\n            if (query) {\r\n                return fromDbNamedQuery(query);\r\n            }\r\n            return undefined;\r\n        });\r\n    }\r\n    saveNamedQuery(transaction, query) {\r\n        return namedQueriesStore(transaction).put(toDbNamedQuery(query));\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the bundles object store.\r\n */\r\nfunction bundlesStore(txn) {\r\n    return getStore(txn, DbBundleStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the namedQueries object store.\r\n */\r\nfunction namedQueriesStore(txn) {\r\n    return getStore(txn, DbNamedQueryStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Implementation of DocumentOverlayCache using IndexedDb.\r\n */\r\nclass IndexedDbDocumentOverlayCache {\r\n    /**\r\n     * @param serializer - The document serializer.\r\n     * @param userId - The userId for which we are accessing overlays.\r\n     */\r\n    constructor(serializer, userId) {\r\n        this.serializer = serializer;\r\n        this.userId = userId;\r\n    }\r\n    static forUser(serializer, user) {\r\n        const userId = user.uid || '';\r\n        return new IndexedDbDocumentOverlayCache(serializer, userId);\r\n    }\r\n    getOverlay(transaction, key) {\r\n        return documentOverlayStore(transaction)\r\n            .get(toDbDocumentOverlayKey(this.userId, key))\r\n            .next(dbOverlay => {\r\n            if (dbOverlay) {\r\n                return fromDbDocumentOverlay(this.serializer, dbOverlay);\r\n            }\r\n            return null;\r\n        });\r\n    }\r\n    getOverlays(transaction, keys) {\r\n        const result = newOverlayMap();\r\n        return PersistencePromise.forEach(keys, (key) => {\r\n            return this.getOverlay(transaction, key).next(overlay => {\r\n                if (overlay !== null) {\r\n                    result.set(key, overlay);\r\n                }\r\n            });\r\n        }).next(() => result);\r\n    }\r\n    saveOverlays(transaction, largestBatchId, overlays) {\r\n        const promises = [];\r\n        overlays.forEach((_, mutation) => {\r\n            const overlay = new Overlay(largestBatchId, mutation);\r\n            promises.push(this.saveOverlay(transaction, overlay));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    removeOverlaysForBatchId(transaction, documentKeys, batchId) {\r\n        const collectionPaths = new Set();\r\n        // Get the set of unique collection paths.\r\n        documentKeys.forEach(key => collectionPaths.add(encodeResourcePath(key.getCollectionPath())));\r\n        const promises = [];\r\n        collectionPaths.forEach(collectionPath => {\r\n            const range = IDBKeyRange.bound([this.userId, collectionPath, batchId], [this.userId, collectionPath, batchId + 1], \r\n            /*lowerOpen=*/ false, \r\n            /*upperOpen=*/ true);\r\n            promises.push(documentOverlayStore(transaction).deleteAll(DbDocumentOverlayCollectionPathOverlayIndex, range));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getOverlaysForCollection(transaction, collection, sinceBatchId) {\r\n        const result = newOverlayMap();\r\n        const collectionPath = encodeResourcePath(collection);\r\n        // We want batch IDs larger than `sinceBatchId`, and so the lower bound\r\n        // is not inclusive.\r\n        const range = IDBKeyRange.bound([this.userId, collectionPath, sinceBatchId], [this.userId, collectionPath, Number.POSITIVE_INFINITY], \r\n        /*lowerOpen=*/ true);\r\n        return documentOverlayStore(transaction)\r\n            .loadAll(DbDocumentOverlayCollectionPathOverlayIndex, range)\r\n            .next(dbOverlays => {\r\n            for (const dbOverlay of dbOverlays) {\r\n                const overlay = fromDbDocumentOverlay(this.serializer, dbOverlay);\r\n                result.set(overlay.getKey(), overlay);\r\n            }\r\n            return result;\r\n        });\r\n    }\r\n    getOverlaysForCollectionGroup(transaction, collectionGroup, sinceBatchId, count) {\r\n        const result = newOverlayMap();\r\n        let currentBatchId = undefined;\r\n        // We want batch IDs larger than `sinceBatchId`, and so the lower bound\r\n        // is not inclusive.\r\n        const range = IDBKeyRange.bound([this.userId, collectionGroup, sinceBatchId], [this.userId, collectionGroup, Number.POSITIVE_INFINITY], \r\n        /*lowerOpen=*/ true);\r\n        return documentOverlayStore(transaction)\r\n            .iterate({\r\n            index: DbDocumentOverlayCollectionGroupOverlayIndex,\r\n            range\r\n        }, (_, dbOverlay, control) => {\r\n            // We do not want to return partial batch overlays, even if the size\r\n            // of the result set exceeds the given `count` argument. Therefore, we\r\n            // continue to aggregate results even after the result size exceeds\r\n            // `count` if there are more overlays from the `currentBatchId`.\r\n            const overlay = fromDbDocumentOverlay(this.serializer, dbOverlay);\r\n            if (result.size() < count ||\r\n                overlay.largestBatchId === currentBatchId) {\r\n                result.set(overlay.getKey(), overlay);\r\n                currentBatchId = overlay.largestBatchId;\r\n            }\r\n            else {\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => result);\r\n    }\r\n    saveOverlay(transaction, overlay) {\r\n        return documentOverlayStore(transaction).put(toDbDocumentOverlay(this.serializer, this.userId, overlay));\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the document overlay object store.\r\n */\r\nfunction documentOverlayStore(txn) {\r\n    return getStore(txn, DbDocumentOverlayStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n// Note: This code is copied from the backend. Code that is not used by\r\n// Firestore was removed.\r\nconst INDEX_TYPE_NULL = 5;\r\nconst INDEX_TYPE_BOOLEAN = 10;\r\nconst INDEX_TYPE_NAN = 13;\r\nconst INDEX_TYPE_NUMBER = 15;\r\nconst INDEX_TYPE_TIMESTAMP = 20;\r\nconst INDEX_TYPE_STRING = 25;\r\nconst INDEX_TYPE_BLOB = 30;\r\nconst INDEX_TYPE_REFERENCE = 37;\r\nconst INDEX_TYPE_GEOPOINT = 45;\r\nconst INDEX_TYPE_ARRAY = 50;\r\nconst INDEX_TYPE_MAP = 55;\r\nconst INDEX_TYPE_REFERENCE_SEGMENT = 60;\r\n// A terminator that indicates that a truncatable value was not truncated.\r\n// This must be smaller than all other type labels.\r\nconst NOT_TRUNCATED = 2;\r\n/** Firestore index value writer.  */\r\nclass FirestoreIndexValueWriter {\r\n    constructor() { }\r\n    // The write methods below short-circuit writing terminators for values\r\n    // containing a (terminating) truncated value.\r\n    //\r\n    // As an example, consider the resulting encoding for:\r\n    //\r\n    // [\"bar\", [2, \"foo\"]] -> (STRING, \"bar\", TERM, ARRAY, NUMBER, 2, STRING, \"foo\", TERM, TERM, TERM)\r\n    // [\"bar\", [2, truncated(\"foo\")]] -> (STRING, \"bar\", TERM, ARRAY, NUMBER, 2, STRING, \"foo\", TRUNC)\r\n    // [\"bar\", truncated([\"foo\"])] -> (STRING, \"bar\", TERM, ARRAY. STRING, \"foo\", TERM, TRUNC)\r\n    /** Writes an index value.  */\r\n    writeIndexValue(value, encoder) {\r\n        this.writeIndexValueAux(value, encoder);\r\n        // Write separator to split index values\r\n        // (see go/firestore-storage-format#encodings).\r\n        encoder.writeInfinity();\r\n    }\r\n    writeIndexValueAux(indexValue, encoder) {\r\n        if ('nullValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_NULL);\r\n        }\r\n        else if ('booleanValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_BOOLEAN);\r\n            encoder.writeNumber(indexValue.booleanValue ? 1 : 0);\r\n        }\r\n        else if ('integerValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_NUMBER);\r\n            encoder.writeNumber(normalizeNumber(indexValue.integerValue));\r\n        }\r\n        else if ('doubleValue' in indexValue) {\r\n            const n = normalizeNumber(indexValue.doubleValue);\r\n            if (isNaN(n)) {\r\n                this.writeValueTypeLabel(encoder, INDEX_TYPE_NAN);\r\n            }\r\n            else {\r\n                this.writeValueTypeLabel(encoder, INDEX_TYPE_NUMBER);\r\n                if (isNegativeZero(n)) {\r\n                    // -0.0, 0 and 0.0 are all considered the same\r\n                    encoder.writeNumber(0.0);\r\n                }\r\n                else {\r\n                    encoder.writeNumber(n);\r\n                }\r\n            }\r\n        }\r\n        else if ('timestampValue' in indexValue) {\r\n            const timestamp = indexValue.timestampValue;\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_TIMESTAMP);\r\n            if (typeof timestamp === 'string') {\r\n                encoder.writeString(timestamp);\r\n            }\r\n            else {\r\n                encoder.writeString(`${timestamp.seconds || ''}`);\r\n                encoder.writeNumber(timestamp.nanos || 0);\r\n            }\r\n        }\r\n        else if ('stringValue' in indexValue) {\r\n            this.writeIndexString(indexValue.stringValue, encoder);\r\n            this.writeTruncationMarker(encoder);\r\n        }\r\n        else if ('bytesValue' in indexValue) {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_BLOB);\r\n            encoder.writeBytes(normalizeByteString(indexValue.bytesValue));\r\n            this.writeTruncationMarker(encoder);\r\n        }\r\n        else if ('referenceValue' in indexValue) {\r\n            this.writeIndexEntityRef(indexValue.referenceValue, encoder);\r\n        }\r\n        else if ('geoPointValue' in indexValue) {\r\n            const geoPoint = indexValue.geoPointValue;\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_GEOPOINT);\r\n            encoder.writeNumber(geoPoint.latitude || 0);\r\n            encoder.writeNumber(geoPoint.longitude || 0);\r\n        }\r\n        else if ('mapValue' in indexValue) {\r\n            if (isMaxValue(indexValue)) {\r\n                this.writeValueTypeLabel(encoder, Number.MAX_SAFE_INTEGER);\r\n            }\r\n            else {\r\n                this.writeIndexMap(indexValue.mapValue, encoder);\r\n                this.writeTruncationMarker(encoder);\r\n            }\r\n        }\r\n        else if ('arrayValue' in indexValue) {\r\n            this.writeIndexArray(indexValue.arrayValue, encoder);\r\n            this.writeTruncationMarker(encoder);\r\n        }\r\n        else {\r\n            fail();\r\n        }\r\n    }\r\n    writeIndexString(stringIndexValue, encoder) {\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_STRING);\r\n        this.writeUnlabeledIndexString(stringIndexValue, encoder);\r\n    }\r\n    writeUnlabeledIndexString(stringIndexValue, encoder) {\r\n        encoder.writeString(stringIndexValue);\r\n    }\r\n    writeIndexMap(mapIndexValue, encoder) {\r\n        const map = mapIndexValue.fields || {};\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_MAP);\r\n        for (const key of Object.keys(map)) {\r\n            this.writeIndexString(key, encoder);\r\n            this.writeIndexValueAux(map[key], encoder);\r\n        }\r\n    }\r\n    writeIndexArray(arrayIndexValue, encoder) {\r\n        const values = arrayIndexValue.values || [];\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_ARRAY);\r\n        for (const element of values) {\r\n            this.writeIndexValueAux(element, encoder);\r\n        }\r\n    }\r\n    writeIndexEntityRef(referenceValue, encoder) {\r\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_REFERENCE);\r\n        const path = DocumentKey.fromName(referenceValue).path;\r\n        path.forEach(segment => {\r\n            this.writeValueTypeLabel(encoder, INDEX_TYPE_REFERENCE_SEGMENT);\r\n            this.writeUnlabeledIndexString(segment, encoder);\r\n        });\r\n    }\r\n    writeValueTypeLabel(encoder, typeOrder) {\r\n        encoder.writeNumber(typeOrder);\r\n    }\r\n    writeTruncationMarker(encoder) {\r\n        // While the SDK does not implement truncation, the truncation marker is\r\n        // used to terminate all variable length values (which are strings, bytes,\r\n        // references, arrays and maps).\r\n        encoder.writeNumber(NOT_TRUNCATED);\r\n    }\r\n}\r\nFirestoreIndexValueWriter.INSTANCE = new FirestoreIndexValueWriter();\n\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law | agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES | CONDITIONS OF ANY KIND, either express | implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** These constants are taken from the backend. */\r\nconst MIN_SURROGATE = '\\uD800';\r\nconst MAX_SURROGATE = '\\uDBFF';\r\nconst ESCAPE1 = 0x00;\r\nconst NULL_BYTE = 0xff; // Combined with ESCAPE1\r\nconst SEPARATOR = 0x01; // Combined with ESCAPE1\r\nconst ESCAPE2 = 0xff;\r\nconst INFINITY = 0xff; // Combined with ESCAPE2\r\nconst FF_BYTE = 0x00; // Combined with ESCAPE2\r\nconst LONG_SIZE = 64;\r\nconst BYTE_SIZE = 8;\r\n/**\r\n * The default size of the buffer. This is arbitrary, but likely larger than\r\n * most index values so that less copies of the underlying buffer will be made.\r\n * For large values, a single copy will made to double the buffer length.\r\n */\r\nconst DEFAULT_BUFFER_SIZE = 1024;\r\n/** Converts a JavaScript number to a byte array (using big endian encoding). */\r\nfunction doubleToLongBits(value) {\r\n    const dv = new DataView(new ArrayBuffer(8));\r\n    dv.setFloat64(0, value, /* littleEndian= */ false);\r\n    return new Uint8Array(dv.buffer);\r\n}\r\n/**\r\n * Counts the number of zeros in a byte.\r\n *\r\n * Visible for testing.\r\n */\r\nfunction numberOfLeadingZerosInByte(x) {\r\n    if (x === 0) {\r\n        return 8;\r\n    }\r\n    let zeros = 0;\r\n    if (x >> 4 === 0) {\r\n        // Test if the first four bits are zero.\r\n        zeros += 4;\r\n        x = x << 4;\r\n    }\r\n    if (x >> 6 === 0) {\r\n        // Test if the first two (or next two) bits are zero.\r\n        zeros += 2;\r\n        x = x << 2;\r\n    }\r\n    if (x >> 7 === 0) {\r\n        // Test if the remaining bit is zero.\r\n        zeros += 1;\r\n    }\r\n    return zeros;\r\n}\r\n/** Counts the number of leading zeros in the given byte array. */\r\nfunction numberOfLeadingZeros(bytes) {\r\n    let leadingZeros = 0;\r\n    for (let i = 0; i < 8; ++i) {\r\n        const zeros = numberOfLeadingZerosInByte(bytes[i] & 0xff);\r\n        leadingZeros += zeros;\r\n        if (zeros !== 8) {\r\n            break;\r\n        }\r\n    }\r\n    return leadingZeros;\r\n}\r\n/**\r\n * Returns the number of bytes required to store \"value\". Leading zero bytes\r\n * are skipped.\r\n */\r\nfunction unsignedNumLength(value) {\r\n    // This is just the number of bytes for the unsigned representation of the number.\r\n    const numBits = LONG_SIZE - numberOfLeadingZeros(value);\r\n    return Math.ceil(numBits / BYTE_SIZE);\r\n}\r\n/**\r\n * OrderedCodeWriter is a minimal-allocation implementation of the writing\r\n * behavior defined by the backend.\r\n *\r\n * The code is ported from its Java counterpart.\r\n */\r\nclass OrderedCodeWriter {\r\n    constructor() {\r\n        this.buffer = new Uint8Array(DEFAULT_BUFFER_SIZE);\r\n        this.position = 0;\r\n    }\r\n    writeBytesAscending(value) {\r\n        const it = value[Symbol.iterator]();\r\n        let byte = it.next();\r\n        while (!byte.done) {\r\n            this.writeByteAscending(byte.value);\r\n            byte = it.next();\r\n        }\r\n        this.writeSeparatorAscending();\r\n    }\r\n    writeBytesDescending(value) {\r\n        const it = value[Symbol.iterator]();\r\n        let byte = it.next();\r\n        while (!byte.done) {\r\n            this.writeByteDescending(byte.value);\r\n            byte = it.next();\r\n        }\r\n        this.writeSeparatorDescending();\r\n    }\r\n    /** Writes utf8 bytes into this byte sequence, ascending. */\r\n    writeUtf8Ascending(sequence) {\r\n        for (const c of sequence) {\r\n            const charCode = c.charCodeAt(0);\r\n            if (charCode < 0x80) {\r\n                this.writeByteAscending(charCode);\r\n            }\r\n            else if (charCode < 0x800) {\r\n                this.writeByteAscending((0x0f << 6) | (charCode >>> 6));\r\n                this.writeByteAscending(0x80 | (0x3f & charCode));\r\n            }\r\n            else if (c < MIN_SURROGATE || MAX_SURROGATE < c) {\r\n                this.writeByteAscending((0x0f << 5) | (charCode >>> 12));\r\n                this.writeByteAscending(0x80 | (0x3f & (charCode >>> 6)));\r\n                this.writeByteAscending(0x80 | (0x3f & charCode));\r\n            }\r\n            else {\r\n                const codePoint = c.codePointAt(0);\r\n                this.writeByteAscending((0x0f << 4) | (codePoint >>> 18));\r\n                this.writeByteAscending(0x80 | (0x3f & (codePoint >>> 12)));\r\n                this.writeByteAscending(0x80 | (0x3f & (codePoint >>> 6)));\r\n                this.writeByteAscending(0x80 | (0x3f & codePoint));\r\n            }\r\n        }\r\n        this.writeSeparatorAscending();\r\n    }\r\n    /** Writes utf8 bytes into this byte sequence, descending */\r\n    writeUtf8Descending(sequence) {\r\n        for (const c of sequence) {\r\n            const charCode = c.charCodeAt(0);\r\n            if (charCode < 0x80) {\r\n                this.writeByteDescending(charCode);\r\n            }\r\n            else if (charCode < 0x800) {\r\n                this.writeByteDescending((0x0f << 6) | (charCode >>> 6));\r\n                this.writeByteDescending(0x80 | (0x3f & charCode));\r\n            }\r\n            else if (c < MIN_SURROGATE || MAX_SURROGATE < c) {\r\n                this.writeByteDescending((0x0f << 5) | (charCode >>> 12));\r\n                this.writeByteDescending(0x80 | (0x3f & (charCode >>> 6)));\r\n                this.writeByteDescending(0x80 | (0x3f & charCode));\r\n            }\r\n            else {\r\n                const codePoint = c.codePointAt(0);\r\n                this.writeByteDescending((0x0f << 4) | (codePoint >>> 18));\r\n                this.writeByteDescending(0x80 | (0x3f & (codePoint >>> 12)));\r\n                this.writeByteDescending(0x80 | (0x3f & (codePoint >>> 6)));\r\n                this.writeByteDescending(0x80 | (0x3f & codePoint));\r\n            }\r\n        }\r\n        this.writeSeparatorDescending();\r\n    }\r\n    writeNumberAscending(val) {\r\n        // Values are encoded with a single byte length prefix, followed by the\r\n        // actual value in big-endian format with leading 0 bytes dropped.\r\n        const value = this.toOrderedBits(val);\r\n        const len = unsignedNumLength(value);\r\n        this.ensureAvailable(1 + len);\r\n        this.buffer[this.position++] = len & 0xff; // Write the length\r\n        for (let i = value.length - len; i < value.length; ++i) {\r\n            this.buffer[this.position++] = value[i] & 0xff;\r\n        }\r\n    }\r\n    writeNumberDescending(val) {\r\n        // Values are encoded with a single byte length prefix, followed by the\r\n        // inverted value in big-endian format with leading 0 bytes dropped.\r\n        const value = this.toOrderedBits(val);\r\n        const len = unsignedNumLength(value);\r\n        this.ensureAvailable(1 + len);\r\n        this.buffer[this.position++] = ~(len & 0xff); // Write the length\r\n        for (let i = value.length - len; i < value.length; ++i) {\r\n            this.buffer[this.position++] = ~(value[i] & 0xff);\r\n        }\r\n    }\r\n    /**\r\n     * Writes the \"infinity\" byte sequence that sorts after all other byte\r\n     * sequences written in ascending order.\r\n     */\r\n    writeInfinityAscending() {\r\n        this.writeEscapedByteAscending(ESCAPE2);\r\n        this.writeEscapedByteAscending(INFINITY);\r\n    }\r\n    /**\r\n     * Writes the \"infinity\" byte sequence that sorts before all other byte\r\n     * sequences written in descending order.\r\n     */\r\n    writeInfinityDescending() {\r\n        this.writeEscapedByteDescending(ESCAPE2);\r\n        this.writeEscapedByteDescending(INFINITY);\r\n    }\r\n    /**\r\n     * Resets the buffer such that it is the same as when it was newly\r\n     * constructed.\r\n     */\r\n    reset() {\r\n        this.position = 0;\r\n    }\r\n    seed(encodedBytes) {\r\n        this.ensureAvailable(encodedBytes.length);\r\n        this.buffer.set(encodedBytes, this.position);\r\n        this.position += encodedBytes.length;\r\n    }\r\n    /** Makes a copy of the encoded bytes in this buffer.  */\r\n    encodedBytes() {\r\n        return this.buffer.slice(0, this.position);\r\n    }\r\n    /**\r\n     * Encodes `val` into an encoding so that the order matches the IEEE 754\r\n     * floating-point comparison results with the following exceptions:\r\n     *   -0.0 < 0.0\r\n     *   all non-NaN < NaN\r\n     *   NaN = NaN\r\n     */\r\n    toOrderedBits(val) {\r\n        const value = doubleToLongBits(val);\r\n        // Check if the first bit is set. We use a bit mask since value[0] is\r\n        // encoded as a number from 0 to 255.\r\n        const isNegative = (value[0] & 0x80) !== 0;\r\n        // Revert the two complement to get natural ordering\r\n        value[0] ^= isNegative ? 0xff : 0x80;\r\n        for (let i = 1; i < value.length; ++i) {\r\n            value[i] ^= isNegative ? 0xff : 0x00;\r\n        }\r\n        return value;\r\n    }\r\n    /** Writes a single byte ascending to the buffer. */\r\n    writeByteAscending(b) {\r\n        const masked = b & 0xff;\r\n        if (masked === ESCAPE1) {\r\n            this.writeEscapedByteAscending(ESCAPE1);\r\n            this.writeEscapedByteAscending(NULL_BYTE);\r\n        }\r\n        else if (masked === ESCAPE2) {\r\n            this.writeEscapedByteAscending(ESCAPE2);\r\n            this.writeEscapedByteAscending(FF_BYTE);\r\n        }\r\n        else {\r\n            this.writeEscapedByteAscending(masked);\r\n        }\r\n    }\r\n    /** Writes a single byte descending to the buffer.  */\r\n    writeByteDescending(b) {\r\n        const masked = b & 0xff;\r\n        if (masked === ESCAPE1) {\r\n            this.writeEscapedByteDescending(ESCAPE1);\r\n            this.writeEscapedByteDescending(NULL_BYTE);\r\n        }\r\n        else if (masked === ESCAPE2) {\r\n            this.writeEscapedByteDescending(ESCAPE2);\r\n            this.writeEscapedByteDescending(FF_BYTE);\r\n        }\r\n        else {\r\n            this.writeEscapedByteDescending(b);\r\n        }\r\n    }\r\n    writeSeparatorAscending() {\r\n        this.writeEscapedByteAscending(ESCAPE1);\r\n        this.writeEscapedByteAscending(SEPARATOR);\r\n    }\r\n    writeSeparatorDescending() {\r\n        this.writeEscapedByteDescending(ESCAPE1);\r\n        this.writeEscapedByteDescending(SEPARATOR);\r\n    }\r\n    writeEscapedByteAscending(b) {\r\n        this.ensureAvailable(1);\r\n        this.buffer[this.position++] = b;\r\n    }\r\n    writeEscapedByteDescending(b) {\r\n        this.ensureAvailable(1);\r\n        this.buffer[this.position++] = ~b;\r\n    }\r\n    ensureAvailable(bytes) {\r\n        const minCapacity = bytes + this.position;\r\n        if (minCapacity <= this.buffer.length) {\r\n            return;\r\n        }\r\n        // Try doubling.\r\n        let newLength = this.buffer.length * 2;\r\n        // Still not big enough? Just allocate the right size.\r\n        if (newLength < minCapacity) {\r\n            newLength = minCapacity;\r\n        }\r\n        // Create the new buffer.\r\n        const newBuffer = new Uint8Array(newLength);\r\n        newBuffer.set(this.buffer); // copy old data\r\n        this.buffer = newBuffer;\r\n    }\r\n}\n\nclass AscendingIndexByteEncoder {\r\n    constructor(orderedCode) {\r\n        this.orderedCode = orderedCode;\r\n    }\r\n    writeBytes(value) {\r\n        this.orderedCode.writeBytesAscending(value);\r\n    }\r\n    writeString(value) {\r\n        this.orderedCode.writeUtf8Ascending(value);\r\n    }\r\n    writeNumber(value) {\r\n        this.orderedCode.writeNumberAscending(value);\r\n    }\r\n    writeInfinity() {\r\n        this.orderedCode.writeInfinityAscending();\r\n    }\r\n}\r\nclass DescendingIndexByteEncoder {\r\n    constructor(orderedCode) {\r\n        this.orderedCode = orderedCode;\r\n    }\r\n    writeBytes(value) {\r\n        this.orderedCode.writeBytesDescending(value);\r\n    }\r\n    writeString(value) {\r\n        this.orderedCode.writeUtf8Descending(value);\r\n    }\r\n    writeNumber(value) {\r\n        this.orderedCode.writeNumberDescending(value);\r\n    }\r\n    writeInfinity() {\r\n        this.orderedCode.writeInfinityDescending();\r\n    }\r\n}\r\n/**\r\n * Implements `DirectionalIndexByteEncoder` using `OrderedCodeWriter` for the\r\n * actual encoding.\r\n */\r\nclass IndexByteEncoder {\r\n    constructor() {\r\n        this.orderedCode = new OrderedCodeWriter();\r\n        this.ascending = new AscendingIndexByteEncoder(this.orderedCode);\r\n        this.descending = new DescendingIndexByteEncoder(this.orderedCode);\r\n    }\r\n    seed(encodedBytes) {\r\n        this.orderedCode.seed(encodedBytes);\r\n    }\r\n    forKind(kind) {\r\n        return kind === 0 /* IndexKind.ASCENDING */ ? this.ascending : this.descending;\r\n    }\r\n    encodedBytes() {\r\n        return this.orderedCode.encodedBytes();\r\n    }\r\n    reset() {\r\n        this.orderedCode.reset();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Represents an index entry saved by the SDK in persisted storage. */\r\nclass IndexEntry {\r\n    constructor(indexId, documentKey, arrayValue, directionalValue) {\r\n        this.indexId = indexId;\r\n        this.documentKey = documentKey;\r\n        this.arrayValue = arrayValue;\r\n        this.directionalValue = directionalValue;\r\n    }\r\n    /**\r\n     * Returns an IndexEntry entry that sorts immediately after the current\r\n     * directional value.\r\n     */\r\n    successor() {\r\n        const currentLength = this.directionalValue.length;\r\n        const newLength = currentLength === 0 || this.directionalValue[currentLength - 1] === 255\r\n            ? currentLength + 1\r\n            : currentLength;\r\n        const successor = new Uint8Array(newLength);\r\n        successor.set(this.directionalValue, 0);\r\n        if (newLength !== currentLength) {\r\n            successor.set([0], this.directionalValue.length);\r\n        }\r\n        else {\r\n            ++successor[successor.length - 1];\r\n        }\r\n        return new IndexEntry(this.indexId, this.documentKey, this.arrayValue, successor);\r\n    }\r\n}\r\nfunction indexEntryComparator(left, right) {\r\n    let cmp = left.indexId - right.indexId;\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    cmp = compareByteArrays(left.arrayValue, right.arrayValue);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    cmp = compareByteArrays(left.directionalValue, right.directionalValue);\r\n    if (cmp !== 0) {\r\n        return cmp;\r\n    }\r\n    return DocumentKey.comparator(left.documentKey, right.documentKey);\r\n}\r\nfunction compareByteArrays(left, right) {\r\n    for (let i = 0; i < left.length && i < right.length; ++i) {\r\n        const compare = left[i] - right[i];\r\n        if (compare !== 0) {\r\n            return compare;\r\n        }\r\n    }\r\n    return left.length - right.length;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A light query planner for Firestore.\r\n *\r\n * This class matches a `FieldIndex` against a Firestore Query `Target`. It\r\n * determines whether a given index can be used to serve the specified target.\r\n *\r\n * The following table showcases some possible index configurations:\r\n *\r\n * Query                                               | Index\r\n * -----------------------------------------------------------------------------\r\n * where('a', '==', 'a').where('b', '==', 'b')         | a ASC, b DESC\r\n * where('a', '==', 'a').where('b', '==', 'b')         | a ASC\r\n * where('a', '==', 'a').where('b', '==', 'b')         | b DESC\r\n * where('a', '>=', 'a').orderBy('a')                  | a ASC\r\n * where('a', '>=', 'a').orderBy('a', 'desc')          | a DESC\r\n * where('a', '>=', 'a').orderBy('a').orderBy('b')     | a ASC, b ASC\r\n * where('a', '>=', 'a').orderBy('a').orderBy('b')     | a ASC\r\n * where('a', 'array-contains', 'a').orderBy('b')      | a CONTAINS, b ASCENDING\r\n * where('a', 'array-contains', 'a').orderBy('b')      | a CONTAINS\r\n */\r\nclass TargetIndexMatcher {\r\n    constructor(target) {\r\n        this.collectionId =\r\n            target.collectionGroup != null\r\n                ? target.collectionGroup\r\n                : target.path.lastSegment();\r\n        this.orderBys = target.orderBy;\r\n        this.equalityFilters = [];\r\n        for (const filter of target.filters) {\r\n            const fieldFilter = filter;\r\n            if (fieldFilter.isInequality()) {\r\n                this.inequalityFilter = fieldFilter;\r\n            }\r\n            else {\r\n                this.equalityFilters.push(fieldFilter);\r\n            }\r\n        }\r\n    }\r\n    /**\r\n     * Returns whether the index can be used to serve the TargetIndexMatcher's\r\n     * target.\r\n     *\r\n     * An index is considered capable of serving the target when:\r\n     * - The target uses all index segments for its filters and orderBy clauses.\r\n     *   The target can have additional filter and orderBy clauses, but not\r\n     *   fewer.\r\n     * - If an ArrayContains/ArrayContainsAnyfilter is used, the index must also\r\n     *   have a corresponding `CONTAINS` segment.\r\n     * - All directional index segments can be mapped to the target as a series of\r\n     *   equality filters, a single inequality filter and a series of orderBy\r\n     *   clauses.\r\n     * - The segments that represent the equality filters may appear out of order.\r\n     * - The optional segment for the inequality filter must appear after all\r\n     *   equality segments.\r\n     * - The segments that represent that orderBy clause of the target must appear\r\n     *   in order after all equality and inequality segments. Single orderBy\r\n     *   clauses cannot be skipped, but a continuous orderBy suffix may be\r\n     *   omitted.\r\n     */\r\n    servedByIndex(index) {\r\n        hardAssert(index.collectionGroup === this.collectionId);\r\n        // If there is an array element, find a matching filter.\r\n        const arraySegment = fieldIndexGetArraySegment(index);\r\n        if (arraySegment !== undefined &&\r\n            !this.hasMatchingEqualityFilter(arraySegment)) {\r\n            return false;\r\n        }\r\n        const segments = fieldIndexGetDirectionalSegments(index);\r\n        let equalitySegments = new Set();\r\n        let segmentIndex = 0;\r\n        let orderBysIndex = 0;\r\n        // Process all equalities first. Equalities can appear out of order.\r\n        for (; segmentIndex < segments.length; ++segmentIndex) {\r\n            // We attempt to greedily match all segments to equality filters. If a\r\n            // filter matches an index segment, we can mark the segment as used.\r\n            if (this.hasMatchingEqualityFilter(segments[segmentIndex])) {\r\n                equalitySegments = equalitySegments.add(segments[segmentIndex].fieldPath.canonicalString());\r\n            }\r\n            else {\r\n                // If we cannot find a matching filter, we need to verify whether the\r\n                // remaining segments map to the target's inequality and its orderBy\r\n                // clauses.\r\n                break;\r\n            }\r\n        }\r\n        // If we already have processed all segments, all segments are used to serve\r\n        // the equality filters and we do not need to map any segments to the\r\n        // target's inequality and orderBy clauses.\r\n        if (segmentIndex === segments.length) {\r\n            return true;\r\n        }\r\n        if (this.inequalityFilter !== undefined) {\r\n            // If there is an inequality filter and the field was not in one of the\r\n            // equality filters above, the next segment must match both the filter\r\n            // and the first orderBy clause.\r\n            if (!equalitySegments.has(this.inequalityFilter.field.canonicalString())) {\r\n                const segment = segments[segmentIndex];\r\n                if (!this.matchesFilter(this.inequalityFilter, segment) ||\r\n                    !this.matchesOrderBy(this.orderBys[orderBysIndex++], segment)) {\r\n                    return false;\r\n                }\r\n            }\r\n            ++segmentIndex;\r\n        }\r\n        // All remaining segments need to represent the prefix of the target's\r\n        // orderBy.\r\n        for (; segmentIndex < segments.length; ++segmentIndex) {\r\n            const segment = segments[segmentIndex];\r\n            if (orderBysIndex >= this.orderBys.length ||\r\n                !this.matchesOrderBy(this.orderBys[orderBysIndex++], segment)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    hasMatchingEqualityFilter(segment) {\r\n        for (const filter of this.equalityFilters) {\r\n            if (this.matchesFilter(filter, segment)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n    matchesFilter(filter, segment) {\r\n        if (filter === undefined || !filter.field.isEqual(segment.fieldPath)) {\r\n            return false;\r\n        }\r\n        const isArrayOperator = filter.op === \"array-contains\" /* Operator.ARRAY_CONTAINS */ ||\r\n            filter.op === \"array-contains-any\" /* Operator.ARRAY_CONTAINS_ANY */;\r\n        return (segment.kind === 2 /* IndexKind.CONTAINS */) === isArrayOperator;\r\n    }\r\n    matchesOrderBy(orderBy, segment) {\r\n        if (!orderBy.field.isEqual(segment.fieldPath)) {\r\n            return false;\r\n        }\r\n        return ((segment.kind === 0 /* IndexKind.ASCENDING */ &&\r\n            orderBy.dir === \"asc\" /* Direction.ASCENDING */) ||\r\n            (segment.kind === 1 /* IndexKind.DESCENDING */ &&\r\n                orderBy.dir === \"desc\" /* Direction.DESCENDING */));\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Provides utility functions that help with boolean logic transformations needed for handling\r\n * complex filters used in queries.\r\n */\r\n/**\r\n * The `in` filter is only a syntactic sugar over a disjunction of equalities. For instance: `a in\r\n * [1,2,3]` is in fact `a==1 || a==2 || a==3`. This method expands any `in` filter in the given\r\n * input into a disjunction of equality filters and returns the expanded filter.\r\n */\r\nfunction computeInExpansion(filter) {\r\n    var _a, _b;\r\n    hardAssert(filter instanceof FieldFilter || filter instanceof CompositeFilter);\r\n    if (filter instanceof FieldFilter) {\r\n        if (filter instanceof InFilter) {\r\n            const expandedFilters = ((_b = (_a = filter.value.arrayValue) === null || _a === void 0 ? void 0 : _a.values) === null || _b === void 0 ? void 0 : _b.map(value => FieldFilter.create(filter.field, \"==\" /* Operator.EQUAL */, value))) || [];\r\n            return CompositeFilter.create(expandedFilters, \"or\" /* CompositeOperator.OR */);\r\n        }\r\n        else {\r\n            // We have reached other kinds of field filters.\r\n            return filter;\r\n        }\r\n    }\r\n    // We have a composite filter.\r\n    const expandedFilters = filter.filters.map(subfilter => computeInExpansion(subfilter));\r\n    return CompositeFilter.create(expandedFilters, filter.op);\r\n}\r\n/**\r\n * Given a composite filter, returns the list of terms in its disjunctive normal form.\r\n *\r\n * <p>Each element in the return value is one term of the resulting DNF. For instance: For the\r\n * input: (A || B) && C, the DNF form is: (A && C) || (B && C), and the return value is a list\r\n * with two elements: a composite filter that performs (A && C), and a composite filter that\r\n * performs (B && C).\r\n *\r\n * @param filter the composite filter to calculate DNF transform for.\r\n * @return the terms in the DNF transform.\r\n */\r\nfunction getDnfTerms(filter) {\r\n    if (filter.getFilters().length === 0) {\r\n        return [];\r\n    }\r\n    const result = computeDistributedNormalForm(computeInExpansion(filter));\r\n    hardAssert(isDisjunctiveNormalForm(result));\r\n    if (isSingleFieldFilter(result) || isFlatConjunction(result)) {\r\n        return [result];\r\n    }\r\n    return result.getFilters();\r\n}\r\n/** Returns true if the given filter is a single field filter. e.g. (a == 10). */\r\nfunction isSingleFieldFilter(filter) {\r\n    return filter instanceof FieldFilter;\r\n}\r\n/**\r\n * Returns true if the given filter is the conjunction of one or more field filters. e.g. (a == 10\r\n * && b == 20)\r\n */\r\nfunction isFlatConjunction(filter) {\r\n    return (filter instanceof CompositeFilter &&\r\n        compositeFilterIsFlatConjunction(filter));\r\n}\r\n/**\r\n * Returns whether or not the given filter is in disjunctive normal form (DNF).\r\n *\r\n * <p>In boolean logic, a disjunctive normal form (DNF) is a canonical normal form of a logical\r\n * formula consisting of a disjunction of conjunctions; it can also be described as an OR of ANDs.\r\n *\r\n * <p>For more info, visit: https://en.wikipedia.org/wiki/Disjunctive_normal_form\r\n */\r\nfunction isDisjunctiveNormalForm(filter) {\r\n    return (isSingleFieldFilter(filter) ||\r\n        isFlatConjunction(filter) ||\r\n        isDisjunctionOfFieldFiltersAndFlatConjunctions(filter));\r\n}\r\n/**\r\n * Returns true if the given filter is the disjunction of one or more \"flat conjunctions\" and\r\n * field filters. e.g. (a == 10) || (b==20 && c==30)\r\n */\r\nfunction isDisjunctionOfFieldFiltersAndFlatConjunctions(filter) {\r\n    if (filter instanceof CompositeFilter) {\r\n        if (compositeFilterIsDisjunction(filter)) {\r\n            for (const subFilter of filter.getFilters()) {\r\n                if (!isSingleFieldFilter(subFilter) && !isFlatConjunction(subFilter)) {\r\n                    return false;\r\n                }\r\n            }\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\r\nfunction computeDistributedNormalForm(filter) {\r\n    hardAssert(filter instanceof FieldFilter || filter instanceof CompositeFilter);\r\n    if (filter instanceof FieldFilter) {\r\n        return filter;\r\n    }\r\n    if (filter.filters.length === 1) {\r\n        return computeDistributedNormalForm(filter.filters[0]);\r\n    }\r\n    // Compute DNF for each of the subfilters first\r\n    const result = filter.filters.map(subfilter => computeDistributedNormalForm(subfilter));\r\n    let newFilter = CompositeFilter.create(result, filter.op);\r\n    newFilter = applyAssociation(newFilter);\r\n    if (isDisjunctiveNormalForm(newFilter)) {\r\n        return newFilter;\r\n    }\r\n    hardAssert(newFilter instanceof CompositeFilter);\r\n    hardAssert(compositeFilterIsConjunction(newFilter));\r\n    hardAssert(newFilter.filters.length > 1);\r\n    return newFilter.filters.reduce((runningResult, filter) => applyDistribution(runningResult, filter));\r\n}\r\nfunction applyDistribution(lhs, rhs) {\r\n    hardAssert(lhs instanceof FieldFilter || lhs instanceof CompositeFilter);\r\n    hardAssert(rhs instanceof FieldFilter || rhs instanceof CompositeFilter);\r\n    let result;\r\n    if (lhs instanceof FieldFilter) {\r\n        if (rhs instanceof FieldFilter) {\r\n            // FieldFilter FieldFilter\r\n            result = applyDistributionFieldFilters(lhs, rhs);\r\n        }\r\n        else {\r\n            // FieldFilter CompositeFilter\r\n            result = applyDistributionFieldAndCompositeFilters(lhs, rhs);\r\n        }\r\n    }\r\n    else {\r\n        if (rhs instanceof FieldFilter) {\r\n            // CompositeFilter FieldFilter\r\n            result = applyDistributionFieldAndCompositeFilters(rhs, lhs);\r\n        }\r\n        else {\r\n            // CompositeFilter CompositeFilter\r\n            result = applyDistributionCompositeFilters(lhs, rhs);\r\n        }\r\n    }\r\n    return applyAssociation(result);\r\n}\r\nfunction applyDistributionFieldFilters(lhs, rhs) {\r\n    // Conjunction distribution for two field filters is the conjunction of them.\r\n    return CompositeFilter.create([lhs, rhs], \"and\" /* CompositeOperator.AND */);\r\n}\r\nfunction applyDistributionCompositeFilters(lhs, rhs) {\r\n    hardAssert(lhs.filters.length > 0 && rhs.filters.length > 0);\r\n    // There are four cases:\r\n    // (A & B) & (C & D) --> (A & B & C & D)\r\n    // (A & B) & (C | D) --> (A & B & C) | (A & B & D)\r\n    // (A | B) & (C & D) --> (C & D & A) | (C & D & B)\r\n    // (A | B) & (C | D) --> (A & C) | (A & D) | (B & C) | (B & D)\r\n    // Case 1 is a merge.\r\n    if (compositeFilterIsConjunction(lhs) && compositeFilterIsConjunction(rhs)) {\r\n        return compositeFilterWithAddedFilters(lhs, rhs.getFilters());\r\n    }\r\n    // Case 2,3,4 all have at least one side (lhs or rhs) that is a disjunction. In all three cases\r\n    // we should take each element of the disjunction and distribute it over the other side, and\r\n    // return the disjunction of the distribution results.\r\n    const disjunctionSide = compositeFilterIsDisjunction(lhs) ? lhs : rhs;\r\n    const otherSide = compositeFilterIsDisjunction(lhs) ? rhs : lhs;\r\n    const results = disjunctionSide.filters.map(subfilter => applyDistribution(subfilter, otherSide));\r\n    return CompositeFilter.create(results, \"or\" /* CompositeOperator.OR */);\r\n}\r\nfunction applyDistributionFieldAndCompositeFilters(fieldFilter, compositeFilter) {\r\n    // There are two cases:\r\n    // A & (B & C) --> (A & B & C)\r\n    // A & (B | C) --> (A & B) | (A & C)\r\n    if (compositeFilterIsConjunction(compositeFilter)) {\r\n        // Case 1\r\n        return compositeFilterWithAddedFilters(compositeFilter, fieldFilter.getFilters());\r\n    }\r\n    else {\r\n        // Case 2\r\n        const newFilters = compositeFilter.filters.map(subfilter => applyDistribution(fieldFilter, subfilter));\r\n        return CompositeFilter.create(newFilters, \"or\" /* CompositeOperator.OR */);\r\n    }\r\n}\r\n/**\r\n * Applies the associativity property to the given filter and returns the resulting filter.\r\n *\r\n * <ul>\r\n *   <li>A | (B | C) == (A | B) | C == (A | B | C)\r\n *   <li>A & (B & C) == (A & B) & C == (A & B & C)\r\n * </ul>\r\n *\r\n * <p>For more info, visit: https://en.wikipedia.org/wiki/Associative_property#Propositional_logic\r\n */\r\nfunction applyAssociation(filter) {\r\n    hardAssert(filter instanceof FieldFilter || filter instanceof CompositeFilter);\r\n    if (filter instanceof FieldFilter) {\r\n        return filter;\r\n    }\r\n    const filters = filter.getFilters();\r\n    // If the composite filter only contains 1 filter, apply associativity to it.\r\n    if (filters.length === 1) {\r\n        return applyAssociation(filters[0]);\r\n    }\r\n    // Associativity applied to a flat composite filter results is itself.\r\n    if (compositeFilterIsFlat(filter)) {\r\n        return filter;\r\n    }\r\n    // First apply associativity to all subfilters. This will in turn recursively apply\r\n    // associativity to all nested composite filters and field filters.\r\n    const updatedFilters = filters.map(subfilter => applyAssociation(subfilter));\r\n    // For composite subfilters that perform the same kind of logical operation as `compositeFilter`\r\n    // take out their filters and add them to `compositeFilter`. For example:\r\n    // compositeFilter = (A | (B | C | D))\r\n    // compositeSubfilter = (B | C | D)\r\n    // Result: (A | B | C | D)\r\n    // Note that the `compositeSubfilter` has been eliminated, and its filters (B, C, D) have been\r\n    // added to the top-level \"compositeFilter\".\r\n    const newSubfilters = [];\r\n    updatedFilters.forEach(subfilter => {\r\n        if (subfilter instanceof FieldFilter) {\r\n            newSubfilters.push(subfilter);\r\n        }\r\n        else if (subfilter instanceof CompositeFilter) {\r\n            if (subfilter.op === filter.op) {\r\n                // compositeFilter: (A | (B | C))\r\n                // compositeSubfilter: (B | C)\r\n                // Result: (A | B | C)\r\n                newSubfilters.push(...subfilter.filters);\r\n            }\r\n            else {\r\n                // compositeFilter: (A | (B & C))\r\n                // compositeSubfilter: (B & C)\r\n                // Result: (A | (B & C))\r\n                newSubfilters.push(subfilter);\r\n            }\r\n        }\r\n    });\r\n    if (newSubfilters.length === 1) {\r\n        return newSubfilters[0];\r\n    }\r\n    return CompositeFilter.create(newSubfilters, filter.op);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An in-memory implementation of IndexManager.\r\n */\r\nclass MemoryIndexManager {\r\n    constructor() {\r\n        this.collectionParentIndex = new MemoryCollectionParentIndex();\r\n    }\r\n    addToCollectionParentIndex(transaction, collectionPath) {\r\n        this.collectionParentIndex.add(collectionPath);\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getCollectionParents(transaction, collectionId) {\r\n        return PersistencePromise.resolve(this.collectionParentIndex.getEntries(collectionId));\r\n    }\r\n    addFieldIndex(transaction, index) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n    deleteFieldIndex(transaction, index) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getDocumentsMatchingTarget(transaction, target) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve(null);\r\n    }\r\n    getIndexType(transaction, target) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve(0 /* IndexType.NONE */);\r\n    }\r\n    getFieldIndexes(transaction, collectionGroup) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve([]);\r\n    }\r\n    getNextCollectionGroupToUpdate(transaction) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve(null);\r\n    }\r\n    getMinOffset(transaction, target) {\r\n        return PersistencePromise.resolve(IndexOffset.min());\r\n    }\r\n    getMinOffsetFromCollectionGroup(transaction, collectionGroup) {\r\n        return PersistencePromise.resolve(IndexOffset.min());\r\n    }\r\n    updateCollectionGroup(transaction, collectionGroup, offset) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n    updateIndexEntries(transaction, documents) {\r\n        // Field indices are not supported with memory persistence.\r\n        return PersistencePromise.resolve();\r\n    }\r\n}\r\n/**\r\n * Internal implementation of the collection-parent index exposed by MemoryIndexManager.\r\n * Also used for in-memory caching by IndexedDbIndexManager and initial index population\r\n * in indexeddb_schema.ts\r\n */\r\nclass MemoryCollectionParentIndex {\r\n    constructor() {\r\n        this.index = {};\r\n    }\r\n    // Returns false if the entry already existed.\r\n    add(collectionPath) {\r\n        const collectionId = collectionPath.lastSegment();\r\n        const parentPath = collectionPath.popLast();\r\n        const existingParents = this.index[collectionId] ||\r\n            new SortedSet(ResourcePath.comparator);\r\n        const added = !existingParents.has(parentPath);\r\n        this.index[collectionId] = existingParents.add(parentPath);\r\n        return added;\r\n    }\r\n    has(collectionPath) {\r\n        const collectionId = collectionPath.lastSegment();\r\n        const parentPath = collectionPath.popLast();\r\n        const existingParents = this.index[collectionId];\r\n        return existingParents && existingParents.has(parentPath);\r\n    }\r\n    getEntries(collectionId) {\r\n        const parentPaths = this.index[collectionId] ||\r\n            new SortedSet(ResourcePath.comparator);\r\n        return parentPaths.toArray();\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$f = 'IndexedDbIndexManager';\r\nconst EMPTY_VALUE = new Uint8Array(0);\r\n/**\r\n * A persisted implementation of IndexManager.\r\n *\r\n * PORTING NOTE: Unlike iOS and Android, the Web SDK does not memoize index\r\n * data as it supports multi-tab access.\r\n */\r\nclass IndexedDbIndexManager {\r\n    constructor(user, databaseId) {\r\n        this.user = user;\r\n        this.databaseId = databaseId;\r\n        /**\r\n         * An in-memory copy of the index entries we've already written since the SDK\r\n         * launched. Used to avoid re-writing the same entry repeatedly.\r\n         *\r\n         * This is *NOT* a complete cache of what's in persistence and so can never be\r\n         * used to satisfy reads.\r\n         */\r\n        this.collectionParentsCache = new MemoryCollectionParentIndex();\r\n        /**\r\n         * Maps from a target to its equivalent list of sub-targets. Each sub-target\r\n         * contains only one term from the target's disjunctive normal form (DNF).\r\n         */\r\n        this.targetToDnfSubTargets = new ObjectMap(t => canonifyTarget(t), (l, r) => targetEquals(l, r));\r\n        this.uid = user.uid || '';\r\n    }\r\n    /**\r\n     * Adds a new entry to the collection parent index.\r\n     *\r\n     * Repeated calls for the same collectionPath should be avoided within a\r\n     * transaction as IndexedDbIndexManager only caches writes once a transaction\r\n     * has been committed.\r\n     */\r\n    addToCollectionParentIndex(transaction, collectionPath) {\r\n        if (!this.collectionParentsCache.has(collectionPath)) {\r\n            const collectionId = collectionPath.lastSegment();\r\n            const parentPath = collectionPath.popLast();\r\n            transaction.addOnCommittedListener(() => {\r\n                // Add the collection to the in memory cache only if the transaction was\r\n                // successfully committed.\r\n                this.collectionParentsCache.add(collectionPath);\r\n            });\r\n            const collectionParent = {\r\n                collectionId,\r\n                parent: encodeResourcePath(parentPath)\r\n            };\r\n            return collectionParentsStore(transaction).put(collectionParent);\r\n        }\r\n        return PersistencePromise.resolve();\r\n    }\r\n    getCollectionParents(transaction, collectionId) {\r\n        const parentPaths = [];\r\n        const range = IDBKeyRange.bound([collectionId, ''], [immediateSuccessor(collectionId), ''], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        return collectionParentsStore(transaction)\r\n            .loadAll(range)\r\n            .next(entries => {\r\n            for (const entry of entries) {\r\n                // This collectionId guard shouldn't be necessary (and isn't as long\r\n                // as we're running in a real browser), but there's a bug in\r\n                // indexeddbshim that breaks our range in our tests running in node:\r\n                // https://github.com/axemclion/IndexedDBShim/issues/334\r\n                if (entry.collectionId !== collectionId) {\r\n                    break;\r\n                }\r\n                parentPaths.push(decodeResourcePath(entry.parent));\r\n            }\r\n            return parentPaths;\r\n        });\r\n    }\r\n    addFieldIndex(transaction, index) {\r\n        // TODO(indexing): Verify that the auto-incrementing index ID works in\r\n        // Safari & Firefox.\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const dbIndex = toDbIndexConfiguration(index);\r\n        delete dbIndex.indexId; // `indexId` is auto-populated by IndexedDb\r\n        const result = indexes.add(dbIndex);\r\n        if (index.indexState) {\r\n            const states = indexStateStore(transaction);\r\n            return result.next(indexId => {\r\n                states.put(toDbIndexState(indexId, this.user, index.indexState.sequenceNumber, index.indexState.offset));\r\n            });\r\n        }\r\n        else {\r\n            return result.next();\r\n        }\r\n    }\r\n    deleteFieldIndex(transaction, index) {\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const states = indexStateStore(transaction);\r\n        const entries = indexEntriesStore(transaction);\r\n        return indexes\r\n            .delete(index.indexId)\r\n            .next(() => states.delete(IDBKeyRange.bound([index.indexId], [index.indexId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true)))\r\n            .next(() => entries.delete(IDBKeyRange.bound([index.indexId], [index.indexId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true)));\r\n    }\r\n    getDocumentsMatchingTarget(transaction, target) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        let canServeTarget = true;\r\n        const indexes = new Map();\r\n        return PersistencePromise.forEach(this.getSubTargets(target), (subTarget) => {\r\n            return this.getFieldIndex(transaction, subTarget).next(index => {\r\n                canServeTarget && (canServeTarget = !!index);\r\n                indexes.set(subTarget, index);\r\n            });\r\n        }).next(() => {\r\n            if (!canServeTarget) {\r\n                return PersistencePromise.resolve(null);\r\n            }\r\n            else {\r\n                let existingKeys = documentKeySet();\r\n                const result = [];\r\n                return PersistencePromise.forEach(indexes, (index, subTarget) => {\r\n                    logDebug(LOG_TAG$f, `Using index ${fieldIndexToString(index)} to execute ${canonifyTarget(target)}`);\r\n                    const arrayValues = targetGetArrayValues(subTarget, index);\r\n                    const notInValues = targetGetNotInValues(subTarget, index);\r\n                    const lowerBound = targetGetLowerBound(subTarget, index);\r\n                    const upperBound = targetGetUpperBound(subTarget, index);\r\n                    const lowerBoundEncoded = this.encodeBound(index, subTarget, lowerBound);\r\n                    const upperBoundEncoded = this.encodeBound(index, subTarget, upperBound);\r\n                    const notInEncoded = this.encodeValues(index, subTarget, notInValues);\r\n                    const indexRanges = this.generateIndexRanges(index.indexId, arrayValues, lowerBoundEncoded, lowerBound.inclusive, upperBoundEncoded, upperBound.inclusive, notInEncoded);\r\n                    return PersistencePromise.forEach(indexRanges, (indexRange) => {\r\n                        return indexEntries\r\n                            .loadFirst(indexRange, target.limit)\r\n                            .next(entries => {\r\n                            entries.forEach(entry => {\r\n                                const documentKey = DocumentKey.fromSegments(entry.documentKey);\r\n                                if (!existingKeys.has(documentKey)) {\r\n                                    existingKeys = existingKeys.add(documentKey);\r\n                                    result.push(documentKey);\r\n                                }\r\n                            });\r\n                        });\r\n                    });\r\n                }).next(() => result);\r\n            }\r\n        });\r\n    }\r\n    getSubTargets(target) {\r\n        let subTargets = this.targetToDnfSubTargets.get(target);\r\n        if (subTargets) {\r\n            return subTargets;\r\n        }\r\n        if (target.filters.length === 0) {\r\n            subTargets = [target];\r\n        }\r\n        else {\r\n            // There is an implicit AND operation between all the filters stored in the target\r\n            const dnf = getDnfTerms(CompositeFilter.create(target.filters, \"and\" /* CompositeOperator.AND */));\r\n            subTargets = dnf.map(term => newTarget(target.path, target.collectionGroup, target.orderBy, term.getFilters(), target.limit, target.startAt, target.endAt));\r\n        }\r\n        this.targetToDnfSubTargets.set(target, subTargets);\r\n        return subTargets;\r\n    }\r\n    /**\r\n     * Constructs a key range query on `DbIndexEntryStore` that unions all\r\n     * bounds.\r\n     */\r\n    generateIndexRanges(indexId, arrayValues, lowerBounds, lowerBoundInclusive, upperBounds, upperBoundInclusive, notInValues) {\r\n        // The number of total index scans we union together. This is similar to a\r\n        // distributed normal form, but adapted for array values. We create a single\r\n        // index range per value in an ARRAY_CONTAINS or ARRAY_CONTAINS_ANY filter\r\n        // combined with the values from the query bounds.\r\n        const totalScans = (arrayValues != null ? arrayValues.length : 1) *\r\n            Math.max(lowerBounds.length, upperBounds.length);\r\n        const scansPerArrayElement = totalScans / (arrayValues != null ? arrayValues.length : 1);\r\n        const indexRanges = [];\r\n        for (let i = 0; i < totalScans; ++i) {\r\n            const arrayValue = arrayValues\r\n                ? this.encodeSingleElement(arrayValues[i / scansPerArrayElement])\r\n                : EMPTY_VALUE;\r\n            const lowerBound = this.generateLowerBound(indexId, arrayValue, lowerBounds[i % scansPerArrayElement], lowerBoundInclusive);\r\n            const upperBound = this.generateUpperBound(indexId, arrayValue, upperBounds[i % scansPerArrayElement], upperBoundInclusive);\r\n            const notInBound = notInValues.map(notIn => this.generateLowerBound(indexId, arrayValue, notIn, \r\n            /* inclusive= */ true));\r\n            indexRanges.push(...this.createRange(lowerBound, upperBound, notInBound));\r\n        }\r\n        return indexRanges;\r\n    }\r\n    /** Generates the lower bound for `arrayValue` and `directionalValue`. */\r\n    generateLowerBound(indexId, arrayValue, directionalValue, inclusive) {\r\n        const entry = new IndexEntry(indexId, DocumentKey.empty(), arrayValue, directionalValue);\r\n        return inclusive ? entry : entry.successor();\r\n    }\r\n    /** Generates the upper bound for `arrayValue` and `directionalValue`. */\r\n    generateUpperBound(indexId, arrayValue, directionalValue, inclusive) {\r\n        const entry = new IndexEntry(indexId, DocumentKey.empty(), arrayValue, directionalValue);\r\n        return inclusive ? entry.successor() : entry;\r\n    }\r\n    getFieldIndex(transaction, target) {\r\n        const targetIndexMatcher = new TargetIndexMatcher(target);\r\n        const collectionGroup = target.collectionGroup != null\r\n            ? target.collectionGroup\r\n            : target.path.lastSegment();\r\n        return this.getFieldIndexes(transaction, collectionGroup).next(indexes => {\r\n            // Return the index with the most number of segments.\r\n            let index = null;\r\n            for (const candidate of indexes) {\r\n                const matches = targetIndexMatcher.servedByIndex(candidate);\r\n                if (matches &&\r\n                    (!index || candidate.fields.length > index.fields.length)) {\r\n                    index = candidate;\r\n                }\r\n            }\r\n            return index;\r\n        });\r\n    }\r\n    getIndexType(transaction, target) {\r\n        let indexType = 2 /* IndexType.FULL */;\r\n        const subTargets = this.getSubTargets(target);\r\n        return PersistencePromise.forEach(subTargets, (target) => {\r\n            return this.getFieldIndex(transaction, target).next(index => {\r\n                if (!index) {\r\n                    indexType = 0 /* IndexType.NONE */;\r\n                }\r\n                else if (indexType !== 0 /* IndexType.NONE */ &&\r\n                    index.fields.length < targetGetSegmentCount(target)) {\r\n                    indexType = 1 /* IndexType.PARTIAL */;\r\n                }\r\n            });\r\n        }).next(() => {\r\n            // OR queries have more than one sub-target (one sub-target per DNF term). We currently consider\r\n            // OR queries that have a `limit` to have a partial index. For such queries we perform sorting\r\n            // and apply the limit in memory as a post-processing step.\r\n            if (targetHasLimit(target) &&\r\n                subTargets.length > 1 &&\r\n                indexType === 2 /* IndexType.FULL */) {\r\n                return 1 /* IndexType.PARTIAL */;\r\n            }\r\n            return indexType;\r\n        });\r\n    }\r\n    /**\r\n     * Returns the byte encoded form of the directional values in the field index.\r\n     * Returns `null` if the document does not have all fields specified in the\r\n     * index.\r\n     */\r\n    encodeDirectionalElements(fieldIndex, document) {\r\n        const encoder = new IndexByteEncoder();\r\n        for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n            const field = document.data.field(segment.fieldPath);\r\n            if (field == null) {\r\n                return null;\r\n            }\r\n            const directionalEncoder = encoder.forKind(segment.kind);\r\n            FirestoreIndexValueWriter.INSTANCE.writeIndexValue(field, directionalEncoder);\r\n        }\r\n        return encoder.encodedBytes();\r\n    }\r\n    /** Encodes a single value to the ascending index format. */\r\n    encodeSingleElement(value) {\r\n        const encoder = new IndexByteEncoder();\r\n        FirestoreIndexValueWriter.INSTANCE.writeIndexValue(value, encoder.forKind(0 /* IndexKind.ASCENDING */));\r\n        return encoder.encodedBytes();\r\n    }\r\n    /**\r\n     * Returns an encoded form of the document key that sorts based on the key\r\n     * ordering of the field index.\r\n     */\r\n    encodeDirectionalKey(fieldIndex, documentKey) {\r\n        const encoder = new IndexByteEncoder();\r\n        FirestoreIndexValueWriter.INSTANCE.writeIndexValue(refValue(this.databaseId, documentKey), encoder.forKind(fieldIndexGetKeyOrder(fieldIndex)));\r\n        return encoder.encodedBytes();\r\n    }\r\n    /**\r\n     * Encodes the given field values according to the specification in `target`.\r\n     * For IN queries, a list of possible values is returned.\r\n     */\r\n    encodeValues(fieldIndex, target, values) {\r\n        if (values === null) {\r\n            return [];\r\n        }\r\n        let encoders = [];\r\n        encoders.push(new IndexByteEncoder());\r\n        let valueIdx = 0;\r\n        for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\r\n            const value = values[valueIdx++];\r\n            for (const encoder of encoders) {\r\n                if (this.isInFilter(target, segment.fieldPath) && isArray(value)) {\r\n                    encoders = this.expandIndexValues(encoders, segment, value);\r\n                }\r\n                else {\r\n                    const directionalEncoder = encoder.forKind(segment.kind);\r\n                    FirestoreIndexValueWriter.INSTANCE.writeIndexValue(value, directionalEncoder);\r\n                }\r\n            }\r\n        }\r\n        return this.getEncodedBytes(encoders);\r\n    }\r\n    /**\r\n     * Encodes the given bounds according to the specification in `target`. For IN\r\n     * queries, a list of possible values is returned.\r\n     */\r\n    encodeBound(fieldIndex, target, bound) {\r\n        return this.encodeValues(fieldIndex, target, bound.position);\r\n    }\r\n    /** Returns the byte representation for the provided encoders. */\r\n    getEncodedBytes(encoders) {\r\n        const result = [];\r\n        for (let i = 0; i < encoders.length; ++i) {\r\n            result[i] = encoders[i].encodedBytes();\r\n        }\r\n        return result;\r\n    }\r\n    /**\r\n     * Creates a separate encoder for each element of an array.\r\n     *\r\n     * The method appends each value to all existing encoders (e.g. filter(\"a\",\r\n     * \"==\", \"a1\").filter(\"b\", \"in\", [\"b1\", \"b2\"]) becomes [\"a1,b1\", \"a1,b2\"]). A\r\n     * list of new encoders is returned.\r\n     */\r\n    expandIndexValues(encoders, segment, value) {\r\n        const prefixes = [...encoders];\r\n        const results = [];\r\n        for (const arrayElement of value.arrayValue.values || []) {\r\n            for (const prefix of prefixes) {\r\n                const clonedEncoder = new IndexByteEncoder();\r\n                clonedEncoder.seed(prefix.encodedBytes());\r\n                FirestoreIndexValueWriter.INSTANCE.writeIndexValue(arrayElement, clonedEncoder.forKind(segment.kind));\r\n                results.push(clonedEncoder);\r\n            }\r\n        }\r\n        return results;\r\n    }\r\n    isInFilter(target, fieldPath) {\r\n        return !!target.filters.find(f => f instanceof FieldFilter &&\r\n            f.field.isEqual(fieldPath) &&\r\n            (f.op === \"in\" /* Operator.IN */ || f.op === \"not-in\" /* Operator.NOT_IN */));\r\n    }\r\n    getFieldIndexes(transaction, collectionGroup) {\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const states = indexStateStore(transaction);\r\n        return (collectionGroup\r\n            ? indexes.loadAll(DbIndexConfigurationCollectionGroupIndex, IDBKeyRange.bound(collectionGroup, collectionGroup))\r\n            : indexes.loadAll()).next(indexConfigs => {\r\n            const result = [];\r\n            return PersistencePromise.forEach(indexConfigs, (indexConfig) => {\r\n                return states\r\n                    .get([indexConfig.indexId, this.uid])\r\n                    .next(indexState => {\r\n                    result.push(fromDbIndexConfiguration(indexConfig, indexState));\r\n                });\r\n            }).next(() => result);\r\n        });\r\n    }\r\n    getNextCollectionGroupToUpdate(transaction) {\r\n        return this.getFieldIndexes(transaction).next(indexes => {\r\n            if (indexes.length === 0) {\r\n                return null;\r\n            }\r\n            indexes.sort((l, r) => {\r\n                const cmp = l.indexState.sequenceNumber - r.indexState.sequenceNumber;\r\n                return cmp !== 0\r\n                    ? cmp\r\n                    : primitiveComparator(l.collectionGroup, r.collectionGroup);\r\n            });\r\n            return indexes[0].collectionGroup;\r\n        });\r\n    }\r\n    updateCollectionGroup(transaction, collectionGroup, offset) {\r\n        const indexes = indexConfigurationStore(transaction);\r\n        const states = indexStateStore(transaction);\r\n        return this.getNextSequenceNumber(transaction).next(nextSequenceNumber => indexes\r\n            .loadAll(DbIndexConfigurationCollectionGroupIndex, IDBKeyRange.bound(collectionGroup, collectionGroup))\r\n            .next(configs => PersistencePromise.forEach(configs, (config) => states.put(toDbIndexState(config.indexId, this.user, nextSequenceNumber, offset)))));\r\n    }\r\n    updateIndexEntries(transaction, documents) {\r\n        // Porting Note: `getFieldIndexes()` on Web does not cache index lookups as\r\n        // it could be used across different IndexedDB transactions. As any cached\r\n        // data might be invalidated by other multi-tab clients, we can only trust\r\n        // data within a single IndexedDB transaction. We therefore add a cache\r\n        // here.\r\n        const memoizedIndexes = new Map();\r\n        return PersistencePromise.forEach(documents, (key, doc) => {\r\n            const memoizedCollectionIndexes = memoizedIndexes.get(key.collectionGroup);\r\n            const fieldIndexes = memoizedCollectionIndexes\r\n                ? PersistencePromise.resolve(memoizedCollectionIndexes)\r\n                : this.getFieldIndexes(transaction, key.collectionGroup);\r\n            return fieldIndexes.next(fieldIndexes => {\r\n                memoizedIndexes.set(key.collectionGroup, fieldIndexes);\r\n                return PersistencePromise.forEach(fieldIndexes, (fieldIndex) => {\r\n                    return this.getExistingIndexEntries(transaction, key, fieldIndex).next(existingEntries => {\r\n                        const newEntries = this.computeIndexEntries(doc, fieldIndex);\r\n                        if (!existingEntries.isEqual(newEntries)) {\r\n                            return this.updateEntries(transaction, doc, fieldIndex, existingEntries, newEntries);\r\n                        }\r\n                        return PersistencePromise.resolve();\r\n                    });\r\n                });\r\n            });\r\n        });\r\n    }\r\n    addIndexEntry(transaction, document, fieldIndex, indexEntry) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        return indexEntries.put({\r\n            indexId: indexEntry.indexId,\r\n            uid: this.uid,\r\n            arrayValue: indexEntry.arrayValue,\r\n            directionalValue: indexEntry.directionalValue,\r\n            orderedDocumentKey: this.encodeDirectionalKey(fieldIndex, document.key),\r\n            documentKey: document.key.path.toArray()\r\n        });\r\n    }\r\n    deleteIndexEntry(transaction, document, fieldIndex, indexEntry) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        return indexEntries.delete([\r\n            indexEntry.indexId,\r\n            this.uid,\r\n            indexEntry.arrayValue,\r\n            indexEntry.directionalValue,\r\n            this.encodeDirectionalKey(fieldIndex, document.key),\r\n            document.key.path.toArray()\r\n        ]);\r\n    }\r\n    getExistingIndexEntries(transaction, documentKey, fieldIndex) {\r\n        const indexEntries = indexEntriesStore(transaction);\r\n        let results = new SortedSet(indexEntryComparator);\r\n        return indexEntries\r\n            .iterate({\r\n            index: DbIndexEntryDocumentKeyIndex,\r\n            range: IDBKeyRange.only([\r\n                fieldIndex.indexId,\r\n                this.uid,\r\n                this.encodeDirectionalKey(fieldIndex, documentKey)\r\n            ])\r\n        }, (_, entry) => {\r\n            results = results.add(new IndexEntry(fieldIndex.indexId, documentKey, entry.arrayValue, entry.directionalValue));\r\n        })\r\n            .next(() => results);\r\n    }\r\n    /** Creates the index entries for the given document. */\r\n    computeIndexEntries(document, fieldIndex) {\r\n        let results = new SortedSet(indexEntryComparator);\r\n        const directionalValue = this.encodeDirectionalElements(fieldIndex, document);\r\n        if (directionalValue == null) {\r\n            return results;\r\n        }\r\n        const arraySegment = fieldIndexGetArraySegment(fieldIndex);\r\n        if (arraySegment != null) {\r\n            const value = document.data.field(arraySegment.fieldPath);\r\n            if (isArray(value)) {\r\n                for (const arrayValue of value.arrayValue.values || []) {\r\n                    results = results.add(new IndexEntry(fieldIndex.indexId, document.key, this.encodeSingleElement(arrayValue), directionalValue));\r\n                }\r\n            }\r\n        }\r\n        else {\r\n            results = results.add(new IndexEntry(fieldIndex.indexId, document.key, EMPTY_VALUE, directionalValue));\r\n        }\r\n        return results;\r\n    }\r\n    /**\r\n     * Updates the index entries for the provided document by deleting entries\r\n     * that are no longer referenced in `newEntries` and adding all newly added\r\n     * entries.\r\n     */\r\n    updateEntries(transaction, document, fieldIndex, existingEntries, newEntries) {\r\n        logDebug(LOG_TAG$f, \"Updating index entries for document '%s'\", document.key);\r\n        const promises = [];\r\n        diffSortedSets(existingEntries, newEntries, indexEntryComparator, \r\n        /* onAdd= */ entry => {\r\n            promises.push(this.addIndexEntry(transaction, document, fieldIndex, entry));\r\n        }, \r\n        /* onRemove= */ entry => {\r\n            promises.push(this.deleteIndexEntry(transaction, document, fieldIndex, entry));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getNextSequenceNumber(transaction) {\r\n        let nextSequenceNumber = 1;\r\n        const states = indexStateStore(transaction);\r\n        return states\r\n            .iterate({\r\n            index: DbIndexStateSequenceNumberIndex,\r\n            reverse: true,\r\n            range: IDBKeyRange.upperBound([this.uid, Number.MAX_SAFE_INTEGER])\r\n        }, (_, state, controller) => {\r\n            controller.done();\r\n            nextSequenceNumber = state.sequenceNumber + 1;\r\n        })\r\n            .next(() => nextSequenceNumber);\r\n    }\r\n    /**\r\n     * Returns a new set of IDB ranges that splits the existing range and excludes\r\n     * any values that match the `notInValue` from these ranges. As an example,\r\n     * '[foo > 2 && foo != 3]` becomes  `[foo > 2 && < 3, foo > 3]`.\r\n     */\r\n    createRange(lower, upper, notInValues) {\r\n        // The notIn values need to be sorted and unique so that we can return a\r\n        // sorted set of non-overlapping ranges.\r\n        notInValues = notInValues\r\n            .sort((l, r) => indexEntryComparator(l, r))\r\n            .filter((el, i, values) => !i || indexEntryComparator(el, values[i - 1]) !== 0);\r\n        const bounds = [];\r\n        bounds.push(lower);\r\n        for (const notInValue of notInValues) {\r\n            const cmpToLower = indexEntryComparator(notInValue, lower);\r\n            const cmpToUpper = indexEntryComparator(notInValue, upper);\r\n            if (cmpToLower === 0) {\r\n                // `notInValue` is the lower bound. We therefore need to raise the bound\r\n                // to the next value.\r\n                bounds[0] = lower.successor();\r\n            }\r\n            else if (cmpToLower > 0 && cmpToUpper < 0) {\r\n                // `notInValue` is in the middle of the range\r\n                bounds.push(notInValue);\r\n                bounds.push(notInValue.successor());\r\n            }\r\n            else if (cmpToUpper > 0) {\r\n                // `notInValue` (and all following values) are out of the range\r\n                break;\r\n            }\r\n        }\r\n        bounds.push(upper);\r\n        const ranges = [];\r\n        for (let i = 0; i < bounds.length; i += 2) {\r\n            // If we encounter two bounds that will create an unmatchable key range,\r\n            // then we return an empty set of key ranges.\r\n            if (this.isRangeMatchable(bounds[i], bounds[i + 1])) {\r\n                return [];\r\n            }\r\n            const lowerBound = [\r\n                bounds[i].indexId,\r\n                this.uid,\r\n                bounds[i].arrayValue,\r\n                bounds[i].directionalValue,\r\n                EMPTY_VALUE,\r\n                []\r\n            ];\r\n            const upperBound = [\r\n                bounds[i + 1].indexId,\r\n                this.uid,\r\n                bounds[i + 1].arrayValue,\r\n                bounds[i + 1].directionalValue,\r\n                EMPTY_VALUE,\r\n                []\r\n            ];\r\n            ranges.push(IDBKeyRange.bound(lowerBound, upperBound));\r\n        }\r\n        return ranges;\r\n    }\r\n    isRangeMatchable(lowerBound, upperBound) {\r\n        // If lower bound is greater than the upper bound, then the key\r\n        // range can never be matched.\r\n        return indexEntryComparator(lowerBound, upperBound) > 0;\r\n    }\r\n    getMinOffsetFromCollectionGroup(transaction, collectionGroup) {\r\n        return this.getFieldIndexes(transaction, collectionGroup).next(getMinOffsetFromFieldIndexes);\r\n    }\r\n    getMinOffset(transaction, target) {\r\n        return PersistencePromise.mapArray(this.getSubTargets(target), (subTarget) => this.getFieldIndex(transaction, subTarget).next(index => index ? index : fail())).next(getMinOffsetFromFieldIndexes);\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the collectionParents\r\n * document store.\r\n */\r\nfunction collectionParentsStore(txn) {\r\n    return getStore(txn, DbCollectionParentStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the index entry object store.\r\n */\r\nfunction indexEntriesStore(txn) {\r\n    return getStore(txn, DbIndexEntryStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the index configuration object store.\r\n */\r\nfunction indexConfigurationStore(txn) {\r\n    return getStore(txn, DbIndexConfigurationStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the index state object store.\r\n */\r\nfunction indexStateStore(txn) {\r\n    return getStore(txn, DbIndexStateStore);\r\n}\r\nfunction getMinOffsetFromFieldIndexes(fieldIndexes) {\r\n    hardAssert(fieldIndexes.length !== 0);\r\n    let minOffset = fieldIndexes[0].indexState.offset;\r\n    let maxBatchId = minOffset.largestBatchId;\r\n    for (let i = 1; i < fieldIndexes.length; i++) {\r\n        const newOffset = fieldIndexes[i].indexState.offset;\r\n        if (indexOffsetComparator(newOffset, minOffset) < 0) {\r\n            minOffset = newOffset;\r\n        }\r\n        if (maxBatchId < newOffset.largestBatchId) {\r\n            maxBatchId = newOffset.largestBatchId;\r\n        }\r\n    }\r\n    return new IndexOffset(minOffset.readTime, minOffset.documentKey, maxBatchId);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Delete a mutation batch and the associated document mutations.\r\n * @returns A PersistencePromise of the document mutations that were removed.\r\n */\r\nfunction removeMutationBatch(txn, userId, batch) {\r\n    const mutationStore = txn.store(DbMutationBatchStore);\r\n    const indexTxn = txn.store(DbDocumentMutationStore);\r\n    const promises = [];\r\n    const range = IDBKeyRange.only(batch.batchId);\r\n    let numDeleted = 0;\r\n    const removePromise = mutationStore.iterate({ range }, (key, value, control) => {\r\n        numDeleted++;\r\n        return control.delete();\r\n    });\r\n    promises.push(removePromise.next(() => {\r\n        hardAssert(numDeleted === 1);\r\n    }));\r\n    const removedDocuments = [];\r\n    for (const mutation of batch.mutations) {\r\n        const indexKey = newDbDocumentMutationKey(userId, mutation.key.path, batch.batchId);\r\n        promises.push(indexTxn.delete(indexKey));\r\n        removedDocuments.push(mutation.key);\r\n    }\r\n    return PersistencePromise.waitFor(promises).next(() => removedDocuments);\r\n}\r\n/**\r\n * Returns an approximate size for the given document.\r\n */\r\nfunction dbDocumentSize(doc) {\r\n    if (!doc) {\r\n        return 0;\r\n    }\r\n    let value;\r\n    if (doc.document) {\r\n        value = doc.document;\r\n    }\r\n    else if (doc.unknownDocument) {\r\n        value = doc.unknownDocument;\r\n    }\r\n    else if (doc.noDocument) {\r\n        value = doc.noDocument;\r\n    }\r\n    else {\r\n        throw fail();\r\n    }\r\n    return JSON.stringify(value).length;\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** A mutation queue for a specific user, backed by IndexedDB. */\r\nclass IndexedDbMutationQueue {\r\n    constructor(\r\n    /**\r\n     * The normalized userId (e.g. null UID => \"\" userId) used to store /\r\n     * retrieve mutations.\r\n     */\r\n    userId, serializer, indexManager, referenceDelegate) {\r\n        this.userId = userId;\r\n        this.serializer = serializer;\r\n        this.indexManager = indexManager;\r\n        this.referenceDelegate = referenceDelegate;\r\n        /**\r\n         * Caches the document keys for pending mutation batches. If the mutation\r\n         * has been removed from IndexedDb, the cached value may continue to\r\n         * be used to retrieve the batch's document keys. To remove a cached value\r\n         * locally, `removeCachedMutationKeys()` should be invoked either directly\r\n         * or through `removeMutationBatches()`.\r\n         *\r\n         * With multi-tab, when the primary client acknowledges or rejects a mutation,\r\n         * this cache is used by secondary clients to invalidate the local\r\n         * view of the documents that were previously affected by the mutation.\r\n         */\r\n        // PORTING NOTE: Multi-tab only.\r\n        this.documentKeysByBatchId = {};\r\n    }\r\n    /**\r\n     * Creates a new mutation queue for the given user.\r\n     * @param user - The user for which to create a mutation queue.\r\n     * @param serializer - The serializer to use when persisting to IndexedDb.\r\n     */\r\n    static forUser(user, serializer, indexManager, referenceDelegate) {\r\n        // TODO(mcg): Figure out what constraints there are on userIDs\r\n        // In particular, are there any reserved characters? are empty ids allowed?\r\n        // For the moment store these together in the same mutations table assuming\r\n        // that empty userIDs aren't allowed.\r\n        hardAssert(user.uid !== '');\r\n        const userId = user.isAuthenticated() ? user.uid : '';\r\n        return new IndexedDbMutationQueue(userId, serializer, indexManager, referenceDelegate);\r\n    }\r\n    checkEmpty(transaction) {\r\n        let empty = true;\r\n        const range = IDBKeyRange.bound([this.userId, Number.NEGATIVE_INFINITY], [this.userId, Number.POSITIVE_INFINITY]);\r\n        return mutationsStore(transaction)\r\n            .iterate({ index: DbMutationBatchUserMutationsIndex, range }, (key, value, control) => {\r\n            empty = false;\r\n            control.done();\r\n        })\r\n            .next(() => empty);\r\n    }\r\n    addMutationBatch(transaction, localWriteTime, baseMutations, mutations) {\r\n        const documentStore = documentMutationsStore(transaction);\r\n        const mutationStore = mutationsStore(transaction);\r\n        // The IndexedDb implementation in Chrome (and Firefox) does not handle\r\n        // compound indices that include auto-generated keys correctly. To ensure\r\n        // that the index entry is added correctly in all browsers, we perform two\r\n        // writes: The first write is used to retrieve the next auto-generated Batch\r\n        // ID, and the second write populates the index and stores the actual\r\n        // mutation batch.\r\n        // See: https://bugs.chromium.org/p/chromium/issues/detail?id=701972\r\n        // We write an empty object to obtain key\r\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\r\n        return mutationStore.add({}).next(batchId => {\r\n            hardAssert(typeof batchId === 'number');\r\n            const batch = new MutationBatch(batchId, localWriteTime, baseMutations, mutations);\r\n            const dbBatch = toDbMutationBatch(this.serializer, this.userId, batch);\r\n            const promises = [];\r\n            let collectionParents = new SortedSet((l, r) => primitiveComparator(l.canonicalString(), r.canonicalString()));\r\n            for (const mutation of mutations) {\r\n                const indexKey = newDbDocumentMutationKey(this.userId, mutation.key.path, batchId);\r\n                collectionParents = collectionParents.add(mutation.key.path.popLast());\r\n                promises.push(mutationStore.put(dbBatch));\r\n                promises.push(documentStore.put(indexKey, DbDocumentMutationPlaceholder));\r\n            }\r\n            collectionParents.forEach(parent => {\r\n                promises.push(this.indexManager.addToCollectionParentIndex(transaction, parent));\r\n            });\r\n            transaction.addOnCommittedListener(() => {\r\n                this.documentKeysByBatchId[batchId] = batch.keys();\r\n            });\r\n            return PersistencePromise.waitFor(promises).next(() => batch);\r\n        });\r\n    }\r\n    lookupMutationBatch(transaction, batchId) {\r\n        return mutationsStore(transaction)\r\n            .get(batchId)\r\n            .next(dbBatch => {\r\n            if (dbBatch) {\r\n                hardAssert(dbBatch.userId === this.userId);\r\n                return fromDbMutationBatch(this.serializer, dbBatch);\r\n            }\r\n            return null;\r\n        });\r\n    }\r\n    /**\r\n     * Returns the document keys for the mutation batch with the given batchId.\r\n     * For primary clients, this method returns `null` after\r\n     * `removeMutationBatches()` has been called. Secondary clients return a\r\n     * cached result until `removeCachedMutationKeys()` is invoked.\r\n     */\r\n    // PORTING NOTE: Multi-tab only.\r\n    lookupMutationKeys(transaction, batchId) {\r\n        if (this.documentKeysByBatchId[batchId]) {\r\n            return PersistencePromise.resolve(this.documentKeysByBatchId[batchId]);\r\n        }\r\n        else {\r\n            return this.lookupMutationBatch(transaction, batchId).next(batch => {\r\n                if (batch) {\r\n                    const keys = batch.keys();\r\n                    this.documentKeysByBatchId[batchId] = keys;\r\n                    return keys;\r\n                }\r\n                else {\r\n                    return null;\r\n                }\r\n            });\r\n        }\r\n    }\r\n    getNextMutationBatchAfterBatchId(transaction, batchId) {\r\n        const nextBatchId = batchId + 1;\r\n        const range = IDBKeyRange.lowerBound([this.userId, nextBatchId]);\r\n        let foundBatch = null;\r\n        return mutationsStore(transaction)\r\n            .iterate({ index: DbMutationBatchUserMutationsIndex, range }, (key, dbBatch, control) => {\r\n            if (dbBatch.userId === this.userId) {\r\n                hardAssert(dbBatch.batchId >= nextBatchId);\r\n                foundBatch = fromDbMutationBatch(this.serializer, dbBatch);\r\n            }\r\n            control.done();\r\n        })\r\n            .next(() => foundBatch);\r\n    }\r\n    getHighestUnacknowledgedBatchId(transaction) {\r\n        const range = IDBKeyRange.upperBound([\r\n            this.userId,\r\n            Number.POSITIVE_INFINITY\r\n        ]);\r\n        let batchId = BATCHID_UNKNOWN;\r\n        return mutationsStore(transaction)\r\n            .iterate({ index: DbMutationBatchUserMutationsIndex, range, reverse: true }, (key, dbBatch, control) => {\r\n            batchId = dbBatch.batchId;\r\n            control.done();\r\n        })\r\n            .next(() => batchId);\r\n    }\r\n    getAllMutationBatches(transaction) {\r\n        const range = IDBKeyRange.bound([this.userId, BATCHID_UNKNOWN], [this.userId, Number.POSITIVE_INFINITY]);\r\n        return mutationsStore(transaction)\r\n            .loadAll(DbMutationBatchUserMutationsIndex, range)\r\n            .next(dbBatches => dbBatches.map(dbBatch => fromDbMutationBatch(this.serializer, dbBatch)));\r\n    }\r\n    getAllMutationBatchesAffectingDocumentKey(transaction, documentKey) {\r\n        // Scan the document-mutation index starting with a prefix starting with\r\n        // the given documentKey.\r\n        const indexPrefix = newDbDocumentMutationPrefixForPath(this.userId, documentKey.path);\r\n        const indexStart = IDBKeyRange.lowerBound(indexPrefix);\r\n        const results = [];\r\n        return documentMutationsStore(transaction)\r\n            .iterate({ range: indexStart }, (indexKey, _, control) => {\r\n            const [userID, encodedPath, batchId] = indexKey;\r\n            // Only consider rows matching exactly the specific key of\r\n            // interest. Note that because we order by path first, and we\r\n            // order terminators before path separators, we'll encounter all\r\n            // the index rows for documentKey contiguously. In particular, all\r\n            // the rows for documentKey will occur before any rows for\r\n            // documents nested in a subcollection beneath documentKey so we\r\n            // can stop as soon as we hit any such row.\r\n            const path = decodeResourcePath(encodedPath);\r\n            if (userID !== this.userId || !documentKey.path.isEqual(path)) {\r\n                control.done();\r\n                return;\r\n            }\r\n            // Look up the mutation batch in the store.\r\n            return mutationsStore(transaction)\r\n                .get(batchId)\r\n                .next(mutation => {\r\n                if (!mutation) {\r\n                    throw fail();\r\n                }\r\n                hardAssert(mutation.userId === this.userId);\r\n                results.push(fromDbMutationBatch(this.serializer, mutation));\r\n            });\r\n        })\r\n            .next(() => results);\r\n    }\r\n    getAllMutationBatchesAffectingDocumentKeys(transaction, documentKeys) {\r\n        let uniqueBatchIDs = new SortedSet(primitiveComparator);\r\n        const promises = [];\r\n        documentKeys.forEach(documentKey => {\r\n            const indexStart = newDbDocumentMutationPrefixForPath(this.userId, documentKey.path);\r\n            const range = IDBKeyRange.lowerBound(indexStart);\r\n            const promise = documentMutationsStore(transaction).iterate({ range }, (indexKey, _, control) => {\r\n                const [userID, encodedPath, batchID] = indexKey;\r\n                // Only consider rows matching exactly the specific key of\r\n                // interest. Note that because we order by path first, and we\r\n                // order terminators before path separators, we'll encounter all\r\n                // the index rows for documentKey contiguously. In particular, all\r\n                // the rows for documentKey will occur before any rows for\r\n                // documents nested in a subcollection beneath documentKey so we\r\n                // can stop as soon as we hit any such row.\r\n                const path = decodeResourcePath(encodedPath);\r\n                if (userID !== this.userId || !documentKey.path.isEqual(path)) {\r\n                    control.done();\r\n                    return;\r\n                }\r\n                uniqueBatchIDs = uniqueBatchIDs.add(batchID);\r\n            });\r\n            promises.push(promise);\r\n        });\r\n        return PersistencePromise.waitFor(promises).next(() => this.lookupMutationBatches(transaction, uniqueBatchIDs));\r\n    }\r\n    getAllMutationBatchesAffectingQuery(transaction, query) {\r\n        const queryPath = query.path;\r\n        const immediateChildrenLength = queryPath.length + 1;\r\n        // TODO(mcg): Actually implement a single-collection query\r\n        //\r\n        // This is actually executing an ancestor query, traversing the whole\r\n        // subtree below the collection which can be horrifically inefficient for\r\n        // some structures. The right way to solve this is to implement the full\r\n        // value index, but that's not in the cards in the near future so this is\r\n        // the best we can do for the moment.\r\n        //\r\n        // Since we don't yet index the actual properties in the mutations, our\r\n        // current approach is to just return all mutation batches that affect\r\n        // documents in the collection being queried.\r\n        const indexPrefix = newDbDocumentMutationPrefixForPath(this.userId, queryPath);\r\n        const indexStart = IDBKeyRange.lowerBound(indexPrefix);\r\n        // Collect up unique batchIDs encountered during a scan of the index. Use a\r\n        // SortedSet to accumulate batch IDs so they can be traversed in order in a\r\n        // scan of the main table.\r\n        let uniqueBatchIDs = new SortedSet(primitiveComparator);\r\n        return documentMutationsStore(transaction)\r\n            .iterate({ range: indexStart }, (indexKey, _, control) => {\r\n            const [userID, encodedPath, batchID] = indexKey;\r\n            const path = decodeResourcePath(encodedPath);\r\n            if (userID !== this.userId || !queryPath.isPrefixOf(path)) {\r\n                control.done();\r\n                return;\r\n            }\r\n            // Rows with document keys more than one segment longer than the\r\n            // query path can't be matches. For example, a query on 'rooms'\r\n            // can't match the document /rooms/abc/messages/xyx.\r\n            // TODO(mcg): we'll need a different scanner when we implement\r\n            // ancestor queries.\r\n            if (path.length !== immediateChildrenLength) {\r\n                return;\r\n            }\r\n            uniqueBatchIDs = uniqueBatchIDs.add(batchID);\r\n        })\r\n            .next(() => this.lookupMutationBatches(transaction, uniqueBatchIDs));\r\n    }\r\n    lookupMutationBatches(transaction, batchIDs) {\r\n        const results = [];\r\n        const promises = [];\r\n        // TODO(rockwood): Implement this using iterate.\r\n        batchIDs.forEach(batchId => {\r\n            promises.push(mutationsStore(transaction)\r\n                .get(batchId)\r\n                .next(mutation => {\r\n                if (mutation === null) {\r\n                    throw fail();\r\n                }\r\n                hardAssert(mutation.userId === this.userId);\r\n                results.push(fromDbMutationBatch(this.serializer, mutation));\r\n            }));\r\n        });\r\n        return PersistencePromise.waitFor(promises).next(() => results);\r\n    }\r\n    removeMutationBatch(transaction, batch) {\r\n        return removeMutationBatch(transaction.simpleDbTransaction, this.userId, batch).next(removedDocuments => {\r\n            transaction.addOnCommittedListener(() => {\r\n                this.removeCachedMutationKeys(batch.batchId);\r\n            });\r\n            return PersistencePromise.forEach(removedDocuments, (key) => {\r\n                return this.referenceDelegate.markPotentiallyOrphaned(transaction, key);\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Clears the cached keys for a mutation batch. This method should be\r\n     * called by secondary clients after they process mutation updates.\r\n     *\r\n     * Note that this method does not have to be called from primary clients as\r\n     * the corresponding cache entries are cleared when an acknowledged or\r\n     * rejected batch is removed from the mutation queue.\r\n     */\r\n    // PORTING NOTE: Multi-tab only\r\n    removeCachedMutationKeys(batchId) {\r\n        delete this.documentKeysByBatchId[batchId];\r\n    }\r\n    performConsistencyCheck(txn) {\r\n        return this.checkEmpty(txn).next(empty => {\r\n            if (!empty) {\r\n                return PersistencePromise.resolve();\r\n            }\r\n            // Verify that there are no entries in the documentMutations index if\r\n            // the queue is empty.\r\n            const startRange = IDBKeyRange.lowerBound(newDbDocumentMutationPrefixForUser(this.userId));\r\n            const danglingMutationReferences = [];\r\n            return documentMutationsStore(txn)\r\n                .iterate({ range: startRange }, (key, _, control) => {\r\n                const userID = key[0];\r\n                if (userID !== this.userId) {\r\n                    control.done();\r\n                    return;\r\n                }\r\n                else {\r\n                    const path = decodeResourcePath(key[1]);\r\n                    danglingMutationReferences.push(path);\r\n                }\r\n            })\r\n                .next(() => {\r\n                hardAssert(danglingMutationReferences.length === 0);\r\n            });\r\n        });\r\n    }\r\n    containsKey(txn, key) {\r\n        return mutationQueueContainsKey(txn, this.userId, key);\r\n    }\r\n    // PORTING NOTE: Multi-tab only (state is held in memory in other clients).\r\n    /** Returns the mutation queue's metadata from IndexedDb. */\r\n    getMutationQueueMetadata(transaction) {\r\n        return mutationQueuesStore(transaction)\r\n            .get(this.userId)\r\n            .next((metadata) => {\r\n            return (metadata || {\r\n                userId: this.userId,\r\n                lastAcknowledgedBatchId: BATCHID_UNKNOWN,\r\n                lastStreamToken: ''\r\n            });\r\n        });\r\n    }\r\n}\r\n/**\r\n * @returns true if the mutation queue for the given user contains a pending\r\n *         mutation for the given key.\r\n */\r\nfunction mutationQueueContainsKey(txn, userId, key) {\r\n    const indexKey = newDbDocumentMutationPrefixForPath(userId, key.path);\r\n    const encodedPath = indexKey[1];\r\n    const startRange = IDBKeyRange.lowerBound(indexKey);\r\n    let containsKey = false;\r\n    return documentMutationsStore(txn)\r\n        .iterate({ range: startRange, keysOnly: true }, (key, value, control) => {\r\n        const [userID, keyPath, /*batchID*/ _] = key;\r\n        if (userID === userId && keyPath === encodedPath) {\r\n            containsKey = true;\r\n        }\r\n        control.done();\r\n    })\r\n        .next(() => containsKey);\r\n}\r\n/** Returns true if any mutation queue contains the given document. */\r\nfunction mutationQueuesContainKey(txn, docKey) {\r\n    let found = false;\r\n    return mutationQueuesStore(txn)\r\n        .iterateSerial(userId => {\r\n        return mutationQueueContainsKey(txn, userId, docKey).next(containsKey => {\r\n            if (containsKey) {\r\n                found = true;\r\n            }\r\n            return PersistencePromise.resolve(!containsKey);\r\n        });\r\n    })\r\n        .next(() => found);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the mutations object store.\r\n */\r\nfunction mutationsStore(txn) {\r\n    return getStore(txn, DbMutationBatchStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the mutationQueues object store.\r\n */\r\nfunction documentMutationsStore(txn) {\r\n    return getStore(txn, DbDocumentMutationStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the mutationQueues object store.\r\n */\r\nfunction mutationQueuesStore(txn) {\r\n    return getStore(txn, DbMutationQueueStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Offset to ensure non-overlapping target ids. */\r\nconst OFFSET = 2;\r\n/**\r\n * Generates monotonically increasing target IDs for sending targets to the\r\n * watch stream.\r\n *\r\n * The client constructs two generators, one for the target cache, and one for\r\n * for the sync engine (to generate limbo documents targets). These\r\n * generators produce non-overlapping IDs (by using even and odd IDs\r\n * respectively).\r\n *\r\n * By separating the target ID space, the query cache can generate target IDs\r\n * that persist across client restarts, while sync engine can independently\r\n * generate in-memory target IDs that are transient and can be reused after a\r\n * restart.\r\n */\r\nclass TargetIdGenerator {\r\n    constructor(lastId) {\r\n        this.lastId = lastId;\r\n    }\r\n    next() {\r\n        this.lastId += OFFSET;\r\n        return this.lastId;\r\n    }\r\n    static forTargetCache() {\r\n        // The target cache generator must return '2' in its first call to `next()`\r\n        // as there is no differentiation in the protocol layer between an unset\r\n        // number and the number '0'. If we were to sent a target with target ID\r\n        // '0', the backend would consider it unset and replace it with its own ID.\r\n        return new TargetIdGenerator(2 - OFFSET);\r\n    }\r\n    static forSyncEngine() {\r\n        // Sync engine assigns target IDs for limbo document detection.\r\n        return new TargetIdGenerator(1 - OFFSET);\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nclass IndexedDbTargetCache {\r\n    constructor(referenceDelegate, serializer) {\r\n        this.referenceDelegate = referenceDelegate;\r\n        this.serializer = serializer;\r\n    }\r\n    // PORTING NOTE: We don't cache global metadata for the target cache, since\r\n    // some of it (in particular `highestTargetId`) can be modified by secondary\r\n    // tabs. We could perhaps be more granular (and e.g. still cache\r\n    // `lastRemoteSnapshotVersion` in memory) but for simplicity we currently go\r\n    // to IndexedDb whenever we need to read metadata. We can revisit if it turns\r\n    // out to have a meaningful performance impact.\r\n    allocateTargetId(transaction) {\r\n        return this.retrieveMetadata(transaction).next(metadata => {\r\n            const targetIdGenerator = new TargetIdGenerator(metadata.highestTargetId);\r\n            metadata.highestTargetId = targetIdGenerator.next();\r\n            return this.saveMetadata(transaction, metadata).next(() => metadata.highestTargetId);\r\n        });\r\n    }\r\n    getLastRemoteSnapshotVersion(transaction) {\r\n        return this.retrieveMetadata(transaction).next(metadata => {\r\n            return SnapshotVersion.fromTimestamp(new Timestamp(metadata.lastRemoteSnapshotVersion.seconds, metadata.lastRemoteSnapshotVersion.nanoseconds));\r\n        });\r\n    }\r\n    getHighestSequenceNumber(transaction) {\r\n        return this.retrieveMetadata(transaction).next(targetGlobal => targetGlobal.highestListenSequenceNumber);\r\n    }\r\n    setTargetsMetadata(transaction, highestListenSequenceNumber, lastRemoteSnapshotVersion) {\r\n        return this.retrieveMetadata(transaction).next(metadata => {\r\n            metadata.highestListenSequenceNumber = highestListenSequenceNumber;\r\n            if (lastRemoteSnapshotVersion) {\r\n                metadata.lastRemoteSnapshotVersion =\r\n                    lastRemoteSnapshotVersion.toTimestamp();\r\n            }\r\n            if (highestListenSequenceNumber > metadata.highestListenSequenceNumber) {\r\n                metadata.highestListenSequenceNumber = highestListenSequenceNumber;\r\n            }\r\n            return this.saveMetadata(transaction, metadata);\r\n        });\r\n    }\r\n    addTargetData(transaction, targetData) {\r\n        return this.saveTargetData(transaction, targetData).next(() => {\r\n            return this.retrieveMetadata(transaction).next(metadata => {\r\n                metadata.targetCount += 1;\r\n                this.updateMetadataFromTargetData(targetData, metadata);\r\n                return this.saveMetadata(transaction, metadata);\r\n            });\r\n        });\r\n    }\r\n    updateTargetData(transaction, targetData) {\r\n        return this.saveTargetData(transaction, targetData);\r\n    }\r\n    removeTargetData(transaction, targetData) {\r\n        return this.removeMatchingKeysForTargetId(transaction, targetData.targetId)\r\n            .next(() => targetsStore(transaction).delete(targetData.targetId))\r\n            .next(() => this.retrieveMetadata(transaction))\r\n            .next(metadata => {\r\n            hardAssert(metadata.targetCount > 0);\r\n            metadata.targetCount -= 1;\r\n            return this.saveMetadata(transaction, metadata);\r\n        });\r\n    }\r\n    /**\r\n     * Drops any targets with sequence number less than or equal to the upper bound, excepting those\r\n     * present in `activeTargetIds`. Document associations for the removed targets are also removed.\r\n     * Returns the number of targets removed.\r\n     */\r\n    removeTargets(txn, upperBound, activeTargetIds) {\r\n        let count = 0;\r\n        const promises = [];\r\n        return targetsStore(txn)\r\n            .iterate((key, value) => {\r\n            const targetData = fromDbTarget(value);\r\n            if (targetData.sequenceNumber <= upperBound &&\r\n                activeTargetIds.get(targetData.targetId) === null) {\r\n                count++;\r\n                promises.push(this.removeTargetData(txn, targetData));\r\n            }\r\n        })\r\n            .next(() => PersistencePromise.waitFor(promises))\r\n            .next(() => count);\r\n    }\r\n    /**\r\n     * Call provided function with each `TargetData` that we have cached.\r\n     */\r\n    forEachTarget(txn, f) {\r\n        return targetsStore(txn).iterate((key, value) => {\r\n            const targetData = fromDbTarget(value);\r\n            f(targetData);\r\n        });\r\n    }\r\n    retrieveMetadata(transaction) {\r\n        return globalTargetStore(transaction)\r\n            .get(DbTargetGlobalKey)\r\n            .next(metadata => {\r\n            hardAssert(metadata !== null);\r\n            return metadata;\r\n        });\r\n    }\r\n    saveMetadata(transaction, metadata) {\r\n        return globalTargetStore(transaction).put(DbTargetGlobalKey, metadata);\r\n    }\r\n    saveTargetData(transaction, targetData) {\r\n        return targetsStore(transaction).put(toDbTarget(this.serializer, targetData));\r\n    }\r\n    /**\r\n     * In-place updates the provided metadata to account for values in the given\r\n     * TargetData. Saving is done separately. Returns true if there were any\r\n     * changes to the metadata.\r\n     */\r\n    updateMetadataFromTargetData(targetData, metadata) {\r\n        let updated = false;\r\n        if (targetData.targetId > metadata.highestTargetId) {\r\n            metadata.highestTargetId = targetData.targetId;\r\n            updated = true;\r\n        }\r\n        if (targetData.sequenceNumber > metadata.highestListenSequenceNumber) {\r\n            metadata.highestListenSequenceNumber = targetData.sequenceNumber;\r\n            updated = true;\r\n        }\r\n        return updated;\r\n    }\r\n    getTargetCount(transaction) {\r\n        return this.retrieveMetadata(transaction).next(metadata => metadata.targetCount);\r\n    }\r\n    getTargetData(transaction, target) {\r\n        // Iterating by the canonicalId may yield more than one result because\r\n        // canonicalId values are not required to be unique per target. This query\r\n        // depends on the queryTargets index to be efficient.\r\n        const canonicalId = canonifyTarget(target);\r\n        const range = IDBKeyRange.bound([canonicalId, Number.NEGATIVE_INFINITY], [canonicalId, Number.POSITIVE_INFINITY]);\r\n        let result = null;\r\n        return targetsStore(transaction)\r\n            .iterate({ range, index: DbTargetQueryTargetsIndexName }, (key, value, control) => {\r\n            const found = fromDbTarget(value);\r\n            // After finding a potential match, check that the target is\r\n            // actually equal to the requested target.\r\n            if (targetEquals(target, found.target)) {\r\n                result = found;\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => result);\r\n    }\r\n    addMatchingKeys(txn, keys, targetId) {\r\n        // PORTING NOTE: The reverse index (documentsTargets) is maintained by\r\n        // IndexedDb.\r\n        const promises = [];\r\n        const store = documentTargetStore(txn);\r\n        keys.forEach(key => {\r\n            const path = encodeResourcePath(key.path);\r\n            promises.push(store.put({ targetId, path }));\r\n            promises.push(this.referenceDelegate.addReference(txn, targetId, key));\r\n        });\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    removeMatchingKeys(txn, keys, targetId) {\r\n        // PORTING NOTE: The reverse index (documentsTargets) is maintained by\r\n        // IndexedDb.\r\n        const store = documentTargetStore(txn);\r\n        return PersistencePromise.forEach(keys, (key) => {\r\n            const path = encodeResourcePath(key.path);\r\n            return PersistencePromise.waitFor([\r\n                store.delete([targetId, path]),\r\n                this.referenceDelegate.removeReference(txn, targetId, key)\r\n            ]);\r\n        });\r\n    }\r\n    removeMatchingKeysForTargetId(txn, targetId) {\r\n        const store = documentTargetStore(txn);\r\n        const range = IDBKeyRange.bound([targetId], [targetId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        return store.delete(range);\r\n    }\r\n    getMatchingKeysForTargetId(txn, targetId) {\r\n        const range = IDBKeyRange.bound([targetId], [targetId + 1], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        const store = documentTargetStore(txn);\r\n        let result = documentKeySet();\r\n        return store\r\n            .iterate({ range, keysOnly: true }, (key, _, control) => {\r\n            const path = decodeResourcePath(key[1]);\r\n            const docKey = new DocumentKey(path);\r\n            result = result.add(docKey);\r\n        })\r\n            .next(() => result);\r\n    }\r\n    containsKey(txn, key) {\r\n        const path = encodeResourcePath(key.path);\r\n        const range = IDBKeyRange.bound([path], [immediateSuccessor(path)], \r\n        /*lowerOpen=*/ false, \r\n        /*upperOpen=*/ true);\r\n        let count = 0;\r\n        return documentTargetStore(txn)\r\n            .iterate({\r\n            index: DbTargetDocumentDocumentTargetsIndex,\r\n            keysOnly: true,\r\n            range\r\n        }, ([targetId, path], _, control) => {\r\n            // Having a sentinel row for a document does not count as containing that document;\r\n            // For the target cache, containing the document means the document is part of some\r\n            // target.\r\n            if (targetId !== 0) {\r\n                count++;\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => count > 0);\r\n    }\r\n    /**\r\n     * Looks up a TargetData entry by target ID.\r\n     *\r\n     * @param targetId - The target ID of the TargetData entry to look up.\r\n     * @returns The cached TargetData entry, or null if the cache has no entry for\r\n     * the target.\r\n     */\r\n    // PORTING NOTE: Multi-tab only.\r\n    getTargetDataForTarget(transaction, targetId) {\r\n        return targetsStore(transaction)\r\n            .get(targetId)\r\n            .next(found => {\r\n            if (found) {\r\n                return fromDbTarget(found);\r\n            }\r\n            else {\r\n                return null;\r\n            }\r\n        });\r\n    }\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the queries object store.\r\n */\r\nfunction targetsStore(txn) {\r\n    return getStore(txn, DbTargetStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the target globals object store.\r\n */\r\nfunction globalTargetStore(txn) {\r\n    return getStore(txn, DbTargetGlobalStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the document target object store.\r\n */\r\nfunction documentTargetStore(txn) {\r\n    return getStore(txn, DbTargetDocumentStore);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst GC_DID_NOT_RUN = {\r\n    didRun: false,\r\n    sequenceNumbersCollected: 0,\r\n    targetsRemoved: 0,\r\n    documentsRemoved: 0\r\n};\r\nconst LRU_COLLECTION_DISABLED = -1;\r\nconst LRU_DEFAULT_CACHE_SIZE_BYTES = 40 * 1024 * 1024;\r\nclass LruParams {\r\n    constructor(\r\n    // When we attempt to collect, we will only do so if the cache size is greater than this\r\n    // threshold. Passing `COLLECTION_DISABLED` here will cause collection to always be skipped.\r\n    cacheSizeCollectionThreshold, \r\n    // The percentage of sequence numbers that we will attempt to collect\r\n    percentileToCollect, \r\n    // A cap on the total number of sequence numbers that will be collected. This prevents\r\n    // us from collecting a huge number of sequence numbers if the cache has grown very large.\r\n    maximumSequenceNumbersToCollect) {\r\n        this.cacheSizeCollectionThreshold = cacheSizeCollectionThreshold;\r\n        this.percentileToCollect = percentileToCollect;\r\n        this.maximumSequenceNumbersToCollect = maximumSequenceNumbersToCollect;\r\n    }\r\n    static withCacheSize(cacheSize) {\r\n        return new LruParams(cacheSize, LruParams.DEFAULT_COLLECTION_PERCENTILE, LruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT);\r\n    }\r\n}\r\nLruParams.DEFAULT_COLLECTION_PERCENTILE = 10;\r\nLruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT = 1000;\r\nLruParams.DEFAULT = new LruParams(LRU_DEFAULT_CACHE_SIZE_BYTES, LruParams.DEFAULT_COLLECTION_PERCENTILE, LruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT);\r\nLruParams.DISABLED = new LruParams(LRU_COLLECTION_DISABLED, 0, 0);\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nconst LOG_TAG$e = 'LruGarbageCollector';\r\nconst LRU_MINIMUM_CACHE_SIZE_BYTES = 1 * 1024 * 1024;\r\n/** How long we wait to try running LRU GC after SDK initialization. */\r\nconst INITIAL_GC_DELAY_MS = 1 * 60 * 1000;\r\n/** Minimum amount of time between GC checks, after the first one. */\r\nconst REGULAR_GC_DELAY_MS = 5 * 60 * 1000;\r\nfunction bufferEntryComparator([aSequence, aIndex], [bSequence, bIndex]) {\r\n    const seqCmp = primitiveComparator(aSequence, bSequence);\r\n    if (seqCmp === 0) {\r\n        // This order doesn't matter, but we can bias against churn by sorting\r\n        // entries created earlier as less than newer entries.\r\n        return primitiveComparator(aIndex, bIndex);\r\n    }\r\n    else {\r\n        return seqCmp;\r\n    }\r\n}\r\n/**\r\n * Used to calculate the nth sequence number. Keeps a rolling buffer of the\r\n * lowest n values passed to `addElement`, and finally reports the largest of\r\n * them in `maxValue`.\r\n */\r\nclass RollingSequenceNumberBuffer {\r\n    constructor(maxElements) {\r\n        this.maxElements = maxElements;\r\n        this.buffer = new SortedSet(bufferEntryComparator);\r\n        this.previousIndex = 0;\r\n    }\r\n    nextIndex() {\r\n        return ++this.previousIndex;\r\n    }\r\n    addElement(sequenceNumber) {\r\n        const entry = [sequenceNumber, this.nextIndex()];\r\n        if (this.buffer.size < this.maxElements) {\r\n            this.buffer = this.buffer.add(entry);\r\n        }\r\n        else {\r\n            const highestValue = this.buffer.last();\r\n            if (bufferEntryComparator(entry, highestValue) < 0) {\r\n                this.buffer = this.buffer.delete(highestValue).add(entry);\r\n            }\r\n        }\r\n    }\r\n    get maxValue() {\r\n        // Guaranteed to be non-empty. If we decide we are not collecting any\r\n        // sequence numbers, nthSequenceNumber below short-circuits. If we have\r\n        // decided that we are collecting n sequence numbers, it's because n is some\r\n        // percentage of the existing sequence numbers. That means we should never\r\n        // be in a situation where we are collecting sequence numbers but don't\r\n        // actually have any.\r\n        return this.buffer.last()[0];\r\n    }\r\n}\r\n/**\r\n * This class is responsible for the scheduling of LRU garbage collection. It handles checking\r\n * whether or not GC is enabled, as well as which delay to use before the next run.\r\n */\r\nclass LruScheduler {\r\n    constructor(garbageCollector, asyncQueue, localStore) {\r\n        this.garbageCollector = garbageCollector;\r\n        this.asyncQueue = asyncQueue;\r\n        this.localStore = localStore;\r\n        this.gcTask = null;\r\n    }\r\n    start() {\r\n        if (this.garbageCollector.params.cacheSizeCollectionThreshold !==\r\n            LRU_COLLECTION_DISABLED) {\r\n            this.scheduleGC(INITIAL_GC_DELAY_MS);\r\n        }\r\n    }\r\n    stop() {\r\n        if (this.gcTask) {\r\n            this.gcTask.cancel();\r\n            this.gcTask = null;\r\n        }\r\n    }\r\n    get started() {\r\n        return this.gcTask !== null;\r\n    }\r\n    scheduleGC(delay) {\r\n        logDebug(LOG_TAG$e, `Garbage collection scheduled in ${delay}ms`);\r\n        this.gcTask = this.asyncQueue.enqueueAfterDelay(\"lru_garbage_collection\" /* TimerId.LruGarbageCollection */, delay, async () => {\r\n            this.gcTask = null;\r\n            try {\r\n                await this.localStore.collectGarbage(this.garbageCollector);\r\n            }\r\n            catch (e) {\r\n                if (isIndexedDbTransactionError(e)) {\r\n                    logDebug(LOG_TAG$e, 'Ignoring IndexedDB error during garbage collection: ', e);\r\n                }\r\n                else {\r\n                    await ignoreIfPrimaryLeaseLoss(e);\r\n                }\r\n            }\r\n            await this.scheduleGC(REGULAR_GC_DELAY_MS);\r\n        });\r\n    }\r\n}\r\n/**\r\n * Implements the steps for LRU garbage collection.\r\n */\r\nclass LruGarbageCollectorImpl {\r\n    constructor(delegate, params) {\r\n        this.delegate = delegate;\r\n        this.params = params;\r\n    }\r\n    calculateTargetCount(txn, percentile) {\r\n        return this.delegate.getSequenceNumberCount(txn).next(targetCount => {\r\n            return Math.floor((percentile / 100.0) * targetCount);\r\n        });\r\n    }\r\n    nthSequenceNumber(txn, n) {\r\n        if (n === 0) {\r\n            return PersistencePromise.resolve(ListenSequence.INVALID);\r\n        }\r\n        const buffer = new RollingSequenceNumberBuffer(n);\r\n        return this.delegate\r\n            .forEachTarget(txn, target => buffer.addElement(target.sequenceNumber))\r\n            .next(() => {\r\n            return this.delegate.forEachOrphanedDocumentSequenceNumber(txn, sequenceNumber => buffer.addElement(sequenceNumber));\r\n        })\r\n            .next(() => buffer.maxValue);\r\n    }\r\n    removeTargets(txn, upperBound, activeTargetIds) {\r\n        return this.delegate.removeTargets(txn, upperBound, activeTargetIds);\r\n    }\r\n    removeOrphanedDocuments(txn, upperBound) {\r\n        return this.delegate.removeOrphanedDocuments(txn, upperBound);\r\n    }\r\n    collect(txn, activeTargetIds) {\r\n        if (this.params.cacheSizeCollectionThreshold === LRU_COLLECTION_DISABLED) {\r\n            logDebug('LruGarbageCollector', 'Garbage collection skipped; disabled');\r\n            return PersistencePromise.resolve(GC_DID_NOT_RUN);\r\n        }\r\n        return this.getCacheSize(txn).next(cacheSize => {\r\n            if (cacheSize < this.params.cacheSizeCollectionThreshold) {\r\n                logDebug('LruGarbageCollector', `Garbage collection skipped; Cache size ${cacheSize} ` +\r\n                    `is lower than threshold ${this.params.cacheSizeCollectionThreshold}`);\r\n                return GC_DID_NOT_RUN;\r\n            }\r\n            else {\r\n                return this.runGarbageCollection(txn, activeTargetIds);\r\n            }\r\n        });\r\n    }\r\n    getCacheSize(txn) {\r\n        return this.delegate.getCacheSize(txn);\r\n    }\r\n    runGarbageCollection(txn, activeTargetIds) {\r\n        let upperBoundSequenceNumber;\r\n        let sequenceNumbersToCollect, targetsRemoved;\r\n        // Timestamps for various pieces of the process\r\n        let countedTargetsTs, foundUpperBoundTs, removedTargetsTs, removedDocumentsTs;\r\n        const startTs = Date.now();\r\n        return this.calculateTargetCount(txn, this.params.percentileToCollect)\r\n            .next(sequenceNumbers => {\r\n            // Cap at the configured max\r\n            if (sequenceNumbers > this.params.maximumSequenceNumbersToCollect) {\r\n                logDebug('LruGarbageCollector', 'Capping sequence numbers to collect down ' +\r\n                    `to the maximum of ${this.params.maximumSequenceNumbersToCollect} ` +\r\n                    `from ${sequenceNumbers}`);\r\n                sequenceNumbersToCollect =\r\n                    this.params.maximumSequenceNumbersToCollect;\r\n            }\r\n            else {\r\n                sequenceNumbersToCollect = sequenceNumbers;\r\n            }\r\n            countedTargetsTs = Date.now();\r\n            return this.nthSequenceNumber(txn, sequenceNumbersToCollect);\r\n        })\r\n            .next(upperBound => {\r\n            upperBoundSequenceNumber = upperBound;\r\n            foundUpperBoundTs = Date.now();\r\n            return this.removeTargets(txn, upperBoundSequenceNumber, activeTargetIds);\r\n        })\r\n            .next(numTargetsRemoved => {\r\n            targetsRemoved = numTargetsRemoved;\r\n            removedTargetsTs = Date.now();\r\n            return this.removeOrphanedDocuments(txn, upperBoundSequenceNumber);\r\n        })\r\n            .next(documentsRemoved => {\r\n            removedDocumentsTs = Date.now();\r\n            if (getLogLevel() <= LogLevel.DEBUG) {\r\n                const desc = 'LRU Garbage Collection\\n' +\r\n                    `\\tCounted targets in ${countedTargetsTs - startTs}ms\\n` +\r\n                    `\\tDetermined least recently used ${sequenceNumbersToCollect} in ` +\r\n                    `${foundUpperBoundTs - countedTargetsTs}ms\\n` +\r\n                    `\\tRemoved ${targetsRemoved} targets in ` +\r\n                    `${removedTargetsTs - foundUpperBoundTs}ms\\n` +\r\n                    `\\tRemoved ${documentsRemoved} documents in ` +\r\n                    `${removedDocumentsTs - removedTargetsTs}ms\\n` +\r\n                    `Total Duration: ${removedDocumentsTs - startTs}ms`;\r\n                logDebug('LruGarbageCollector', desc);\r\n            }\r\n            return PersistencePromise.resolve({\r\n                didRun: true,\r\n                sequenceNumbersCollected: sequenceNumbersToCollect,\r\n                targetsRemoved,\r\n                documentsRemoved\r\n            });\r\n        });\r\n    }\r\n}\r\nfunction newLruGarbageCollector(delegate, params) {\r\n    return new LruGarbageCollectorImpl(delegate, params);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/** Provides LRU functionality for IndexedDB persistence. */\r\nclass IndexedDbLruDelegateImpl {\r\n    constructor(db, params) {\r\n        this.db = db;\r\n        this.garbageCollector = newLruGarbageCollector(this, params);\r\n    }\r\n    getSequenceNumberCount(txn) {\r\n        const docCountPromise = this.orphanedDocumentCount(txn);\r\n        const targetCountPromise = this.db.getTargetCache().getTargetCount(txn);\r\n        return targetCountPromise.next(targetCount => docCountPromise.next(docCount => targetCount + docCount));\r\n    }\r\n    orphanedDocumentCount(txn) {\r\n        let orphanedCount = 0;\r\n        return this.forEachOrphanedDocumentSequenceNumber(txn, _ => {\r\n            orphanedCount++;\r\n        }).next(() => orphanedCount);\r\n    }\r\n    forEachTarget(txn, f) {\r\n        return this.db.getTargetCache().forEachTarget(txn, f);\r\n    }\r\n    forEachOrphanedDocumentSequenceNumber(txn, f) {\r\n        return this.forEachOrphanedDocument(txn, (docKey, sequenceNumber) => f(sequenceNumber));\r\n    }\r\n    addReference(txn, targetId, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    removeReference(txn, targetId, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    removeTargets(txn, upperBound, activeTargetIds) {\r\n        return this.db.getTargetCache().removeTargets(txn, upperBound, activeTargetIds);\r\n    }\r\n    markPotentiallyOrphaned(txn, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    /**\r\n     * Returns true if anything would prevent this document from being garbage\r\n     * collected, given that the document in question is not present in any\r\n     * targets and has a sequence number less than or equal to the upper bound for\r\n     * the collection run.\r\n     */\r\n    isPinned(txn, docKey) {\r\n        return mutationQueuesContainKey(txn, docKey);\r\n    }\r\n    removeOrphanedDocuments(txn, upperBound) {\r\n        const documentCache = this.db.getRemoteDocumentCache();\r\n        const changeBuffer = documentCache.newChangeBuffer();\r\n        const promises = [];\r\n        let documentCount = 0;\r\n        const iteration = this.forEachOrphanedDocument(txn, (docKey, sequenceNumber) => {\r\n            if (sequenceNumber <= upperBound) {\r\n                const p = this.isPinned(txn, docKey).next(isPinned => {\r\n                    if (!isPinned) {\r\n                        documentCount++;\r\n                        // Our size accounting requires us to read all documents before\r\n                        // removing them.\r\n                        return changeBuffer.getEntry(txn, docKey).next(() => {\r\n                            changeBuffer.removeEntry(docKey, SnapshotVersion.min());\r\n                            return documentTargetStore(txn).delete(sentinelKey$1(docKey));\r\n                        });\r\n                    }\r\n                });\r\n                promises.push(p);\r\n            }\r\n        });\r\n        return iteration\r\n            .next(() => PersistencePromise.waitFor(promises))\r\n            .next(() => changeBuffer.apply(txn))\r\n            .next(() => documentCount);\r\n    }\r\n    removeTarget(txn, targetData) {\r\n        const updated = targetData.withSequenceNumber(txn.currentSequenceNumber);\r\n        return this.db.getTargetCache().updateTargetData(txn, updated);\r\n    }\r\n    updateLimboDocument(txn, key) {\r\n        return writeSentinelKey(txn, key);\r\n    }\r\n    /**\r\n     * Call provided function for each document in the cache that is 'orphaned'. Orphaned\r\n     * means not a part of any target, so the only entry in the target-document index for\r\n     * that document will be the sentinel row (targetId 0), which will also have the sequence\r\n     * number for the last time the document was accessed.\r\n     */\r\n    forEachOrphanedDocument(txn, f) {\r\n        const store = documentTargetStore(txn);\r\n        let nextToReport = ListenSequence.INVALID;\r\n        let nextPath;\r\n        return store\r\n            .iterate({\r\n            index: DbTargetDocumentDocumentTargetsIndex\r\n        }, ([targetId, docKey], { path, sequenceNumber }) => {\r\n            if (targetId === 0) {\r\n                // if nextToReport is valid, report it, this is a new key so the\r\n                // last one must not be a member of any targets.\r\n                if (nextToReport !== ListenSequence.INVALID) {\r\n                    f(new DocumentKey(decodeResourcePath(nextPath)), nextToReport);\r\n                }\r\n                // set nextToReport to be this sequence number. It's the next one we\r\n                // might report, if we don't find any targets for this document.\r\n                // Note that the sequence number must be defined when the targetId\r\n                // is 0.\r\n                nextToReport = sequenceNumber;\r\n                nextPath = path;\r\n            }\r\n            else {\r\n                // set nextToReport to be invalid, we know we don't need to report\r\n                // this one since we found a target for it.\r\n                nextToReport = ListenSequence.INVALID;\r\n            }\r\n        })\r\n            .next(() => {\r\n            // Since we report sequence numbers after getting to the next key, we\r\n            // need to check if the last key we iterated over was an orphaned\r\n            // document and report it.\r\n            if (nextToReport !== ListenSequence.INVALID) {\r\n                f(new DocumentKey(decodeResourcePath(nextPath)), nextToReport);\r\n            }\r\n        });\r\n    }\r\n    getCacheSize(txn) {\r\n        return this.db.getRemoteDocumentCache().getSize(txn);\r\n    }\r\n}\r\nfunction sentinelKey$1(key) {\r\n    return [0, encodeResourcePath(key.path)];\r\n}\r\n/**\r\n * @returns A value suitable for writing a sentinel row in the target-document\r\n * store.\r\n */\r\nfunction sentinelRow(key, sequenceNumber) {\r\n    return { targetId: 0, path: encodeResourcePath(key.path), sequenceNumber };\r\n}\r\nfunction writeSentinelKey(txn, key) {\r\n    return documentTargetStore(txn).put(sentinelRow(key, txn.currentSequenceNumber));\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * An in-memory buffer of entries to be written to a RemoteDocumentCache.\r\n * It can be used to batch up a set of changes to be written to the cache, but\r\n * additionally supports reading entries back with the `getEntry()` method,\r\n * falling back to the underlying RemoteDocumentCache if no entry is\r\n * buffered.\r\n *\r\n * Entries added to the cache *must* be read first. This is to facilitate\r\n * calculating the size delta of the pending changes.\r\n *\r\n * PORTING NOTE: This class was implemented then removed from other platforms.\r\n * If byte-counting ends up being needed on the other platforms, consider\r\n * porting this class as part of that implementation work.\r\n */\r\nclass RemoteDocumentChangeBuffer {\r\n    constructor() {\r\n        // A mapping of document key to the new cache entry that should be written.\r\n        this.changes = new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\r\n        this.changesApplied = false;\r\n    }\r\n    /**\r\n     * Buffers a `RemoteDocumentCache.addEntry()` call.\r\n     *\r\n     * You can only modify documents that have already been retrieved via\r\n     * `getEntry()/getEntries()` (enforced via IndexedDbs `apply()`).\r\n     */\r\n    addEntry(document) {\r\n        this.assertNotApplied();\r\n        this.changes.set(document.key, document);\r\n    }\r\n    /**\r\n     * Buffers a `RemoteDocumentCache.removeEntry()` call.\r\n     *\r\n     * You can only remove documents that have already been retrieved via\r\n     * `getEntry()/getEntries()` (enforced via IndexedDbs `apply()`).\r\n     */\r\n    removeEntry(key, readTime) {\r\n        this.assertNotApplied();\r\n        this.changes.set(key, MutableDocument.newInvalidDocument(key).setReadTime(readTime));\r\n    }\r\n    /**\r\n     * Looks up an entry in the cache. The buffered changes will first be checked,\r\n     * and if no buffered change applies, this will forward to\r\n     * `RemoteDocumentCache.getEntry()`.\r\n     *\r\n     * @param transaction - The transaction in which to perform any persistence\r\n     *     operations.\r\n     * @param documentKey - The key of the entry to look up.\r\n     * @returns The cached document or an invalid document if we have nothing\r\n     * cached.\r\n     */\r\n    getEntry(transaction, documentKey) {\r\n        this.assertNotApplied();\r\n        const bufferedEntry = this.changes.get(documentKey);\r\n        if (bufferedEntry !== undefined) {\r\n            return PersistencePromise.resolve(bufferedEntry);\r\n        }\r\n        else {\r\n            return this.getFromCache(transaction, documentKey);\r\n        }\r\n    }\r\n    /**\r\n     * Looks up several entries in the cache, forwarding to\r\n     * `RemoteDocumentCache.getEntry()`.\r\n     *\r\n     * @param transaction - The transaction in which to perform any persistence\r\n     *     operations.\r\n     * @param documentKeys - The keys of the entries to look up.\r\n     * @returns A map of cached documents, indexed by key. If an entry cannot be\r\n     *     found, the corresponding key will be mapped to an invalid document.\r\n     */\r\n    getEntries(transaction, documentKeys) {\r\n        return this.getAllFromCache(transaction, documentKeys);\r\n    }\r\n    /**\r\n     * Applies buffered changes to the underlying RemoteDocumentCache, using\r\n     * the provided transaction.\r\n     */\r\n    apply(transaction) {\r\n        this.assertNotApplied();\r\n        this.changesApplied = true;\r\n        return this.applyChanges(transaction);\r\n    }\r\n    /** Helper to assert this.changes is not null  */\r\n    assertNotApplied() {\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * The RemoteDocumentCache for IndexedDb. To construct, invoke\r\n * `newIndexedDbRemoteDocumentCache()`.\r\n */\r\nclass IndexedDbRemoteDocumentCacheImpl {\r\n    constructor(serializer) {\r\n        this.serializer = serializer;\r\n    }\r\n    setIndexManager(indexManager) {\r\n        this.indexManager = indexManager;\r\n    }\r\n    /**\r\n     * Adds the supplied entries to the cache.\r\n     *\r\n     * All calls of `addEntry` are required to go through the RemoteDocumentChangeBuffer\r\n     * returned by `newChangeBuffer()` to ensure proper accounting of metadata.\r\n     */\r\n    addEntry(transaction, key, doc) {\r\n        const documentStore = remoteDocumentsStore(transaction);\r\n        return documentStore.put(doc);\r\n    }\r\n    /**\r\n     * Removes a document from the cache.\r\n     *\r\n     * All calls of `removeEntry`  are required to go through the RemoteDocumentChangeBuffer\r\n     * returned by `newChangeBuffer()` to ensure proper accounting of metadata.\r\n     */\r\n    removeEntry(transaction, documentKey, readTime) {\r\n        const store = remoteDocumentsStore(transaction);\r\n        return store.delete(dbReadTimeKey(documentKey, readTime));\r\n    }\r\n    /**\r\n     * Updates the current cache size.\r\n     *\r\n     * Callers to `addEntry()` and `removeEntry()` *must* call this afterwards to update the\r\n     * cache's metadata.\r\n     */\r\n    updateMetadata(transaction, sizeDelta) {\r\n        return this.getMetadata(transaction).next(metadata => {\r\n            metadata.byteSize += sizeDelta;\r\n            return this.setMetadata(transaction, metadata);\r\n        });\r\n    }\r\n    getEntry(transaction, documentKey) {\r\n        let doc = MutableDocument.newInvalidDocument(documentKey);\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({\r\n            index: DbRemoteDocumentDocumentKeyIndex,\r\n            range: IDBKeyRange.only(dbKey(documentKey))\r\n        }, (_, dbRemoteDoc) => {\r\n            doc = this.maybeDecodeDocument(documentKey, dbRemoteDoc);\r\n        })\r\n            .next(() => doc);\r\n    }\r\n    /**\r\n     * Looks up an entry in the cache.\r\n     *\r\n     * @param documentKey - The key of the entry to look up.\r\n     * @returns The cached document entry and its size.\r\n     */\r\n    getSizedEntry(transaction, documentKey) {\r\n        let result = {\r\n            size: 0,\r\n            document: MutableDocument.newInvalidDocument(documentKey)\r\n        };\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({\r\n            index: DbRemoteDocumentDocumentKeyIndex,\r\n            range: IDBKeyRange.only(dbKey(documentKey))\r\n        }, (_, dbRemoteDoc) => {\r\n            result = {\r\n                document: this.maybeDecodeDocument(documentKey, dbRemoteDoc),\r\n                size: dbDocumentSize(dbRemoteDoc)\r\n            };\r\n        })\r\n            .next(() => result);\r\n    }\r\n    getEntries(transaction, documentKeys) {\r\n        let results = mutableDocumentMap();\r\n        return this.forEachDbEntry(transaction, documentKeys, (key, dbRemoteDoc) => {\r\n            const doc = this.maybeDecodeDocument(key, dbRemoteDoc);\r\n            results = results.insert(key, doc);\r\n        }).next(() => results);\r\n    }\r\n    /**\r\n     * Looks up several entries in the cache.\r\n     *\r\n     * @param documentKeys - The set of keys entries to look up.\r\n     * @returns A map of documents indexed by key and a map of sizes indexed by\r\n     *     key (zero if the document does not exist).\r\n     */\r\n    getSizedEntries(transaction, documentKeys) {\r\n        let results = mutableDocumentMap();\r\n        let sizeMap = new SortedMap(DocumentKey.comparator);\r\n        return this.forEachDbEntry(transaction, documentKeys, (key, dbRemoteDoc) => {\r\n            const doc = this.maybeDecodeDocument(key, dbRemoteDoc);\r\n            results = results.insert(key, doc);\r\n            sizeMap = sizeMap.insert(key, dbDocumentSize(dbRemoteDoc));\r\n        }).next(() => {\r\n            return { documents: results, sizeMap };\r\n        });\r\n    }\r\n    forEachDbEntry(transaction, documentKeys, callback) {\r\n        if (documentKeys.isEmpty()) {\r\n            return PersistencePromise.resolve();\r\n        }\r\n        let sortedKeys = new SortedSet(dbKeyComparator);\r\n        documentKeys.forEach(e => (sortedKeys = sortedKeys.add(e)));\r\n        const range = IDBKeyRange.bound(dbKey(sortedKeys.first()), dbKey(sortedKeys.last()));\r\n        const keyIter = sortedKeys.getIterator();\r\n        let nextKey = keyIter.getNext();\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({ index: DbRemoteDocumentDocumentKeyIndex, range }, (_, dbRemoteDoc, control) => {\r\n            const potentialKey = DocumentKey.fromSegments([\r\n                ...dbRemoteDoc.prefixPath,\r\n                dbRemoteDoc.collectionGroup,\r\n                dbRemoteDoc.documentId\r\n            ]);\r\n            // Go through keys not found in cache.\r\n            while (nextKey && dbKeyComparator(nextKey, potentialKey) < 0) {\r\n                callback(nextKey, null);\r\n                nextKey = keyIter.getNext();\r\n            }\r\n            if (nextKey && nextKey.isEqual(potentialKey)) {\r\n                // Key found in cache.\r\n                callback(nextKey, dbRemoteDoc);\r\n                nextKey = keyIter.hasNext() ? keyIter.getNext() : null;\r\n            }\r\n            // Skip to the next key (if there is one).\r\n            if (nextKey) {\r\n                control.skip(dbKey(nextKey));\r\n            }\r\n            else {\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => {\r\n            // The rest of the keys are not in the cache. One case where `iterate`\r\n            // above won't go through them is when the cache is empty.\r\n            while (nextKey) {\r\n                callback(nextKey, null);\r\n                nextKey = keyIter.hasNext() ? keyIter.getNext() : null;\r\n            }\r\n        });\r\n    }\r\n    getDocumentsMatchingQuery(transaction, query, offset, mutatedDocs) {\r\n        const collection = query.path;\r\n        const startKey = [\r\n            collection.popLast().toArray(),\r\n            collection.lastSegment(),\r\n            toDbTimestampKey(offset.readTime),\r\n            offset.documentKey.path.isEmpty()\r\n                ? ''\r\n                : offset.documentKey.path.lastSegment()\r\n        ];\r\n        const endKey = [\r\n            collection.popLast().toArray(),\r\n            collection.lastSegment(),\r\n            [Number.MAX_SAFE_INTEGER, Number.MAX_SAFE_INTEGER],\r\n            ''\r\n        ];\r\n        return remoteDocumentsStore(transaction)\r\n            .loadAll(IDBKeyRange.bound(startKey, endKey, true))\r\n            .next(dbRemoteDocs => {\r\n            let results = mutableDocumentMap();\r\n            for (const dbRemoteDoc of dbRemoteDocs) {\r\n                const document = this.maybeDecodeDocument(DocumentKey.fromSegments(dbRemoteDoc.prefixPath.concat(dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId)), dbRemoteDoc);\r\n                if (document.isFoundDocument() &&\r\n                    (queryMatches(query, document) || mutatedDocs.has(document.key))) {\r\n                    // Either the document matches the given query, or it is mutated.\r\n                    results = results.insert(document.key, document);\r\n                }\r\n            }\r\n            return results;\r\n        });\r\n    }\r\n    getAllFromCollectionGroup(transaction, collectionGroup, offset, limit) {\r\n        let results = mutableDocumentMap();\r\n        const startKey = dbCollectionGroupKey(collectionGroup, offset);\r\n        const endKey = dbCollectionGroupKey(collectionGroup, IndexOffset.max());\r\n        return remoteDocumentsStore(transaction)\r\n            .iterate({\r\n            index: DbRemoteDocumentCollectionGroupIndex,\r\n            range: IDBKeyRange.bound(startKey, endKey, true)\r\n        }, (_, dbRemoteDoc, control) => {\r\n            const document = this.maybeDecodeDocument(DocumentKey.fromSegments(dbRemoteDoc.prefixPath.concat(dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId)), dbRemoteDoc);\r\n            results = results.insert(document.key, document);\r\n            if (results.size === limit) {\r\n                control.done();\r\n            }\r\n        })\r\n            .next(() => results);\r\n    }\r\n    newChangeBuffer(options) {\r\n        return new IndexedDbRemoteDocumentChangeBuffer(this, !!options && options.trackRemovals);\r\n    }\r\n    getSize(txn) {\r\n        return this.getMetadata(txn).next(metadata => metadata.byteSize);\r\n    }\r\n    getMetadata(txn) {\r\n        return documentGlobalStore(txn)\r\n            .get(DbRemoteDocumentGlobalKey)\r\n            .next(metadata => {\r\n            hardAssert(!!metadata);\r\n            return metadata;\r\n        });\r\n    }\r\n    setMetadata(txn, metadata) {\r\n        return documentGlobalStore(txn).put(DbRemoteDocumentGlobalKey, metadata);\r\n    }\r\n    /**\r\n     * Decodes `dbRemoteDoc` and returns the document (or an invalid document if\r\n     * the document corresponds to the format used for sentinel deletes).\r\n     */\r\n    maybeDecodeDocument(documentKey, dbRemoteDoc) {\r\n        if (dbRemoteDoc) {\r\n            const doc = fromDbRemoteDocument(this.serializer, dbRemoteDoc);\r\n            // Whether the document is a sentinel removal and should only be used in the\r\n            // `getNewDocumentChanges()`\r\n            const isSentinelRemoval = doc.isNoDocument() && doc.version.isEqual(SnapshotVersion.min());\r\n            if (!isSentinelRemoval) {\r\n                return doc;\r\n            }\r\n        }\r\n        return MutableDocument.newInvalidDocument(documentKey);\r\n    }\r\n}\r\n/** Creates a new IndexedDbRemoteDocumentCache. */\r\nfunction newIndexedDbRemoteDocumentCache(serializer) {\r\n    return new IndexedDbRemoteDocumentCacheImpl(serializer);\r\n}\r\n/**\r\n * Handles the details of adding and updating documents in the IndexedDbRemoteDocumentCache.\r\n *\r\n * Unlike the MemoryRemoteDocumentChangeBuffer, the IndexedDb implementation computes the size\r\n * delta for all submitted changes. This avoids having to re-read all documents from IndexedDb\r\n * when we apply the changes.\r\n */\r\nclass IndexedDbRemoteDocumentChangeBuffer extends RemoteDocumentChangeBuffer {\r\n    /**\r\n     * @param documentCache - The IndexedDbRemoteDocumentCache to apply the changes to.\r\n     * @param trackRemovals - Whether to create sentinel deletes that can be tracked by\r\n     * `getNewDocumentChanges()`.\r\n     */\r\n    constructor(documentCache, trackRemovals) {\r\n        super();\r\n        this.documentCache = documentCache;\r\n        this.trackRemovals = trackRemovals;\r\n        // A map of document sizes and read times prior to applying the changes in\r\n        // this buffer.\r\n        this.documentStates = new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\r\n    }\r\n    applyChanges(transaction) {\r\n        const promises = [];\r\n        let sizeDelta = 0;\r\n        let collectionParents = new SortedSet((l, r) => primitiveComparator(l.canonicalString(), r.canonicalString()));\r\n        this.changes.forEach((key, documentChange) => {\r\n            const previousDoc = this.documentStates.get(key);\r\n            promises.push(this.documentCache.removeEntry(transaction, key, previousDoc.readTime));\r\n            if (documentChange.isValidDocument()) {\r\n                const doc = toDbRemoteDocument(this.documentCache.serializer, documentChange);\r\n                collectionParents = collectionParents.add(key.path.popLast());\r\n                const size = dbDocumentSize(doc);\r\n                sizeDelta += size - previousDoc.size;\r\n                promises.push(this.documentCache.addEntry(transaction, key, doc));\r\n            }\r\n            else {\r\n                sizeDelta -= previousDoc.size;\r\n                if (this.trackRemovals) {\r\n                    // In order to track removals, we store a \"sentinel delete\" in the\r\n                    // RemoteDocumentCache. This entry is represented by a NoDocument\r\n                    // with a version of 0 and ignored by `maybeDecodeDocument()` but\r\n                    // preserved in `getNewDocumentChanges()`.\r\n                    const deletedDoc = toDbRemoteDocument(this.documentCache.serializer, documentChange.convertToNoDocument(SnapshotVersion.min()));\r\n                    promises.push(this.documentCache.addEntry(transaction, key, deletedDoc));\r\n                }\r\n            }\r\n        });\r\n        collectionParents.forEach(parent => {\r\n            promises.push(this.documentCache.indexManager.addToCollectionParentIndex(transaction, parent));\r\n        });\r\n        promises.push(this.documentCache.updateMetadata(transaction, sizeDelta));\r\n        return PersistencePromise.waitFor(promises);\r\n    }\r\n    getFromCache(transaction, documentKey) {\r\n        // Record the size of everything we load from the cache so we can compute a delta later.\r\n        return this.documentCache\r\n            .getSizedEntry(transaction, documentKey)\r\n            .next(getResult => {\r\n            this.documentStates.set(documentKey, {\r\n                size: getResult.size,\r\n                readTime: getResult.document.readTime\r\n            });\r\n            return getResult.document;\r\n        });\r\n    }\r\n    getAllFromCache(transaction, documentKeys) {\r\n        // Record the size of everything we load from the cache so we can compute\r\n        // a delta later.\r\n        return this.documentCache\r\n            .getSizedEntries(transaction, documentKeys)\r\n            .next(({ documents, sizeMap }) => {\r\n            // Note: `getAllFromCache` returns two maps instead of a single map from\r\n            // keys to `DocumentSizeEntry`s. This is to allow returning the\r\n            // `MutableDocumentMap` directly, without a conversion.\r\n            sizeMap.forEach((documentKey, size) => {\r\n                this.documentStates.set(documentKey, {\r\n                    size,\r\n                    readTime: documents.get(documentKey).readTime\r\n                });\r\n            });\r\n            return documents;\r\n        });\r\n    }\r\n}\r\nfunction documentGlobalStore(txn) {\r\n    return getStore(txn, DbRemoteDocumentGlobalStore);\r\n}\r\n/**\r\n * Helper to get a typed SimpleDbStore for the remoteDocuments object store.\r\n */\r\nfunction remoteDocumentsStore(txn) {\r\n    return getStore(txn, DbRemoteDocumentStore);\r\n}\r\n/**\r\n * Returns a key that can be used for document lookups on the\r\n * `DbRemoteDocumentDocumentKeyIndex` index.\r\n */\r\nfunction dbKey(documentKey) {\r\n    const path = documentKey.path.toArray();\r\n    return [\r\n        /* prefix path */ path.slice(0, path.length - 2),\r\n        /* collection id */ path[path.length - 2],\r\n        /* document id */ path[path.length - 1]\r\n    ];\r\n}\r\n/**\r\n * Returns a key that can be used for document lookups via the primary key of\r\n * the DbRemoteDocument object store.\r\n */\r\nfunction dbReadTimeKey(documentKey, readTime) {\r\n    const path = documentKey.path.toArray();\r\n    return [\r\n        /* prefix path */ path.slice(0, path.length - 2),\r\n        /* collection id */ path[path.length - 2],\r\n        toDbTimestampKey(readTime),\r\n        /* document id */ path[path.length - 1]\r\n    ];\r\n}\r\n/**\r\n * Returns a key that can be used for document lookups on the\r\n * `DbRemoteDocumentDocumentCollectionGroupIndex` index.\r\n */\r\nfunction dbCollectionGroupKey(collectionGroup, offset) {\r\n    const path = offset.documentKey.path.toArray();\r\n    return [\r\n        /* collection id */ collectionGroup,\r\n        toDbTimestampKey(offset.readTime),\r\n        /* prefix path */ path.slice(0, path.length - 2),\r\n        /* document id */ path.length > 0 ? path[path.length - 1] : ''\r\n    ];\r\n}\r\n/**\r\n * Comparator that compares document keys according to the primary key sorting\r\n * used by the `DbRemoteDocumentDocument` store (by prefix path, collection id\r\n * and then document ID).\r\n *\r\n * Visible for testing.\r\n */\r\nfunction dbKeyComparator(l, r) {\r\n    const left = l.path.toArray();\r\n    const right = r.path.toArray();\r\n    // The ordering is based on https://chromium.googlesource.com/chromium/blink/+/fe5c21fef94dae71c1c3344775b8d8a7f7e6d9ec/Source/modules/indexeddb/IDBKey.cpp#74\r\n    let cmp = 0;\r\n    for (let i = 0; i < left.length - 2 && i < right.length - 2; ++i) {\r\n        cmp = primitiveComparator(left[i], right[i]);\r\n        if (cmp) {\r\n            return cmp;\r\n        }\r\n    }\r\n    cmp = primitiveComparator(left.length, right.length);\r\n    if (cmp) {\r\n        return cmp;\r\n    }\r\n    cmp = primitiveComparator(left[left.length - 2], right[right.length - 2]);\r\n    if (cmp) {\r\n        return cmp;\r\n    }\r\n    return primitiveComparator(left[left.length - 1], right[right.length - 1]);\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Schema Version for the Web client:\r\n * 1.  Initial version including Mutation Queue, Query Cache, and Remote\r\n *     Document Cache\r\n * 2.  Used to ensure a targetGlobal object exists and add targetCount to it. No\r\n *     longer required because migration 3 unconditionally clears it.\r\n * 3.  Dropped and re-created Query Cache to deal with cache corruption related\r\n *     to limbo resolution. Addresses\r\n *     https://github.com/firebase/firebase-ios-sdk/issues/1548\r\n * 4.  Multi-Tab Support.\r\n * 5.  Removal of held write acks.\r\n * 6.  Create document global for tracking document cache size.\r\n * 7.  Ensure every cached document has a sentinel row with a sequence number.\r\n * 8.  Add collection-parent index for Collection Group queries.\r\n * 9.  Change RemoteDocumentChanges store to be keyed by readTime rather than\r\n *     an auto-incrementing ID. This is required for Index-Free queries.\r\n * 10. Rewrite the canonical IDs to the explicit Protobuf-based format.\r\n * 11. Add bundles and named_queries for bundle support.\r\n * 12. Add document overlays.\r\n * 13. Rewrite the keys of the remote document cache to allow for efficient\r\n *     document lookup via `getAll()`.\r\n * 14. Add overlays.\r\n * 15. Add indexing support.\r\n */\r\nconst SCHEMA_VERSION = 15;\n\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * Represents a local view (overlay) of a document, and the fields that are\r\n * locally mutated.\r\n */\r\nclass OverlayedDocument {\r\n    constructor(overlayedDocument, \r\n    /**\r\n     * The fields that are locally mutated by patch mutations.\r\n     *\r\n     * If the overlayed\tdocument is from set or delete mutations, this is `null`.\r\n     * If there is no overlay (mutation) for the document, this is an empty `FieldMask`.\r\n     */\r\n    mutatedFields) {\r\n        this.overlayedDocument = overlayedDocument;\r\n        this.mutatedFields = mutatedFields;\r\n    }\r\n}\n\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n/**\r\n * A readonly view of the local state of all documents we're tracking (i.e. we\r\n * have a cached version in remoteDocumentCache or local mutations for the\r\n * document). The view is computed by applying the mutations in the\r\n * MutationQueue to the RemoteDocumentCache.\r\n */\r\nclass LocalDocumentsView {\r\n    constructor(remoteDocumentCache, mutationQueue, documentOverlayCache, indexManager) {\r\n        this.remoteDocumentCache = remoteDocumentCache;\r\n        this.mutationQueue = mutationQueue;\r\n        this.documentOverlayCache = documentOverlayCache;\r\n        this.indexManager = indexManager;\r\n    }\r\n    /**\r\n     * Get the local view of the document identified by `key`.\r\n     *\r\n     * @returns Local view of the document or null if we don't have any cached\r\n     * state for it.\r\n     */\r\n    getDocument(transaction, key) {\r\n        let overlay = null;\r\n        return this.documentOverlayCache\r\n            .getOverlay(transaction, key)\r\n            .next(value => {\r\n            overlay = value;\r\n            return this.remoteDocumentCache.getEntry(transaction, key);\r\n        })\r\n            .next(document => {\r\n            if (overlay !== null) {\r\n                mutationApplyToLocalView(overlay.mutation, document, FieldMask.empty(), Timestamp.now());\r\n            }\r\n            return document;\r\n        });\r\n    }\r\n    /**\r\n     * Gets the local view of the documents identified by `keys`.\r\n     *\r\n     * If we don't have cached state for a document in `keys`, a NoDocument will\r\n     * be stored for that key in the resulting set.\r\n     */\r\n    getDocuments(transaction, keys) {\r\n        return this.remoteDocumentCache\r\n            .getEntries(transaction, keys)\r\n            .next(docs => this.getLocalViewOfDocuments(transaction, docs, documentKeySet()).next(() => docs));\r\n    }\r\n    /**\r\n     * Similar to `getDocuments`, but creates the local view from the given\r\n     * `baseDocs` without retrieving documents from the local store.\r\n     *\r\n     * @param transaction - The transaction this operation is scoped to.\r\n     * @param docs - The documents to apply local mutations to get the local views.\r\n     * @param existenceStateChanged - The set of document keys whose existence state\r\n     *   is changed. This is useful to determine if some documents overlay needs\r\n     *   to be recalculated.\r\n     */\r\n    getLocalViewOfDocuments(transaction, docs, existenceStateChanged = documentKeySet()) {\r\n        const overlays = newOverlayMap();\r\n        return this.populateOverlays(transaction, overlays, docs).next(() => {\r\n            return this.computeViews(transaction, docs, overlays, existenceStateChanged).next(computeViewsResult => {\r\n                let result = documentMap();\r\n                computeViewsResult.forEach((documentKey, overlayedDocument) => {\r\n                    result = result.insert(documentKey, overlayedDocument.overlayedDocument);\r\n                });\r\n                return result;\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Gets the overlayed documents for the given document map, which will include\r\n     * the local view of those documents and a `FieldMask` indicating which fields\r\n     * are mutated locally, `null` if overlay is a Set or Delete mutation.\r\n     */\r\n    getOverlayedDocuments(transaction, docs) {\r\n        const overlays = newOverlayMap();\r\n        return this.populateOverlays(transaction, overlays, docs).next(() => this.computeViews(transaction, docs, overlays, documentKeySet()));\r\n    }\r\n    /**\r\n     * Fetches the overlays for {@code docs} and adds them to provided overlay map\r\n     * if the map does not already contain an entry for the given document key.\r\n     */\r\n    populateOverlays(transaction, overlays, docs) {\r\n        const missingOverlays = [];\r\n        docs.forEach(key => {\r\n            if (!overlays.has(key)) {\r\n                missingOverlays.push(key);\r\n            }\r\n        });\r\n        return this.documentOverlayCache\r\n            .getOverlays(transaction, missingOverlays)\r\n            .next(result => {\r\n            result.forEach((key, val) => {\r\n                overlays.set(key, val);\r\n            });\r\n        });\r\n    }\r\n    /**\r\n     * Computes the local view for the given documents.\r\n     *\r\n     * @param docs - The documents to compute views for. It also has the base\r\n     *   version of the documents.\r\n     * @param overlays - The overlays that need to be applied to the given base\r\n     *   version of the documents.\r\n     * @param existenceStateChanged - A set of documents whose existence states\r\n     *   might have changed. This is used to determine if we